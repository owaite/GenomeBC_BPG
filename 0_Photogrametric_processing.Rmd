# Agisoft Photogrammetry pipeline {#processing-orthomosaics}

**Processing P1 data and registering multispectral to it**

Software: Agisoft LLC. (2024). _Agisoft Metashape Professional, Version 2.1.1_. 

We use Agisoft Metashape Pro to process aerial photogrammetry into orthomosaics. The following is a complete workflow for processing a P1 RGB that is geometrically accurate, registering the less accurate multispectral imagery to it, and producing calibrated ten band orthomosaics for further processing into vegetative indices.  We process all DAP flights from the same site on the same day in one Agisoft project. Each flight is referred to as a “chunk” by Agisoft. In this case we are processing both a MS and MS_P chunk in order to test and provide examples, but normally only one would be used.  The settings used will vastly affect processing times and quality and each setting has been carefully considered.

Agisoft processes can be started using GUI tools or scripted using the python console.  We don’t use a full scripted workflow as there are several manual steps outlined below, but we do use snippets of code for some steps.

The workflow is based on a site approximately 1.5 Ha buffered to 2Ha and the raw data would be approximately:

-	P1 (RGB): 500 photos and 12 GB of data
-	MS_P (10 band plus panchro): 12,000 photos equaling 42 GB
-	MS (10 band): 10,500 photos equaling 25GB 

## Overview.

1.	Import all photos for each chunk and locate the calibration panel captures for the multispectral chunks. Then save to the project file structure.
2.	Manually assess all flight lines and disable outliers.
3.	Build the P1 RGB through to orthomosaic using the settings detailed below.
4.	Confirm the P1 is aligned to the GCPs and template P1 if this is not the primary flight.
5.	For each of the other DAP sensors, specifically MS and MS_P chunks, build through to orthomosaic following the steps outlined below paying particular attention to the chunk align.
6.	Check the ortho registration and reflectance values.
7.	Run the closing script for each multispectral sensor after entering the variables into the script.  This script will:
    -	Normalize the 16bit pixel values.  
    -	Import the 15m buffered shape of the trial and export the orthomosaic clipped to this shape in NAD83 to the project specific folder. 

Approximate processing times:

  -	 ~12 hours for the P1 RGB.
  -	 ~18-38 hours for each of the Micasense Dual sensors, with the MS_P on the higher end. 
  
This process produces an Agisoft folder with the P1 and MS chunks containing ~80GB of files. 
The exported orthomosaic tifs contain 10 (MS) or 11 (MS_P) bands and are ready for analysis.


```{r aligned-agisoft, echo=FALSE, out.width="100%",fig.align='center', fig.cap = "A dense point cloud processed in Agisoft Metashape with the camera locations turned on and disabled flight trajectories shown."}
knitr::include_graphics(here("Photos_&_gifs\\Aligned_agisoft.png"))
```

**Coordinate Reference Systems:**

- 	DJI drones and GNSS units work in the geodetic coordinate system (GCS) WGS84,  EPSG:4326
- 	We chose to work in WGS84 during photogrammetry processing until exporting orthomosaics.  We export orthomosaics for further processing into the coordinate reference system NAD83/UTM.  

**Agisoft Preferences Setup:**
Prior to commencing processing, we recommend the following settings in Tools-Preferences.

- GPU tab: Uncheck the Use CPU when performing GPU accelerated processing.  This will free up your CPU during the heavy processing steps.
- Advanced tab: The settings in Figure \@ref(fig:metashape-pref) are recommended. 

```{r metashape-pref, echo=FALSE, fig.show="hold",fig.align='center', out.width="50%", fig.cap = "Recommended Metashape preferences setup in Tools, Metashape Preferences, under the Advanced tab."}
# Include the pre-brushing and post-brushing images
knitr::include_graphics(c(
  here("Photos_&_gifs/metashape_pref.png"),
  here("Photos_&_gifs/metashape_pref_2.png")
))
```

## Detailed Workflow

1. **Load Photos and locate calibration panels** - Script (or GUI). 15 minutes.

  - Agisoft processes all ten bands as a multicamera with one primary channel. We import imagery using a python script located [here](https://github.com/owaite/GenomeBC_BPG/blob/main/Scripts/TheOpener_BPG.py).  However, you can also import folders containing imagery in metashape via the import tool or by dragging and dropping the folder into the specified chunk. 

  - The script will import all photos for each chunk, then locate the calibration panel captures for the multispectral and move them to a “Calibration images” folder in the Agisoft Workspace tab. If you chose to manually import imagery, you can locate calibration panels using the Calibrate Reflectance tool in the Tools GUI as seen below in Figure \@ref(fig:cal-ref-agisoft).


```{r cal-ref-agisoft, echo=FALSE, out.width="100%", fig.align='center', fig.cap = "Calibrate reflectance tool found under Tools in Metashape."}
knitr::include_graphics(here("Photos_&_gifs\\cal_ref_metashape.png"))
```

**Note:** To leverage the higher resolution imagery available with the MS_P sensor you are required to set the panchromatic camera as the primary channel for processing.  Go to Tools – Set Primary Channel – select Panchro.

2. **Micasense Corrections outside of Agisoft(if necessary)** – R Script. 2 hours.

  - We have encountered incorrect irradiance data in metadata of MicaSense imagery due to incorrect measurements by the DLS2. To correct the irradiance data prior to processing we use methods from MicaSense’s DLS1 system. See the chapter on [MicaSense Irradiance Correction](#MicaSense-Irradiance-Correction) for checks on whether your data needs correction and a detailed explanation of the correction process.

3.	**Visually Assess Flight Trajectory and Disable Outliers** – Manual. 10 minutes.

  - The MS cameras start taking pictures as soon as image capture has been started via the webpage on the ground and don’t stop until we power off at the end of the flight. Hence, we examine the flight trajectory and disable photos taken during flight to and from the site.

```{r trajectory, echo=FALSE, out.width="100%",fig.align='center', fig.cap = "Flight trajectory with light blue disabled and dark blue as enabled cameras prior to photo alignment."}
knitr::include_graphics(here("Photos_&_gifs\\trajectory.png"))
```

4.	**Select the Best Calibration Panels and Calibrate Reflectance** – Manual and Processed. 15 minutes.

  - Each panel has a unique set of calibration values provided by the manufacturer in a csv.  The Select Panel button in the Calibrate Reflectance tool will direct you to import the csv. 
  - Agisoft Metashape takes the latest before the flight and the earliest after the flight panel captures and linearly interpolates between them.
  - We select one panel capture from the start and one from the end of the flight, by visually selecting a centered capture without shadow or glare and moving the rest to an “Unused_Cal_Images” folder (this folder needs to be created in the GUI). 
  - We run the calibration process using both the DLS and Calibration Panels checked in . However, Micasense recommends only calibrating with panels on full sun days. Please refer to the [Micasense Knowledge Base - Best practices: Collecting Data with MicaSense Sensors](https://support.micasense.com/hc/en-us/articles/224893167-Best-practices-Collecting-Data-with-MicaSense-Sensors#:~:text=Hold%20the%20aircraft%20at%20chest,offset%20slightly%20to%20prevent%20shadows.) for more details.

```{r cal-ref-sunPanel-agisoft, echo=FALSE, out.width="100%",fig.align='center', fig.cap = "Calibrate reflectance tool found under Tools in Metashape with sun sensor and reflectance panels enabled for calibration."}
knitr::include_graphics(here("Photos_&_gifs\\cal_ref_sun_panel_metashape.png"))
```

5.	**Align Photos** – GUI - Processed. 3-10 hours.

  - Photogrammetric alignment is the process of matching and orienting overlapping images by identifying common features known as tie points, across multiple images. This alignment generates a geometrically accurate sparse point cloud from the tie points.
  - We use the batch processing tool and settings as seen below.
  - The sparse clouds vary greatly in density, the P1 clouds on a closed canopy can be as low as 150,000 points.  The MS_P sparse clouds could be 48 million points on the same flight plan.  


```{r align-photo-agisoft, echo=FALSE, out.width="40%", fig.align='center', fig.cap = "Photogrametric camera alignment settings using the alignment tool in Metashape"}
knitr::include_graphics(here("Photos_&_gifs\\align_photos_metashape.png"))
```

6.	**Import and Align GCPs** – Manual. 15 minutes.

  - The GCP coordinates are imported in WGS 84.
  -	**Note** GCPs can be used either actively as control points in which case they will “pull” the images into alignment or passively as check points to measure error.
  - We are only using GCPs as check points now, but they will be implemented if registration fails.
  -Instructions to align GCPs if necessary
    -	In the Reference pane check GCP_1 right click on it and filter photos by marker
    - Open the photos tab, and double click the first photo.  Drag the marker to the center of the photo.  Do this for several photos - then click on the update transform tool found in the Reference pane.
    - Scroll through at least 3 more pics to make sure they are aligned.
    - Repeat for each GCPs hitting update transform tool located in the reference panel once you have set the photos.
    - Leave at least one GCP for a “checkpoint”.

7.	 **Sparse Cloud - Filter and Optimize Camera Alignment** - manual. 30-45 minutes.

  - This iterative step filters the sparse cloud for outliers and optimizes the camera calibration.  Most literature suggests more aggressive filtering, but on closed canopy sites this tends to cause loss.  Visually assess the cloud at each stage to confirm the sparse cloud is not becoming too sparse.
  - In the GUI select Model – Gradual Selection – Reconstruction Uncertainty: Adjust the slider until the value is 25-75, while selecting a Max of %10 of the points.
  - In Figure \@ref(fig:sparseCloud-agisoft), there are 400,000 points selected out of 40 million and the uncertainty is at 30.
  - Hit okay and delete to remove points.
```{r sparseCloud-agisoft, echo=FALSE, out.width="100%",fig.align='center', fig.cap = "Sparse cloud created in image alignment. Pink points are those selected by the Gradual Selection tool that have a reconstruction uncertainty greater than or equal to 30.4608. These points will be filtered out."}
knitr::include_graphics(here("Photos_&_gifs\\sparse_cloud_metashape.png"))
```

  - Next we optimize the camera alignment. Tools - Optimize Camera Alignment: default settings as in Figure \@ref(fig:opt-align-agisoft).
     
```{r opt-align-agisoft, echo=FALSE, out.width="60%",fig.align='center', fig.cap = "Settings used for the Optimize Camera Alignment tool"}
knitr::include_graphics(here("Photos_&_gifs\\opt_camera_align_metashape.png"))
```
  
  - We now go through the same steps again filtering points and optimizing the cameras for the following settings.
  - Model – Gradual Selection – Projection Accuracy: 5-20. Max%10
  - Optimize Camera Alignment at stock - check estimate tiepoint covariance
  - Model – Gradual Selection – Reprojection Error: 0.3-1. Max%10
  - Optimize Camera Alignment at stock with fit additional corrections and estimate tiepoint covariance.
  - Finally using the select tool we manually clip any edge points not cohesive to the flight.

8.	**Point based chunk alignment:** multispectral to P1 - (requires manual verification). 2-6 hours. Batch processing

  - Leveraging the accuracy of the P1 as a reference, the MS sparse cloud is aligned to the higher accuracy P1 sparse cloud using the batch processing steps according to the settings in Figure \@ref(fig:chunk-align-agisoft).
```{r chunk-align-agisoft, echo=FALSE, out.width="60%", fig.align='center',fig.cap = "Settings used for point based chunk alignment when aligning the multispectral data to the P1."}
knitr::include_graphics(here("Photos_&_gifs\\align_chunks_metashape.png"))
```

  - Visually confirm each multispectral has in fact aligned by seeing the [T] for transposed as in Figure \@ref(fig:chunk-align-agisoft).  If the chunk align on medium setting fails, try again on low.  If this also fails, you will need to align with GCP's or attempt georeferencing the output orthomosaics in a GIS.

```{r chunk-align-confirm-agisoft, echo=FALSE, out.width="60%",fig.align='center', fig.cap = "Visual of the [T] that will appear next to chunks that have successfully aligned via chunk alignment."}
knitr::include_graphics(here("Photos_&_gifs\\transposed_metashape.png"))
```

9.	**Build Depth Maps and Dense Cloud** – Script or Batch Process. 5-24 hours.

  - A dense point cloud, a high-resolution 3D representation with significantly more detail than the sparse cloud is built in this step.  First depth maps are created for each image, capturing detailed information about the distance to surfaces in the scene. These depth maps are then combined to generate the dense point cloud. This is a primary product and can be exported.  We have been building these to the highest settings as outlined in Figure \@ref(fig:DC-settings-agisoft), but if this is not required significant time can be saved by lowering the setting.  An example dense cloud is seen in Figure \@ref(fig:aligned-agisoft)
     
```{r DC-settings-agisoft, echo=FALSE, out.width="60%",fig.align='center', fig.cap = "Metashape settings for building the depth maps and the dense cloud."}
knitr::include_graphics(here("Photos_&_gifs\\DC_settings_metashape.png"))
```

10.	**Build DEM** – Script or Batch Processed. ~1 hour.

  - Generates a Digital Elevation Model from the dense cloud data. Batch settings in Figure \@ref(fig:DEM-settings-agisoft)

```{r DEM-settings-agisoft, echo=FALSE, out.width="60%",fig.align='center', fig.cap = "Metashape settings for DEM generation."}
knitr::include_graphics(here("Photos_&_gifs\\build_DEM_metashape.png"))
```

11.	**Build Orthomosaic** – Script or Batch Processed. 1-4 hours.

  - Creates an orthomosaic by stitching together the aligned images and the DEM according to batch processing steps in Figure \@ref(fig:ortho-settings-agisoft).
  - **NOTE: We do NOT enable hole filling for multispectral** – only used for RGB orthomosaics that were needed strictly for visual purposes and not analysis.

```{r ortho-settings-agisoft, echo=FALSE, out.width="60%", fig.align='center', fig.cap = "Metashape settings for building the orthomosaic."}
knitr::include_graphics(here("Photos_&_gifs\\build_ortho_metashape.png"))
```

12.	**Check Orthomosaic Registration* - Manual. 15 minutes to redo from GCP alignment.

  - The orthomosaic is checked for alignment to the GCPs. If it is not close on each GCP, we go back and try alignment with GCPs. It is very important to check all corners, as occasionally three out of four will align beautifully, and the fourth will be 25-50cm out of alignment.
  - In the cases where we were unable to establish GCPs the orthomosaic is exported and checked against the P1 in a GIS. 
    
13.	**Sanity Check Reflectance Values** – Manual. 15 minutes – 2 hours if remaking orthomosaics.

  - The orthomosaic is sanity checked for reflectance values. We look at the histogram of three different bands to ensure the values make sense. If they do not, another calibration panel is chosen, the reflectance is recalibrated, and the orthomosaic is rebuilt.
  - To check values - go into Raster Transform - Enable Transform
  - Select B1 (blue band). Click the circular arrows to the left of the “interpolate colors” button, then click auto and then apply. B1 should be around ~0.02 for the trees. The range it gives will be for the entire ortho so roads and other vegetation will increase the range so I suggest zooming into the ortho so you can see approximately what values correspond to trees. Repeat for B4 (green) and B10 (NIR). If the values are too large or small, i.e. B10 (NIR) > 1, choose another calibration panel, delete the ortho, recalibrate, remake the ortho and recheck the values. 
  -**Note** these values are general estimates of what a “tree” should look like however, different vegetation will have different ranges, for example values for cedar may be a little lower compared to Douglas fir.
  
**General benchmarks:**

  - B1 should be around ~0.02.
```{r b1-check-ref-agisoft, echo=FALSE, fig.align='center',out.width="100%", fig.cap = "Raster calculator tool in Metashape to look at reflectance values of trees for band 1 (Blue - 444nm) prior to exporting the orthomosaic."}
knitr::include_graphics(here("Photos_&_gifs\\reflectance_values_metashape.png"))

```

  - B4 should be around ~0.06
```{r b4-check-ref-agisoft, echo=FALSE, fig.align='center',out.width="100%", fig.cap = "Reflectance values of trees for band 4 (Green - 560nm)."}
knitr::include_graphics(here("Photos_&_gifs\\b4_reflectance_values_metashape.png"))

```

  - B10 should be around ~0.3 – 0.5, but shouldn’t exceed 0.7
```{r b10-check-ref-agisoft, echo=FALSE, fig.align='center', out.width="100%", fig.cap = "Reflectance values of trees for band 10 (NIR - 842nm)."}
knitr::include_graphics(here("Photos_&_gifs\\b10_reflectance_values_metashape.png"))

```

14.	Normalize and export – Script 5 minutes.

  - Run the closing script for each multispectral sensor after entering the variables into the scripts.  [MS](https://github.com/owaite/GenomeBC_BPG/blob/main/Scripts/BPG_closing_MS.py), [MS_P](https://github.com/owaite/GenomeBC_BPG/blob/main/Scripts/BPG_closing_MS_.py) these scripts will:
    - Normalize the 16bit pixel values for the MS and MS_P. **Note** We do not recommend pan sharpening, but if this is part of your workflow you would do it now as per the micasense knowledge base [here](https://support.micasense.com/hc/en-us/articles/4416696717847-Pan-sharpening-processing-data-from-Altum-PT-and-RedEdge-P-cameras-in-Agisoft-Metashape).
    - Import the 15m buffered shape of the trial and export the orthomosaic clipped to this shape in NAD83 to the project specific folder. **Note** Importing and clipping the orthomosaic will reduce the size of the export and help speed up further processing.
      
The following are helpful Agisoft processing references:

  - [Agisoft Metashape User Manual: Professional Edition, Version 2.1.](https://www.agisoft.com/downloads/user-manuals/)
  
  - [USGS_ Processing Coastal Imagery With Agisoft Metashape Professional Edition](https://pubs.usgs.gov/publication/ofr20211039)
  
  - [Micasense - Process MicaSense sensor data in Agisoft Metashape](https://support.micasense.com/hc/en-us/articles/360002693373-Process-MicaSense-sensor-data-in-Agisoft-Metashape)
  
  - [Processing Multi-spectral Imagery with Agisoft MetaShape Pro](https://pressbooks.bccampus.ca/ericsaczuk/)

The knowledge base at Agisoft also has many other useful articles.
