---
title: "Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials"
author: "Jake King*, Olivia Waite, Miriam Isaac-Renton, Nicholas C. Coops, Samuel Grubinger, Liam Irwin, Lise Van Der Merwe, Jon Degner, Alex Liu"
output: bookdown::gitbook
---

# Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials {-}

```{r echo=FALSE,out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\Cover_photo_crowns.png")
```

Date last updated: `r Sys.Date()`


*Jake King^1^, Olivia Waite^1^, Samuel Grubinger^2^, Alex Liu^1^, Miriam Isaac-Renton^1^, Nicholas C. Coops^2^, Liam Irwin^2^, Lise Van Der Merwe^3^, Jon Degner^3^, Alvin Yanchuk^3^* 

^1^ Natural Resources Canada, Canadian Forest Services, Canadian Wood Fibre Center, 506 Burnside Road West, Victoria, British Columbia, V8Z 1MZ.  

^2^ Integrated Remote Sensing Studio, Faculty of Forestry, University of British Columbia,2424 Main Mall, Vancouver, BC V6T 1Z4, Canada.

^3^ BC Ministry of Forests, Cowichan Lake Research Station, 7060 Forestry Rd, Mesachie Lake, BC V0R 2N0.

## GitHub link {-}
Below is the link to the GitHub where you can access full scripts referenced in this document

**GitHub Link: ** [PARSER-GBC-GitHub](https://github.com/owaite/PARSER-GBC-GitHub.git)

```{r echo=FALSE, results='asis'}
# Define the image paths
img_paths <- c(
  "C:/Users/owaite/OneDrive - NRCan RNCan/Documents/GitHub/GenomeBC_BPG/Photos_&_gifs/IRSS.png",
  "C:/Users/owaite/OneDrive - NRCan RNCan/Documents/GitHub/GenomeBC_BPG/Photos_&_gifs/NRCan_english.PNG",
  "C:/Users/owaite/OneDrive - NRCan RNCan/Documents/GitHub/GenomeBC_BPG/Photos_&_gifs/GeneSolve.png",
  "C:/Users/owaite/OneDrive - NRCan RNCan/Documents/GitHub/GenomeBC_BPG/Photos_&_gifs/GenomeBC.jpg"
)

# Create the HTML for the images
html <- paste0(
  '<div style="display: flex; flex-direction: row; justify-content: center; align-items: center;">',
  paste0('<img src="', img_paths, '" style="height: 60px; margin: 0 10px;" />', collapse = ""),
  '</div>'
)

# Print the HTML
cat(html)
```


## Acknowledgements {-}
These guidelines were made possible thanks for project funding from Genome British Columbia’s GeneSolve program, the Canadian Forest Service’s Fibre Solutions, 2 Billion Tree programs, and Assistant Deputy Minister’s Innovation Fund. For administrative assistance, we thank Adam Dick, Olivier van Lier, Marlene Francis, Annick Pelletier, Lise Carron, Guy Smith and Amélie Roberge. For practical input, we thank Bill Lakeland, Alec Wilson, Eric Saczuk, Keenan Rudichuk and David Huntley. 

## How to cite this report:{-}

<!--chapter:end:index.Rmd-->

# Range of Sites Assessed

## Jordan River
```{r, echo = FALSE}
library(knitr)
library(kableExtra)

# Data as a dataframe
data <- data.frame(
  `Western Red cedar (Cw)` = c("[Cw_East](https://projects.spexigeo.com/cw-jr-east/map/1bf9b0e2-21b5-4e96-a96c-033db45424fd) – planted in 2000, 3980 live trees on 4480 positions ",
                               "[Cw_Sandcut](https://projects.spexigeo.com/cw-jr-sandcut/map/b4d345ed-a69a-43bf-b622-1431504ff041) – planted in 2012, 3500 live trees on 3841 positions"),
  `Douglas fir (Fdc)` = c("[Fdc_East](https://projects.spexigeo.com/fdc-east-gca/map/01275e24-4077-446a-9d31-f2c1f55dbb8e) – planted in 2003, 1612 live trees on 3019 positions ",
                          "[Fdc_W45](https://projects.spexigeo.com/fdc-jr-w45/map/86e4f176-d3d8-466f-895b-5ab21b396ca5) – planted in 2019, 2076 live trees on 2494 positions")
)

# Create the table and center it
kable(data, col.names = c("Western Red cedar (Cw)", "Douglas fir (Fdc)"))

```


## Powell River
```{r, echo = FALSE}
library(knitr)
library(kableExtra)

# Data as a dataframe
data <- data.frame(
  `Western Red cedar (Cw)` = c("[Cw_Rainbow](https://projects.spexigeo.com/cw-rainbow/map/884b78af-30b7-4158-93a4-7516c5c2568b) – planted in 2000, 3674 live trees on 4745 positions"),
  `Douglas fir (Fdc)` = c("[Fdc_Canoe](https://projects.spexigeo.com/9536b041-e269-4d4d-a02a-9311bda57ae8/map/dd07addc-a47c-485b-9bce-5b35f0508611) – Planted in 1999, 1739 live trees on 3164 positions")
)

# Create the table and center it
kable(data, col.names = c("Western Red cedar (Cw)", "Douglas fir (Fdc)"))

```
## 2024 Sites
```{r, echo = FALSE}
# Data as a dataframe
data <- data.frame(
  `Douglas fir (Fdc)` = c("[Big Tree GCA](https://projects.spexigeo.com/fdc-big-tree-gca/map/0c1d3c05-63ff-46a9-b7ba-b85fd3a429a8) – Planted in 2003, 2229 live trees on 2900 positions"),
  `Douglas fir (Fdc)` = c("[Hillcrest GCA](https://projects.spexigeo.com/fdc-hillcrest-gca/map/9a13807e-55ed-4993-b70a-f3a6697c5d7b) – Planted in 2003, 1514 live trees")
)

# Create the table and center it
kable(data, col.names = c("Douglas fir (Fdc)", "Douglas fir (Fdc)"))

```

<!--chapter:end:0_Sites.Rmd-->

# Preflight Planning

## Achieving positional accuracy on multi-temporal and multi-sensor data collections 

### Kinematic processing: RTK/PPK 

### Ground Control Points (GCP) 

```{r GCP-figure, echo=FALSE, out.width="100%", fig.cap = "Aerial view of a GCP at the 12-year old Western redcedar Sandcut site taken with the P1 sensor."}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\Tree_tops_GCP.png")
```

### Absolute and Relative Reference with Precise Point Positioning (PPP) 

### Terrain Following on Sites with Elevation Change 

```{r terrain-following-figure, echo=FALSE, out.width="100%", fig.cap = "Terrain following on variable topography."}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\Terrain_following.PNG")
```

## Site Selection Considerations



**Ideal sites:**

```{r ideal-site-figure, echo=FALSE, out.width="100%", fig.cap = "Aerial view of a 5 year old Douglas fir trial."}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\ideal_sites.png")
```

**Effects of topography: Warning to take extra care**

Below Figure \@ref(fig:GCP-elevation) 

```{r GCP-elevation, echo=FALSE, fig.show="hold",fig.align='center', out.width="50%", fig.cap = "Effect of brushing on crown visibility. Left: GCP alignment on flat ground (Error ~4cm). Right: GCP alignment on ground with elevation change (Error ~28cm)"}
# Include the pre-brushing and post-brushing images
knitr::include_graphics(c(
  "C:/Users/owaite/OneDrive - NRCan RNCan/Documents/GitHub/GenomeBC_BPG/Photos_&_gifs/GCP_flat_ground.png",
  "C:/Users/owaite/OneDrive - NRCan RNCan/Documents/GitHub/GenomeBC_BPG/Photos_&_gifs/GCP_elevation_change_terrain.png"
))
```

<!--
```{r echo=FALSE, results='asis'}
# Define the image paths
img_paths <- c(
  "C:/Users/owaite/OneDrive - NRCan RNCan/Documents/GitHub/GenomeBC_BPG/Photos_&_gifs/GCP_flat_ground.png",
  "C:/Users/owaite/OneDrive - NRCan RNCan/Documents/GitHub/GenomeBC_BPG/Photos_&_gifs/GCP_elevation_change_terrain.png"
)

# Define the captions
captions <- c(
  "a. GCP alignment on flat ground (Error ~4cm)",
  "b. GCP alignment on ground with elevation change (Error ~28cm)"
)

# Create the HTML for the images with captions
html <- paste0(
  '<div style="display: flex; justify-content: center; gap: 20px;">',  # Centered horizontally with a gap between
  paste0(
    '<div style="text-align: center;">',
    '<img src="', img_paths, '" style="height: 350px; margin: 0 10px;" />',
    '<p style="margin: 0;">', captions, '</p></div>', collapse = ""  # Captions are directly below the images
  ),
  '</div>'
)

# Print the HTML
cat(html)

```
-->











Below Figure \@ref(fig:brushingEffect) shows an example of the effect of brushing on crown visibility at a moist, high-productivity common-garden trial.

```{r brushingEffect, echo=FALSE, fig.show="hold",fig.align='center', out.width="50%", fig.cap = "Effect of brushing on crown visibility. Left: early August prior to brushing. Right: late August post brushing"}
# Include the pre-brushing and post-brushing images
knitr::include_graphics(c(
  "C:/Users/owaite/OneDrive - NRCan RNCan/Documents/GitHub/GenomeBC_BPG/Photos_&_gifs/pre_brush.jpg",
  "C:/Users/owaite/OneDrive - NRCan RNCan/Documents/GitHub/GenomeBC_BPG/Photos_&_gifs/post_brush.jpg"
))
```



## Drone and Sensors: What will I need to purchase and how much will it cost?

### Hardware Costs




Table \@ref(tab:droneTable)
```{r droneTable, echo = FALSE}
library(knitr)
library(kableExtra)

data <- data.frame(
  Hardware  = c("*Zenmuse P1, RGB",
                "Micasense RedEdge-MX Dual",
                "*Micasense RedEdge-P Dual (Panchromatic)",
                "Zenmuse H20T, Thermal and RGB",
                "Zenmuse L1, LiDAR and RGB",
                "*Zenmuse L2 LiDAR and RGB",
                "DJI Matrice (M)300 RTK",
                "*DJI Matrice (M)350 RTK",
                "*DJI TB65 batteries",
                "DJI DRTK2",
                "*Emlid RS3",
                "*Emlid RS3"
  ),
  Purpose = c("Sensor",
              "Sensor",
              "Sensor",
              "Sensor",
              "Sensor",
              "Sensor",
              "Drone",
              "Drone",
              "Drone batteries",
              "GNSS receiver",
              "GNSS receiver",
              "GNSS receiver"
  ),
  Cost = c("$9,000",
           "$16,000",
           "$22,500",
           "$13,350",
           "$11,600",
           "$16,660",
           "$12,000",
           "$13,500",
           "$1,910",
           "$4,200",
           "$3,600",
           "$3,600"
  ),
  Detail = c(
  "Positional accuracy - Horizontal: 3 cm, Vertical: 5 cm. Ground Sampling Distance (GSD) - 1cm at 80 m elevation",
  "Ten band multispectral camera, no longer in production",
  "Ten band multispectral camera with Panchromatic sharpening",
  "Thermal and RGB sensor",
  "LiDAR sensor released in 2020",
  "LiDAR sensor released in 2023, now with 5 returns and better accuracy",
  "2020 released DJI enterprise drone with RC, intelligent battery case and one set of batteries",
  "2023 released DJI enterprise drone with RC plus, intelligent battery case and one set of batteries",
  "Pair of the newer model M300/350 batteries. Good for one approximately 30-minute mapping flight including safety margins.  At least 4 pairs are recommended for continuous flight when paired with a small generator",
  "GNSS receiver for use as base to stream RTK to drone",
  "GNSS receiver for use as base to stream RTK to drone and rover",
  "GNSS receiver for use as rover for centimeter precise GCP or stem mapping"
  )
  
  
)
# Create the table and center it
kable(data, col.names = c("Hardware", "Purpose","Cost","Detail"),caption = 'A breakdown of drone and sensor costs, where newer versions are available, both are listed.  The * would be a “wishlist” full set up.  This is just the major purchases and doesn’t include general field equipment.')

```



Table \@ref(tab:Untested-droneTable) are some untested options:


```{r Untested-droneTable, echo = FALSE}
data <- data.frame(
  Hardware  = c("DJI Mavic 3 Multispectral0",
                "Senterra 6X",
                "Micasense Altum PT"
  ),
  Purpose = c("Sensor, Drone",
              "Sensor",
              "Sensor"
  ),
  Cost = c("$5,945",
           "$17,120",
           "$25,950"
  ),
  Detail = c("RTK capable drone, RGB and 4 multispectral wavelengths",
             "RTK capable, 5 multispectral plus RGB, potential to customize wavelengths, but extra price unknown",
             "Thermal, and multispectral panchromatic, RTK capable sensor"
  )
)
# Create the table and center it
kable(data, col.names = c("Hardware", "Purpose","Cost","Detail"), caption = 'Options that we would like to test in the future but have not yet had the chance to.')
```

### Drone Training in Canada

## 

<!--chapter:end:0_Preflight_Planning.Rmd-->


# Data Collection and Flights

## Hardware - DJI Matrice 3000 RTK (M300)

```{r M300-flying, echo=FALSE, out.width="100%", fig.cap = "DJI Matrice 300 RTK in flight over one of our Vancouver Island feild sites, taken by Alec Liu with a Mavic3."}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\M300_flying.png")
```

A complete SOP for mapping flights with the M300 was developed in-house and can be found in the [SOPs folder](https://github.com/owaite/GenomeBC_BPG/tree/main/SOPs) on GitHub. Transport Canada (TC) regulations require logging all flights and to have the logs on-hand when flying. We have developed a simple spreadsheet that logs the details required by TC and other useful details for each flight. When undertaking a project with repeated data collection campaigns, we would recommend developing a similar approach to organizing these data.  A copy of the one we use is available within the [Example Documents](https://github.com/owaite/GenomeBC_BPG/tree/main/Example_Documents) folder on GitHub. 




## Workflow: Site Reconnaissance 
Ahead of the first data collection flights we follow the reconnaissance workflow outlined in Figure \@ref(fig:Reconnasissance-flowDiagram)


```{r Reconnasissance-flowDiagram, echo=FALSE, out.width="100%", fig.cap = "Reconnaissance workflow."}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\Reconnasissance_flowDiagram.PNG")
```


## Data Collection: Flight Planning  


### Flight planning theory 

Figure \@ref(fig:effective-overlap) outlines the difference in ground overlap and canopy overlap.  We have developed a calculator in which you enter the flight altitude, overlaps, and canopy height and it will output the canopy overlap.  You can then adjust the inputs until you get the required overlap. It is available in the [Example Documents](https://github.com/owaite/GenomeBC_BPG/tree/main/Example_Documents) folder on GitHub. Figure \@ref(fig:effective-overlap) below is an example calculation detailed in the next paragraph.    

```{r effective-overlap, echo=FALSE, out.width="100%", fig.cap = "Effective overlap calculation."}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\effective_overlap.png")
```





### Flight planning with DJI Pilot 

Figure \@ref(fig:P1-flightPlan)

```{r P1-flightPlan, echo=FALSE, out.width="100%", fig.cap = "Flight planning a P1 flight on the remote controller."}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\P1_flight_plan.png")
```

## Data Collection: Sensors, Parameters, and Ideal Conditions 
### MicaSense: 10-Band Spectral Sensor for Vegetative Indices 

Figure \@ref(fig:DLS-Micasense-connection)

```{r DLS-Micasense-connection, echo=FALSE, out.width="100%", fig.cap = "Micasense RedEdge-MX Dual and the DLS2 mounted on the M300. Here we are connecting the DLS2 to the Micasense camera."}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\DLS2_Micasense_connection.png")
```
**Sensor Specifications:** 

- **Camera Type:** MicaSense Dual cameras 

- **Image Format:** .tif with a suffix indicating the band (e.g., _1, _2) 

- **Bands/Wavelengths:** 

```{r MS-Table, echo = FALSE}
data <- data.frame(
  Band_Name  = c("Blue",
                 "Green",
                 "Red",
                 "Red Edge",
                 "NIR",
                 "Panchromatic",
                 "Coastal Blue",
                 "Green",
                 "Red",
                 "Red Edge",
                 "Red Edge"
                 
  ),
  Center_Wavelength = c("475",
                        "560",
                        "668",
                        "717",
                        "842",
                        "634",
                        "444",
                        "531",
                        "650",
                        "705",
                        "740"
  ),
  Bandwidth = c("32",
                "27",
                "14",
                "12",
                "57",
                "463",
                "28",
                "14",
                "16",
                "10",
                "18"
  ),
  Camera = c("Red",
             "Red",
             "Red",
             "Red",
             "Red",
             "Red",
             "Blue",
             "Blue",
             "Blue",
             "Blue",
             "Blue"
  )
  
  
)
# Create the table and center it
kable(data, col.names = c("Band Name", "Center Wavelength (nm)","Bandwidth (nm)","Camera"),caption = 'Band names, center wavelengths, and bandwidths collected by the Red and Blue cameras within the Micasense Dual camera systems. Panchro* is only available in the pan chromatic models.')

```

- **Image Capture Rate:** 

  - The default setting for timed intervals is one image capture every two seconds, with simultaneous capture for all bands. 

  - The capture rate can be adjusted to a maximum of one image per second 

- **Output:** 5-6 folders per camera (10-12 thousand images) for a 25–30-minute flight covering a 1.5-2 Ha site 

- **GSD:** 2.5-4cm at 40m above canopy.  1.5-2cm when processed pansharpened with the panchromatic band.  

**Flight Parameters: **

- **Flight Elevation: **

  - 40 m above canopy 

- **Flight Speed:** ~2 m/s (slower speeds reduce motion blur, especially in windy conditions) 

- **Image Overlap:** Extra high to ensure sufficient coverage and allow for exclusion of poor-quality images during processing. 

  - Front: ~86% (timed interval is the limiting factor) 

  - Side: ~86%  

- **Margins:** 10 m around the site perimeter is sufficient for flight planning due to the wide-angle shot of the MS camera 

- **Flight Time:** Approximately 30 minutes for a 1.5-2 Ha area site.  This allows for a single battery flight, which helps to maintain even light conditions. 

**Best Practices: **

- **Weather Conditions:** Avoid flying in any precipitation as MicaSense cameras are unprotected from moisture, with exposed data and power connections. 

- **Lighting:** Conduct flights within two hours of solar noon to minimize shadowing in the imagery. 

  - The DLS calibration helps to reduce the effect changing light conditions, but extreme changes in light conditions will leave residual effects that are difficult to process out. Figure \@ref(fig:drone-variable-light) below shows a flight in variable conditions (left) in which the calibration has failed to correct the changing light conditions.  The flight on the right was flown on an ideal day with even diffuse light.  

Figure \@ref(fig:drone-variable-light)

```{r drone-variable-light, echo=FALSE, fig.show="hold",fig.align='center', out.width="50%", fig.cap = "Left: MS orthomosaic in variable light. Right: MS orthomosaic in even diffuse light."}
knitr::include_graphics(c("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\variable_light.png",
                        "C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\diffuse_light.png"
                        )
)
```

  - The ideal flight as in Figure \@ref(fig:drone-variable-light) would be at solar noon in evenly overcast (diffuse) light conditions. 

  - **Wind Conditions:** Lower wind speeds yield better results; however, the impact of wind will vary depending on species and site-specific factors.  Consider flying at a higher altitude and overlap in windier conditions. 

  - **Flight Elevation Considerations:** While lower flight elevations produce higher resolution images, they increase flight time, making calibration more challenging. 

Click [here](https://github.com/owaite/GenomeBC_BPG/tree/main/Example_Documents) to access the Micasense field SOP housed on GitHub. 

[Best practices: Collecting Data with MicaSense Sensors – MicaSense Knowledge Base](https://support.micasense.com/hc/en-us/articles/224893167-Best-practices-Collecting-Data-with-MicaSense-Sensors) is an important resource.


### Zenmuse P1: High-Quality Natural-Colour (RGB) Imagery 

Figure \@ref(fig:P1-RGB)
```{r P1-RGB, echo=FALSE, fig.show="hold",fig.align='center', out.width="50%", fig.cap = "These images are the same area, the left was taken with the above parameters with the P1 before greenup. The right was taken with the wide angle RGB on the Zenmuse H20T, a lower resolution camera, mid-August."}
knitr::include_graphics(c("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\P1_pre_greenup.png",
                        "C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\RGB_mid_Aug.jpg")
                        )
```






















```{r RC-L1, echo=FALSE, out.width="100%", fig.cap = "L1 LiDAR screenshot from the remote controller during a flight at Big Tree Creek site on July 7, 2024. On the left is the RGB image instantaneously acquired for the frame and the right shows the LIDAR data acquisition from the scanner as it acquired the data up the frame."}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\RC_L1.png")
```






<!--chapter:end:0_Data_Collection_Flights.Rmd-->

# Data Processing: Software and hardware costs and options

## Software costs
We chose open-source options where we could, but there are still significant software costs. Table \@ref(tab: software-cost) outlines cost and explores options.

```{r software-cost, echo = FALSE}
data <- data.frame(
  Software  = c("DJI Terra Pro",
                "DJI Terra Pro"
                 
  ),
  Purpose = c(
  ),
  Cost = c(
  ),
  Options_notes = c(
  )
  
  
)
# Create the table and center it
kable(data, col.names = c("Software", "Purpose","Cost","Options - notes"),caption = 'Purpose, cost, and notes on software that we have used for this project.')

```

<!--chapter:end:0_Data_Processing.Rmd-->

# Photogrametric Processing

Figure \@ref(fig: aligned-agisoft)

```{r aligned-agisoft, echo=FALSE, out.width="100%", fig.cap = "A dense point cloud processed in Agisoft Metashape with the camera locations turned on and disabled flight trajectories shown."}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\Aligned_agisoft.png")
```



```{r metashape-pref, echo=FALSE, fig.show="hold",fig.align='center', out.width="50%", fig.cap = "Recommended Metashape preferences setup under the Advanced tab."}
# Include the pre-brushing and post-brushing images
knitr::include_graphics(c(
  "C:/Users/owaite/OneDrive - NRCan RNCan/Documents/GitHub/GenomeBC_BPG/Photos_&_gifs/metashape_pref.png",
  "C:/Users/owaite/OneDrive - NRCan RNCan/Documents/GitHub/GenomeBC_BPG/Photos_&_gifs/metashape_pref_2.png"
))
```

```{r cal-ref-agisoft, echo=FALSE, out.width="100%", fig.cap = "."}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\cal_ref_metashape.png")
```



```{r align-photo-agisoft, echo=FALSE, out.width="100%", fig.cap = "."}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\align_photos_metashape.png")
```
## Processing Orthomosaics

## Shadow Masks

Pixels containing shadows and openings in the canopy can introduce error in vegetation indices as reflectance values are often lower in shaded pixels (Malenovský et al., 2013; Zhang et al., 2024).  Masking out shaded pixels and openings within the crown is necessary to calculate accurate vegetation indices. We found masking with NIR reflectance to be a consistent and effective approach for masking out shadows and gaps. We use the NIR reflectance band (842nm) from the Micasense RedEgde-MX Dual. NIR thresholds are determined using the shape of the NIR distribution. If the NIR distribution is bimodal, the threshold is set to the local minimum, whereas if the distribution is unimodal, the threshold is set to the local maximum (Chen et al., 2007; D’Odorico et al., 2021; Otsu et al., 2019).

Below we define a function *find_local_min* that takes a list of first derivatives and returns a dataframe containing:

- **neg_value**: The value of the first derivative directly before the minimum (i.e. the negative derivative value to the left of the minimum)

- **pos_value**: The value of the first derivative directly after the minimum (i.e. the positive derivative value to the right of the minimum)

-	**pos_def**: The summation of the positive gradient values (a.k.a. slope values) in the first 15 gradient values following the local minimum. This value provides an idea of how great the increase in slope is after the local minimum.

-	**neg_def**: The summation of the negative gradient values in the first 15 gradient values preceding the local minimum. This value provides an idea of how great the decrease in slope is before the local minimum.

-	**Index**: Index position of the negative gradient value bordering the local minimum. This is the index of our threshold value since we do not get a true zero slope value at the local minimum but rather a change from a very small negative gradient value to a very small positive gradient value.

-	**definition**: The summation of the absolute value of pos_def and neg_def values. The greater this number, the more defined the minimum. 
The above parameters were added to describe each minimum found by the function so that the true local minimum could be filtered for.

This function will be used in the next few steps to isolate minimums in the NIR distribution.
```{r,eval=FALSE, echo=TRUE}
# This function finds local minimums and ranks how defined they are by the 'definition' attribute
find_local_min <- function(values) { #input "values" is a list of 1st derivatives of NIR values
  #initializing variables
  neg_slope <- numeric() 
  pos_slope <- numeric()
  index_of_closest <- numeric()
  definition = numeric()
  neg_sum = numeric()
  pos_sum = numeric()
  
  k <- 1 #initializing iterator 
  
  for (i in 2:length(values)) { #skip index 1 so following "values[i-1]..." can properly index
    #print(i)
    if (values[i - 1] < 0 & values[i] >= 0 & i > 15) { 
      #finds a change from negative to positive 1st derivative (local min)
      #i>15 because local min will not be in first 15 values and this stops an error occurring where a local min is found in the first 15 values and the def_positive indexing does not work
      def_positive <- c(values[i:(i+15)]) # vector of next 15 gradient values
      def_pos <- def_positive[def_positive > 0] #only taking positive 1st derivative values (aka positive slopes)
      pos_sum[k] <- sum(def_pos) #adding up the positive 1st derivative values
      
      def_negative <- c(values[(i - 15):i]) # vector of the 15 values to the left of the local min (negative 1st derivatives)
      def_neg <- def_negative[def_negative < 0] #only taking the negative slopes in the list
      neg_sum[k] <- sum(def_neg) #adding up negative values
      
      neg_slope[k] <- values[i - 1] #1st derivative value at i-1 (right before sign change from neg to pos)
      pos_slope[k] <- values[i] #1st derivative at i (at the switch from neg to positive)
      index_of_closest[k] <- i - 1 #index value of the last neg 1st derivative before the local min
      definition[k] <- sum(abs(def_neg), abs(def_pos)) # higher the value, more pronounced the local min
      k <- k + 1
    }
  }
  return(data.frame(neg_value = neg_slope, pos_value = pos_slope, Index = index_of_closest, definition = definition, neg_def = neg_sum, pos_def = pos_sum))
}
```


To begin, we load the necessary packages, the multispectral orthomosaic that contains the NIR (842nm) band, and the delineated crowns shapefile and create the folder where the shadow mask will be saved to:

```{r,eval=FALSE, echo=TRUE}
#Required Packages
library(ggplot2) # to plot 
library(terra) # to work with the orthomosaics (rasters)
library(dplyr) #for data manipulation
library(tidyverse) # for data manipulation
library(sf) # to work with the delineated crown polygons
library(pracma) # for gradient/ derivative function
library(LaplacesDemon) # is.multimodal function

# Setting directory 
dir = "Change to match your folder strucutre" #directory where shadow mask folder will be made. This dir is also used in path names for the orhtomosaics. Change up paths throughout the code to call your data.

# Reading in the multispectral orthomosaic
ms_temp = rast(list.files(paste0(dir, "metashape\\3_MS_ORTHO\\"), pattern = ".*MS_Calibrated.*_bestPanel.tif$", full.names = TRUE)) # here we read in the ortho that is in the set directory and contains "MS_Calibrated" in the name and end in "_bestPanel.tif". We had multiple orthos in this folder, so did this to ensure the proper one was called. Change to match your ortho name, or remove the pattern if you only have one ortho in the defined path.
  
# Reading in the shapefile containing delineated crowns and buffering inward by 5cm to limit any mixed pixels from neighboring vegetation
dir_crowns <- "D:\\Sync\\Fdc_PR_Canoe\\Crowns.shp" #path to crowns shapefile
pols_spat = st_read(paste0(dir_crowns)) %>% # reading in crown shp
  filter(!st_is_empty(.)) %>% #removing empty
  st_buffer(dist = -.05) %>% #buffering inward by 5cm
  vect()

# Creating a folder for shadow masks
Nir_shadow_folder_baseName <- "NIR_shadow_mask_localMinOrMax" #name of the folder shadow masks will be written to. We have the folder name defined outside the folder creation step below so that we could change the folder name once without having to change it throughout the code.
if (!dir.exists(paste0(dir, Nir_shadow_folder_baseName,"\\"))) {
  dir.create(paste0(dir,Nir_shadow_folder_baseName,"\\"))
  }

```

Next, the NIR band is selected from the multispectral orthomosaic. Here you can choose to:

1) Crop the mulispectral othomosaic to the extent of the crowns shapefile. We have found this to be a good option for mature sites with minimal exposed ground.

2) Mask the mulispectral othomosaic to the individual crowns. We have found this to be a good option for younger sites with lots of ground exposure. 

Below we crop the raster to the extent of the polygons and reformat from a raster to a vector of values:

```{r,eval=FALSE, echo=TRUE}
#Create a vector of near-infrared (NIR) values from the 10th multispectral band (842nm)
NIR = ms_temp[[10]] %>% #isolating the NIR (842nm) band
  crop(pols_spat) %>% #cropping to the extent of the crown polygon shp
  clamp(upper = 50000, values = FALSE) %>% 
  as.vector()

```

Density of the NIR values is calculated and plotted, after removing NA values.
```{r,eval=FALSE, echo=TRUE}
NIR_na <- na.omit(NIR) #remove NA values from the NIR vector
density_values <- density(NIR_na) # calculates density of NIR values

# Plot to see the distribution of NIR values
plot(density_values) # check to see the distribution of NIR values (ie a visual check for whether or not a local min exists)
  
```


At this point the threshold can be estimated manually from the graph however, this is not a feasible method when you have many data acquisitions, nor is it the most accurate method. Below we describe the method we used to create shadow masks for many acquisitions withouth having to visually inspect the NIR distributions. To computationally find the threshold we first need to decipher whether the NIR distribution is unimodal (one peak) or multimodal (more than one peak). To do so we use the *is.multimodal* function from the LaplacesDemon package. 

If the distribution was multimodal, we found the local minimum by:

1) calculating the first derivatives fo the NIR density values and storing the derivatives in dy_dt

2) applying the *find_local_min* function to the list of first derivatives to find minimums

3) filtering out small minimums that are not true minimums but rather dips on a larger slope

4) Identifying false multimodals and assigning the threshold value to be the local maximum

5) Identifying the largest minimum (a.k.a. the local minimum) and setting that as the threshold value

If the distribution was not multimodal (and therefore unimodal), we found the local maximum by:

1) locating the NIR value that corresponds to the greatest density of values in the NIR distribution

**Note:** for each method there are **mode** and **thresh_name** variables defined. These variables are used to annotate the histrogram made in the next step 

```{r,eval=FALSE, echo=TRUE}
if(is.multimodal(NIR_na)){ # if the NIR distribution is multimodal, continue to the below steps
    
    dy_dt <- pracma::gradient(density_values$y) #list of first derivatives of NIR vector
    
    zeros <- find_local_min(dy_dt) # Finds index locations where slope switches from neg to post (local min) and ranks the intensity of each local min
    zeros_filtered <- zeros[(zeros$pos_def > 0),] # Filters rows with pos_def > 0, filtering out small minimums on negative slopes (not true local mins)
    
    if(nrow(zeros_filtered)==0){#if empty, then detected a false multimodal distribution and defaulting to max as threshold value
      print("It is a false multimodal")
      max_density = max(density_values$y, na.rm = TRUE)
      threshold = density_values$x[which(density_values$y == max_density)]
      mode <- "Unimodal (False Multi)"
      thresh_name <- "LocalMax"
      
      }else{ 
      zeros_local_min <- zeros_filtered[which.max(zeros_filtered$definition), ] #isolating the largest local min
      # x_zeros <- density_values$x[zeros_local_min$Index] #selecting NIR (aka x_mid) value that corresponds to the index value of the most defined local min 
      threshold <- density_values$x[zeros_local_min$Index] #selecting NIR (aka x_mid) value that corresponds to the index value of the most defined local min 
      
      # Catching cases of "inf" returns:
      if (threshold <= 0.7 & threshold > 0){#setting limits on the threshold to remove any thresholds from the tails of the distribution
        print(threshold)
        mode <- "Multimodal"
        thresh_name <- "LocalMin"
        
        }else{
        print("Multimodal with org thresh > 0.7") #This output usually indicates an error in the function, make sure to look at the NIR distribution to confirm this is in fact correct or if the code needs modification for a special case
        #This often indicates a false multimodal as well, and therefore the NIR value with the max frequency in the NIR distribution with be used as the threshold
        max_density = max(density_values$y, na.rm = TRUE)
        threshold = density_values$x[which(density_values$y == max_density)]
        mode <- "Unimodal (False Multi with org thresh > 0.7)"
        thresh_name <- "LocalMax"
      }
    }
  }else{ #if the NIR vector is NOT multimodal, continue to the below steps
    #Finding NIR value with the greatest frequency in the distribution - this max value will be the threshold for unimodal distributions 
    max_density = max(density_values$y, na.rm = TRUE)
    threshold = density_values$x[which(density_values$y == max_density)]
    print(threshold)
    thresh_name <- "LocalMax"
    mode <- "Unimodal"
  }
```

Next, we plot a histogram of the NIR values and annotate it to contain:

- a vertical line at the threshold

- the threshold value

- the mode (i.e. unimodal verse mulitmodal)

```{r,eval=FALSE, echo=TRUE}
(hist = NIR %>% #plotting a histogram of NIR values with a vertical red line for the defined threshold value
    as_tibble() %>% 
    ggplot() +
    geom_histogram(aes(x = NIR), bins = 150) +      
    geom_vline(xintercept = threshold, color = "red3") +
    labs(title = paste0(mode, " , threshold: ",round(threshold, digits = 2)))+
    theme_bw()+
    theme(panel.grid.major = element_blank(),           
          panel.grid.minor = element_blank(),
          plot.title = element_text(size = 13,hjust = 0.75, vjust = -28)))
  
ggsave(hist, #saving out the plot
       filename = paste0(dir, Nir_shadow_folder_baseName,"\\", date_list[x], "_NIR_shadow_hist_localMin_orMax.jpeg"),
       device = jpeg,
       width = 8,
       height = 8)
```


```{r echo=FALSE, out.width="60%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\multimodal_hist_canoe_2022_08_05.jfif")
```

We then isolate the NIR band from the multispectral orthomosaic and filter the raster to set pixels with NIR values greater than the threshold to NA and those that are less than or equal to the threshold to 1.

```{r,eval=FALSE, echo=TRUE}
shadow_mask = ms_temp[[10]] #isolating the NIR band of the multispectral ortho
shadow_mask[shadow_mask > threshold] = NA #for NIR values > threshold, make them NA
shadow_mask[shadow_mask <= threshold] = 1 #for NIR values < or = to the threshold value, make them 1
  
# Writing out the shadow mask that has values of 1 for all pixels that will be masked out
terra::writeRaster(shadow_mask, paste0(dir, Nir_shadow_folder_baseName,"\\",  date_list[x], "_NIR_shadow_thresh",threshold ,"_",thresh_name, ".tif"),
                     overwrite = TRUE)
```


```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\shadow_mask_example_no_filter_2022_08_05.PNG")
```


Lastly, we apply the *clump* function to group adjacent pixels that represent shadows creating "clumps" a.k.a. shadow patches. These shadow patches are then converted into a dataframe containing a unique ID per shadow patch and the number of pixels within each patch. 

```{r,eval=FALSE, echo=TRUE}
# Grouping adjacent pixels with the same value (i.e. representing shadowed areas) into distinct patches or clumps
shadow_patches = raster::clump(raster::raster(shadow_mask), directions = 4) %>%
  rast()
  
# Summarizing the clumps obtained from the shadow patches raster. It contains two columns: 
#1. value: representing the unique ID of each clump:
#2. count: representing the number of pixels within each clump
clumps = data.frame(freq(shadow_patches))
 
```

Here we filter the shadow patches by a threshold to remove insignificant patches and write out the cleaned shadow mask for future use.

The threshold **num_pix** is calculated by:

- setting an initial area threshold of 0.02m^2 (200cm^2)
 
- dividing the 0.02m^2 threshold by the area of a pixel to determine the threshold number of pixels a shadow patch must have to not be filtered out

In our case, the resolution of the raster was just over ~3cm, giving us a threshold of ~23 pixels.

```{r,eval=FALSE, echo=TRUE}
# Calculating the threshold for the number of pixels that a clump must contain to be considered significant
# It is calculated based on the desired area threshold (200 cm²) divided by the area of a single pixel
num_pix = 0.02 / (res(shadow_patches)[1]^2) #0.02 represents 200cm² in meters
flecks = clumps[clumps$count > num_pix,] # remove clump observations with frequency smaller than the threshold
flecks = as.vector(flecks$value) # record IDs from clumps which met the criteria in previous step
  
new_mask = shadow_patches %in% flecks #keep clumps that have IDS in flecks
new_mask[new_mask == 0] = NA # make clumps that are zero, NA
  
#writing out a 'cleaned' shadow mask  
terra::writeRaster(new_mask, paste0(dir, Nir_shadow_folder_baseName,"\\",  date_list[x], "_NIR_shadow_thresh",threshold ,"_",thresh_name, "_mask2.tif"),
                     overwrite = TRUE)
```

Below you can see that in this case the filter did not result in a large change in the shadow mask as there were minimal small patches to begin with. Here the white areas in the filtered mask image on the right are areas that will not be masked given that the mask did not contain enough pixels. This will result in a less patchy shadow mask.

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\shadow_mask_example_filtered_2022_08_05.PNG")
```

## Crown-level Vegetation Indicies

Vegetation indices (VIs) are important tools for near-real time monitoring and function as proxies for health, productivity, structural changes, and stress.  

Below we describe a few commonly used VIs that will be calculated in this workflow: 

- **Normalized Difference Vegetation Index (NDVI)** is of the oldest vegetation indices established in vegetation monitoring and is used as a metric of greenness. However, due to saturation of the chlorophyll absorption peak around 660-680nm at moderately low chlorophyll levels, NDVI is often not able to tease out small differences between healthy plants (Sims & Gamon, 2002). To solve this issue, the red reflectance band can be replaced with a red edge reflectance band creating the normalized difference red edge index mentioned below. 

- **Normalized Difference Red Edge index (NDRE)** normalizes a reflectance band in the red edge to a reflectance band in the near-infrared region to estimate chlorophyll content. This index is a derivative of NDVI that is sensitive to shifts in chlorophyll content at high concentrations (Clevers & Gitelson, 2013; Evangelides & Nobajas, 2020). 

- **Chlorophyll Carotenoid Index (CCI)** is a proxy for the ratio of chlorophylls to carotenoids in pigment pools. It is often used as a metric for tracking the onset of the growing season and photosynthetic activity (Gamon et al., 2016).  

- **Photochemical Reflectance Index (PRI)** can be used as a proxy for xanthophyll pigment epoxidation or changes in bulk seasonal pigment pool ratios depending on the timescale of analysis. Over a scale of milliseconds to minutes PRI can isolate the photoprotective conversion of violaxanthin into antheraxanthin and zeaxanthin within the xanthophyll cycle by normalizing the 531nm reflectance band to the 560nm reference reflectance band. However, on the seasonal scale the 560nm reflectance band no longer acts as a reference for isolating changes in xanthophyll pigments since it changes with bulk pigment shifts. Hence, seasonal PRI meaurments are instead used as another proxy for the ratio of carotenoid to chlorophyll pigments and are used to show changes in photosynthetic activity (Wong et al., 2020). 

- **Red Edge (RE) slope** captures changes in chlorophyl content and structural health. Steep RE slopes generally indicate healthy vegetation while more gradual slopes often indicate stressed vegetation. This is due to the base of the RE slope sitting around a main chlorophyll absorption peak and the end of the RE slope sitting near the near-infrared (NIR) range. Hence, the more chlorophyll the lower the reflectance around the base of the RE slope and the greater the reflectance in the NIR, often attributed to healthy vegetative material, the steeper the overall slope of the RE (Sims and Gamon, 2002, Clevers and Gitelson)  

- **Green Chromatic Cordinate (GCC)** is a measure of green reflectance relative to the total reflectance in the visible light portion on the electromagnetic spectrum. It is a metric of greeness that is often used as a proxy for vegetation health (Reid et al., 2016). 

### VI Workflow 

Below are code chunks that take the [delineated crowns](09-Crown_delineation#Crown-delineation) and [multispectral orthomosaics](Photogrametric_processing#Processing Orthomosaics) and output a .rds file containing several popular vegetation indices at the crown-level. In this script we demonstrate how to calculate both mean and median values of vegetation indices per crown.

We begin by loading the necessary packages:

```{r,eval=FALSE, echo=TRUE}
library(terra) # to work with the orthomosaics (rasters)
library(dplyr) #for data manipulation
library(tidyverse) # for data manipulation
library(sf) # to work with the delineated crown polygons
library(exactextractr) # for the exact_extract function

```

Next, we read in the mulispectral orthomosaic and shadow mask. The shadow mask will work to remove shadowed pixels so they do not skew the values for the vegetation indices. See the section on [shadow masks](Photogrametric_processing#Shadow Masks) for a detailed workflow.

```{r,eval=FALSE, echo=TRUE}
# Load in multispectral orthomosaic 
ms_ortho = rast(list.files(paste0(dir, "metashape\\3_MS_ORTHO\\"), pattern = ".*MS_Calibrated.*_bestPanel.tif$", full.names = TRUE)) #path to multispectral ortho

# Reading in the NIR shadow mask
shadow_mask = rast(list.files(paste0(dir, Nir_shadow_folder,"\\"),  pattern = ".*_NIR_shadow.*_thresh.*_mask2.tif$", full.names = TRUE))# NIR shadow mask


# Below selects the root name of the multispectral ortho ie: "Name_of_multispectral_ortho" that will be used to name the multispectral shadow masked orthos created in the following steps
ms_ortho_name_root <- substr(names(ms_ortho)[1], 1, nchar(names(ms_ortho)[1]) - 2)

#names(ms_ortho)[1] grabs the name for band 1 of the ms_ortho: ie: "Name_of_multispectral_ortho_1", so that the substr function can selection the rootname as done above
  
```

To speed up processing, we mask the shadow mask to only contain areas inside delineated crowns.

The shadow mask is a raster containing values 1 (shadowed pixel) and NA (not shadowed pixel). The *mask* function below identifies pixels in the multispectral orthomosaic that align with shadowed pixels in the shadow mask and changes their value to NA. 

```{r,eval=FALSE, echo=TRUE}
# Mask shadow mask to delineated crowns:
shadow_mask <- terra::mask(shadow_mask, pols)
  
# Mask the multispectral ortho using the shadow mask
ms_mask <- terra::mask(ms_ortho, shadow_mask, maskvalues = 1, updatevalue = NA)

#writing out the shadow masked multispectral ortho
terra::writeRaster(ms_mask, paste0(dir, Nir_shadow_folder,"\\",ms_ortho_name_root,"_NirShadow_masked.tif"),
                     overwrite = TRUE)
  
```

The shadow masked multispectral orthomosaic is then masked to the delineated crowns polygon to speed raster calculations in the next step.

```{r,eval=FALSE, echo=TRUE}

# Mask shadow masked raster to delineated crowns
ms_mask <- terra::mask(ms_mask, pols)

# Write out the masked raster
terra::writeRaster(ms_mask, paste0(dir, Nir_shadow_folder,"\\",ms_ortho_name_root,"_NirShadow_ms_mask_to_pols.tif"),
                     overwrite = TRUE)

```

Here we calculate several vegetation indices using the multispectral orthomosaic from above with shadowed pixels and non-crown pixels removed.

```{r,eval=FALSE, echo=TRUE}
# List of band reflectances (R444 - R842) and index values that will be calculated per tree crown
rast_list = list(
  # reflectance values
  R444 = ms_mask[[1]],
  R475 = ms_mask[[2]],
  R531 = ms_mask[[3]],
  R560 = ms_mask[[4]],               
  R650 = ms_mask[[5]],               
  R668 = ms_mask[[6]],               
  R705 = ms_mask[[7]],               
  R717 = ms_mask[[8]],               
  R740 = ms_mask[[9]],               
  R842 = ms_mask[[10]],                              
    
  # Near-infrared greenness
  mDatt = (ms_mask[[10]] - ms_mask[[8]]) / (ms_mask[[10]] - ms_mask[[6]]),
  NDVI = (ms_mask[[10]] - ms_mask[[6]]) / (ms_mask[[10]] + ms_mask[[6]]), 
    
  # Chlorophyll 
  NDRE1 = (ms_mask[[10]] - ms_mask[[7]]) / (ms_mask[[10]] + ms_mask[[7]]),     
  NDRE2 = (ms_mask[[10]] - ms_mask[[8]]) / (ms_mask[[10]] + ms_mask[[8]]), 
  NDRE3 = (ms_mask[[10]] - ms_mask[[9]]) / (ms_mask[[10]] + ms_mask[[9]]), 
  EVI = (2.5 * (ms_mask[[10]] - ms_mask[[6]])) / (ms_mask[[10]] + (6 * ms_mask[[6]]) - ( 7.5 * ms_mask[[1]] + 1)),             
  GCC = (ms_mask[[4]]+ ms_mask[[3]]) / (ms_mask[[1]] + ms_mask[[2]] + ms_mask[[3]]+ms_mask[[4]] + ms_mask[[5]] + ms_mask[[6]]),             
  
  # Carotenoids, waxes
  ARI = (1 / ms_mask[[4]]) - (1 / ms_mask[[7]]),    
  EWI9 = (ms_mask[[6]] - ms_mask[[8]]) / (ms_mask[[6]] + ms_mask[[8]]),
    
  # Carotenoids             
  PRI = (ms_mask[[3]] - ms_mask[[4]]) / (ms_mask[[3]] + ms_mask[[4]]),             
  CCI = (ms_mask[[3]] - ms_mask[[5]]) / (ms_mask[[3]] + ms_mask[[5]]),             
    
  # Red edge             
  RE_upper = (ms_mask[[9]] - ms_mask[[8]]) / 23,             
  RE_lower = (ms_mask[[8]] - ms_mask[[7]]) / 12,             
  RE_total = (ms_mask[[9]] - ms_mask[[7]]) / 35
)
  
rast_all = rast(rast_list)
```


Below is an example of the resolution of the calculated vegetation indices. The image shows the chlorophyll carotenoid index (CCI) of a single crown across four summer time points. Greener pixels represent a higher ratio of chlorophylls to carotenoids. As you can see, the proportion of the crown with high CCI values increases from the May to July acquisitions, when productivity is likely at its highest, and decreases during the august acquisition suggesting lower productivity, likely due to end of summer drought conditions. 

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\CCI_timeseries.png")
```

Lastly, we use the *exact_extract* function to calculate crown-level statistics per vegetation index raster created above. Below we show an example of taking the mean and median crown values per index as well as count the number of pixels that were used for the index calculation. We do this as a data quality check so we can remove crowns that had very few pixels involved in their index calculation.

**Note:** the append_cols parameter should be set to a list of column names that exist in pols  (the shapefile of delineated crowns) that you would like to be appended to the dataframe containing vegetation indcies per tree crown. As you can see below we have attached many columns we found useful, however the only column necessary to append is a column in pols that represents a unique ID per tree crown.


```{r,eval=FALSE, echo=TRUE}

# Calculating Mean Index values per crown
df_spectral_mean = exact_extract(rast_all, pols, fun = "mean", append_cols = c("treeID","Edited","tag__","rep","blk","fam","fem"))
  
# Calculating the number of non-masked pixels used for index calculation
df_count = exact_extract(ms_mask[[1]], pols, fun = "count", progress = TRUE, append_cols = c("treeID","Edited","tag__","rep","blk","fam","fem"))
  
# Joining the mean index values df and the count values df
df_spectral_mean_count <- merge(df_spectral_mean,df_count, by = c("treeID","Edited","tag__","rep","blk","fam","fem"))
  
# Saving out mean crown values + non masked pixel count to an rds file
saveRDS(df_spectral_mean_count, paste0(dir_D,"\\",updated_metrics_folder,"\\", date_list[x], "_NIRshadowMask_MeanCrownSpectralIndices.rds"))

# Calculating Median Index values per crown
df_spectral_median = exact_extract(rast_all, pols, fun = "median", append_cols = c("treeID","Edited","tag__","rep","blk","fam","fem"))
  
df_spectral_median_count <- merge(df_spectral_median,df_count, by = c("treeID","Edited","tag__","rep","blk","fam","fem"))
  
# Saving out median crown values + non masked pixel count to an rds file
saveRDS(df_spectral_median_count, paste0(dir_D,"\\",updated_metrics_folder,"\\", date_list[x], "_NIRshadowMask_MedianCrownSpectralIndices.rds"))
```


References: 

Clevers, J. G. P. W., & Gitelson, A. A. (2013). Remote estimation of crop and grass chlorophyll and nitrogen content using red-edge bands on Sentinel-2 and -3. International Journal of Applied Earth Observation and Geoinformation, 23(1), 344–351. https://doi.org/10.1016/J.JAG.2012.10.008 

Evangelides, C., & Nobajas, A. (2020). Red-Edge Normalised Difference Vegetation Index (NDVI705) from Sentinel-2 imagery to assess post-fire regeneration. Remote Sensing Applications: Society and Environment, 17, 100283. https://doi.org/10.1016/J.RSASE.2019.100283 

Gamon, J. A., Huemmrich, K. F., Wong, C. Y. S., Ensminger, I., Garrity, S., Hollinger, D. Y., Noormets, A., & Peñuelask, J. (2016). A remotely sensed pigment index reveals photosynthetic phenology in evergreen conifers. Proceedings of the National Academy of Sciences of the United States of America, 113(46), 13087–13092. https://doi.org/10.1073/pnas.1606162113 

Reid, A. M., Chapman, W. K., Prescott, C. E., & Nijland, W. (2016). Using excess greenness and green chromatic coordinate colour indices from aerial images to assess lodgepole pine vigour, mortality and disease occurrence. Forest Ecology and Management, 374, 146–153. https://doi.org/10.1016/J.FORECO.2016.05.006 

Sims, D. A., & Gamon, J. A. (2002). Relationships between leaf pigment content and spectral reflectance across a wide range of species, leaf structures and developmental stages. Remote Sensing of Environment, 81(2–3), 337–354. https://doi.org/10.1016/S0034-4257(02)00010-X 

Wong, C. Y. S., D’Odorico, P., Arain, M. A., & Ensminger, I. (2020). Tracking the phenology of photosynthesis using carotenoid-sensitive and near-infrared reflectance vegetation indices in a temperate evergreen and mixed deciduous forest. New Phytologist, 226(6). https://doi.org/10.1111/nph.16479 

<!--chapter:end:0_Photogrametric_processing.Rmd-->

# LiDAR Processing Workflow

## DJI Terra 


- Open DJI Terra and start a LiDAR Point Cloud Processing session
- Import the flight files, including imagery if you would like a colorized point cloud
- Below are the settings we use to process the point cloud:
  - We use the deafult settings set by DJI with the exception of setting the DEM resolution to 10cm
  
```{r echo=FALSE, fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\DJI_settings.PNG")
```


## Register the LiDAR to the DAP point cloud
	
Convention is generally to register DAP to LiDAR however for this specific project it made more sense to register the LiDAR to the DAP because:

- We were flying biweekly multispectral and RGB imagery and only 2-3 LiDAR acquisitions a year. 

- We could not load the LiDAR point cloud into Agisoft Metashape to use as a reference for registration however could register all DAP clouds to a reference DAP cloud in Agisoft.

- This allowed us to more easily register the many multispectral and RGB acquisitions to a defined template in Agisoft Metashape and use the exported point cloud of the template DAP to register the LiDAR in CloudCompare.

- This process worked well for us since our priority was centimeter-level registration of orthomosaics and LiDAR between many dates and across multiple sensors. 


### Prepare LiDAR for registration

Below are snipets of a script written in LAStools that is used to ensure the LiDAR and DAP clouds are in the proper projection, filters out points in the LiDAR that are above a defined threshold, and clips the LiDAR to a boundary polygon to speed up registration by avoiding working with excess data in CloudCompare.

The unparsed script can be found on the project GitHub, see [GitHub link](index.html#github-link) 

1. Ensures both the DAP point cloud and LiDAR file are in the proper projection (NAD83 UTM 10N in our case) 
    - path_to_L1_las is the path to the LiDAR .las file from DJI terra.
    - path_to_write is where the path the projected .laz file will be written to. The files will be saved out with the same file name as the origianl LiDAR file with a "_nad83" suffix.
    - "REM" works to comment out a line in LAStools as "#" does in R.
  
```{LAStools eval=FALSE, include=TRUE}
REM Project DAP and save out as a .laz
las2las -i path_to_DAP_las_file ^
	-odir path_to_write ^
	-odix _nad83 ^
	-olaz ^
	-nad83 ^
	-utm 10north ^
	-cpu64 ^
	-v

REM Project LiDAR and save out as a .laz
las2las -i path_to_L1_las ^
	-odir path_to_write ^
	-odix _nad83 ^
	-olaz ^
	-nad83 ^
	-utm 10north ^
	-cpu64 ^
	-v

```


2. Drops points in the LiDAR point cloud above a user defined threshold to remove noisy points that are not from the canopy (i.e. due to air moisture, birds, etc.). To determine this height threshold you can plot a histogram of Z values or visualize the point cloud in a software that allows 3D point cloud visualization to ensure that the threshold will not result in any top canopy points being removed. Though we use CloudCompare for point cloud registration, we recommend Potree to visualize point clouds as it handles large point clouds quickly and with ease. Both Potree and CloudCompare are open-source software.
    - If you do not have any point above the canopy or below the ground that need removing you can skip this step.
    - path_to_projected_L1 is the path to the projected LiDAR .laz file from above
```{LAStools eval=FALSE, include=TRUE}
REM dropping points above 160m (160m is the height cutoff for this dataset)
las2las -i path_to_projected_L1 ^
	-odir path_to_write ^
	-odix _droppedPtsAbove160m ^
	-olaz ^
	-drop_z_above 160 ^
	-cpu64 ^
	-v
```

3. Lastly we clip the LiDAR file to a boundary polygon of the site to remove excess surrounding data that can slow processing in CloudCompare.
    - path_to_shp is the path to the site shapefile.
    - path_to_projected_below160m_L1 is the path to .laz file that has been projected and filtered for points above a set threshold in steps 1 and 2 above.
    - "-odix _clipped" will save the new laz file with the same name as the input file with "_clipped" attached to the end, change the "_clipped" to work with your naming system.

```{LAStools eval=FALSE, include=TRUE}
lasclip -i path_to_projected_below160m_L1 -merged -path_to_shp -odir path_to_write -odix _clipped -olaz
```

### Register LiDAR to DAP cloud in CloudCompare
Import both the LiDAR and DAP clouds into CloudCompare (CC).  This can take up to twenty minutes.  Allow all for the first two warnings. 

#### Examine the point clouds for artifacts.  
The L1 will reflect flight lines when there is moisture in the air.  Below is an example point cloud from a damp day on the coast just after a fog past through. For sake of the example, it was not filtered in LAStools with a height threshold. 

```{r echo=FALSE, fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\CloudCompare_1.PNG")
```

The residual fog patches seen above the canopy can be clipped out using the cross section or segment tool.

 ** IMAGE of TOOLS MENTIONED ** ******************************************

#### Rough Alignment 

The goal is to roughly align (move) the LiDAR to our reference “template” DAP dense cloud ahead of the ICP fine adjustment algorithm.  

Problem: Manually moving the LiDAR cloud to roughly align with the DAP cloud caused Cloud Compare to crash or hang when working with larger data sets.   

Solution: Use the Apply Transformation function in CloudCompare – Highlight the LiDAR layer in the DB tree - Hit ctrl T or Apply Transformation in the Edit menu. 


In this example the LiDAR is the lower. 
```{r echo=FALSE,fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\CloudCompare_2.PNG")
```


First apply a Z transformation. It is best to do this while viewing the edge of the plot. Here we applied a Z transformation of +6. 

```{r echo=FALSE,fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\CloudCompare_3.PNG")
```


Next Apply transformations in the x and y axis.   

Find a section of the clouds where you can easily see the alignment.  In this case there is a single large leave tree in the centre of the plot.  The LiDAR is the one on the left. 

Here we see the LiDAR is offset to the left (negative) on the Y(green) axis and up (positive) on the x(red) axis.  

```{r echo=FALSE,fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\CloudCompare_4.PNG")
```

Be careful not to rotate, only transform. 

```{r echo=FALSE,fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\CloudCompare_5.PNG")
```

In this case the LiDAR was shifted -5 in the X and +5 in the Y axis in total.  The shift was done in three smaller iterations to achieve the alignment seen in the above screen capture. 

#### Fine Registration: Iterative Closest Point (ICP) 

Highlight both clouds in the DB tree and apply the fine registration (ICP) algorithm using the following settings. Careful to set a max thread count that matches the resources you have available. 

```{r echo=FALSE,fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\CloudCompare_6.PNG")
```

Save out the registered LiDAR point clouds at the highest resolution

### Rescale and tile the registered LiDAR: 

Below are snipets of a script written in LAStools that is used to project, rescale and tile the registered LiDAR point cloud.

The unparsed script can be found on the project GitHub, see [GitHub link](index.html#github-link)

1. Projecting the registered LiDAR to the proper projection (NAD83 UTM 10N). 
    - If you are working with already tiled data or multiple .laz/.las files on a multi-core computer than you can use the cores command shown below. Here we defined cores=4 which allows 4 cores to work on the command simultaneously on different files. If you are only working with one .laz/.las file at this point there is no need to specify the number of cores and the "^  -cores %cores%" following the "-v ^" should be removed from the below code
    - Change "path_to_registered_lidar" to the path to the registered LiDAR data exported from CloudCompare.
    - "\*.las" takes the las file in that folder, change to the file name if have more than one .las in the folder and you want to specify a specific file.
    - Change "path_to_write\01_proj_NAD83" to the path you would like the new projected  .laz file to be written to.

```{LAStools eval=FALSE, include=TRUE}
set cores=4

las2las -i path_to_registered_lidar\*.las ^
    -odir path_to_write\01_proj_NAD83 ^
    -odix _nad83 ^
    -olaz ^
    -nad83 ^
    -utm 10north ^
    -cpu64 ^
    -v ^
    -cores %cores% 
```

2. Rescales the data which is necessary to later load into R for normalization and metric calculations.
    - Registered LiDAR is exported from CloudCompare at the highest resolution which changes the scale of the data, hence rescaling the x,y,z to 0.01 is necessary to avoid warnings/errors in R
    - "path_to_write" is in the input dir and the output dir because its the main folder we are now working in
    - "path_to_write\01_proj_NAD83\*.laz" selects the .laz file in the "path_to_write\01_proj_NAD83" folder, if there are more than one .laz file in your folder and you want to specify which to call, change the "*" to the name of the file.
    - The output .laz file will be written to the "path_to_write\02_rescaled" folder with the same name as the original file.
```{LAStools eval=FALSE, include=TRUE}
las2las -i path_to_write\01_proj_NAD83\*.laz ^
        -rescale 0.01 0.01 0.01 ^
        -cpu64 ^
        -utm 10north ^
        -v ^
        -odir path_to_write\02_rescaled ^
        -olaz
```

3. Next the code indexes the LiDAR data. Indexing creates a ".lax" file for a given .las or .laz file that contains spatial indexing information. When this LAX file is present it will be used to speed up access to the relevant areas of the LAS/LAZ file for spatial queries.
```{LAStools eval=FALSE, include=TRUE}
REM Indexing
lasindex -i path_to_write\02_rescaled\*.laz
```

4. Lastly tiling divides the point clouds into tiles to allow for parallel processing in the following R steps.
    - "tile_size 15" sets the size of the tiles to 15m.
    - "buffer 4 " sets the size of the buffer surrounding the tiles to 4m.
    - "flag_as_withheld " flags the buffer points so that they can be easily filtered out in the following steps in R.
```{LAStools eval=FALSE, include=TRUE}
REM Creating 15m tiles 
lastile -i path_to_write\02_rescaled\*.laz ^
    -tile_size 15 ^
    -buffer 4 ^
    -flag_as_withheld ^
    -odir path_to_write\03_tile ^
    -olaz 
```

## Normalization and individual tree point clouds

The below workflow includes explanations and code where applicable for each of the following:

1. Creating a 10cm DTM from the registered LiDAR tiles

2. Normalizing the tiles using the DTM 

3. Creating a 4cm CHM from the max Z values in each pixel 

4. Segmenting the tiles to only retain the top 25% of each tree using the crown polygons. The threshold will be site and age dependent. We set our threshold to the top 25% given that the trees were mature with crown closure and we were not confident the lower 75% of the point cloud did not contain any invading neighboring branches. 

5. Merging segmented tiles to create one large point cloud containing the top 25% of each tree 

6. Clipping the point cloud into individual point clouds per tree

Note: The full code that can be found on the project [GitHub](index.html#github-link) runs through the above steps in a for loop. For sake of this guide we have rearranged the ordering of some steps (i.e. functions will be defined as we go in this guide, however are defined at the beginning in the full R script to allow the for loop to run). 

### Creating a DTM from the registered LiDAR tiles 

To being, when working with LiDAR data in R, an incredibly useful package is the lidR package. We highly recommend taking a look at the [lidR bookdown](https://r-lidar.github.io/lidRbook/) for useful tips and examples on using the package. 

The packages required for our workflow are:
```{r eval=FALSE, include=TRUE}
library(tidyverse)
library(sf)
library(sp)
library(spatial)
library(raster) # working with raster data
library(terra)
library(lidR) # reading and processing LiDAR data
library(sp) # defines and allows us to work with spatial objects
library(nngeo)
library(future)
library(rmapshaper)
library(concaveman)
library(parallel)
library(foreach)
library(smoothr)
library(ForestTools)
library(gdalUtilities)
library(exactextractr)
library(alphashape3d) # Creates alpha shapes used to calculate crown volume 
library(lwgeom)
library(dplyr)
```


Here we are reading in the output tiles from the [tiling and rescaling step above](index.html#Rescale-and-tile-the-registered-LiDAR). We then drop the buffer points that were flagged as withheld in the LAStools tiling stage, set the new chunk buffer to 0.5m and set the opt_output_files to empty given that we do not want to save DTMs for the individual tiles but rather one for the entire site.

```{r, eval=FALSE, echo=TRUE}
  dir <- "set_path_to_folders"
  
  TILES = readLAScatalog(folder = paste0(dir, "\\03_tile\\"), filter = "drop_withheld")
  opt_filter(TILES) <- "-drop_withheld"   # set filtering options for the LAScatalog object to drop withheld points
  opt_chunk_buffer(TILES) = .5   # set the buffer size for chunks in the LAScatalog object to 0.5m
  opt_laz_compression(TILES) = TRUE   # enable LAZ compression for the LAScatalog object
  opt_output_files(TILES) = ""   # set output file options for the LAScatalog object to empty
  opt_progress(TILES) = TRUE # enable progress tracking for processing
```

Next we create a 10cm DTM using the tin() algorithm and smooth the raster by using the mean focal statistic and a focal window of 25m by 25m over the DTM. As the focal window shifts over the raster, it updates the pixel that sits at its center to the mean value within the 25m by 25m window. We then assigned the proper CRS and exported the raster as a tif.

```{r, eval=FALSE, echo=TRUE}
  # Create a DTM
  DTM = grid_terrain(TILES, res = 0.1, tin(), full_raster = FALSE) %>% 
    # applying a focal operation to the DTM raster: computing the mean value within a moving window defined by a matrix.
    focal(w = matrix(1, 25, 25),  # define a 25x25 window with all values as 1
          fun = mean,             # use the mean function to compute the focal statistic
          na.rm = TRUE,           # remove NA values from computation
          pad = TRUE)             # pad the edges of the raster with NAs to maintain the original extent
    
  crs(DTM) <- CRS("+proj=utm +zone=10 +datum=NAD83") # assign a CRS to the raster using the proj4 string representation
  
  writeRaster(DTM, paste0(dir, "\\04_RASTER\\", site, "_DTM_0.1m.tif"), overwrite = TRUE) #save the DTM
```



### Normalizing the tiles using the DTM  

The R code below reads in the tiled registered LiDAR as a LAScatalog and normalizes the tiles to the DTM made in the previous step. The normalized tiles are then saved out and read back into R to filter out points below "ground" designated to be -0.25m. Here we save out the "cleaned" normalized tiles in a new folder, however for space saving reasons you can also overwrite the original normalized tiles by setting the "opt_output_files()" parameter in the filtering step to the same path as that set for the normalization step. 

```{r,eval=FALSE, echo=TRUE}
# Read the tiles again, but with high-res parameters
  CTG = readLAScatalog(folder = paste0(dir, "\\03_tile\\"))
  opt_chunk_buffer(CTG) = .5 # small buffer; we're just normalizing
  opt_laz_compression(CTG) = TRUE
  opt_filter(CTG) = "-thin_with_voxel 0.01" # 1cm voxel thinning
  opt_output_files(CTG) = paste0(dir, "\\05_NORM\\{*}_NORM") #saving out normalized individual laz files with an extension of _NORM to the 05_NORM folder
  opt_progress(CTG) = TRUE
  
  # Normalizing the tiles in the catalog using the DTM, tiles will be saved to the location designated in the above "opt_output_files" as they are processed
  NORM = normalize_height(CTG, DTM, na.rm = TRUE)#normalizing the laz files in the CTG to the previously made DTM
  
  # Read in the normalized laz files and filter out points below -0.25 (below ground)
  NORM = readLAScatalog(paste0(dir, "\\05_NORM\\"))
  opt_chunk_buffer(NORM) = 0  # no buffer, just filtering
  opt_laz_compression(NORM) = TRUE
  opt_filter(NORM) = "-drop_z_below -.25" #drop points below -25cm
  opt_output_files(NORM) = paste0(dir, "\\06_NORM_clean\\{*}") #save the filtered normalized filed to the "06..." folder, here you can also choose to overwrite the original normalized laz files by setting the output location to the "05_norm" folder designated as the output in the normalization step
  opt_progress(NORM) = TRUE #show the progress
  
  NORM_clean = catalog_retile(NORM) # applying the filter to all files in the catalog
  
```

### Creating a 4cm CHM from the max Z values   

A 4cm CHM was made using max Z values. We chose a resolution of 4cm as it gave us a good looking (no holes, no visible noise) high resolution CHM. We recommend testing out multiple resolutions at this stage to narrow down parameters that work for your data.

```{r,eval=FALSE, echo=TRUE}
# Making a 4cm resolution CHM
CHM_max = grid_metrics(NORM_clean, #NORM_clean is the catalog the CHM is being made from
                        res = 0.04, #4cm resolution
                        func = ~max(Z)) #using max Z values
  
crs(CHM_max) <- CRS("+proj=utm +zone=10 +datum=NAD83") # assign a CRS to the raster using the proj4 string representation
writeRaster(CHM_max, paste0(dir, "\\04_RASTER\\", site, "_CHM_max_0.04m.tif"), overwrite = TRUE) #saving out the CHM
  
```

### Segmenting tiles

The normalized tiles were then segmented to only retain the top 25% of each tree. This was done using a shapefile (.shp) containing a polygon for each tree crown, see the chapter on [crown delineation](09-Crown_delineation#Crown-delineation) for a detailed workflow showing how to create crown polygons. 

The threshold set for segmentation will be site specific. We chose to only retain the top 25% of the point cloud for each tree in our mature (~25 years old) sites with crown closure in order to:
  1. limit the chance of capturing invading branches in our LiDAR data
  2. Capture a similar scope of the trees in both the LiDAR and photogrametic data, given that the aerial imagery is limited to branches that can be seen from an aerial perspective. 

A range a thresholds, from 50% (for ~10 year old trees that were around ~8m in height) to 80% (for ~4 year old trees that were around ~1m in height), were used for the younger sites we worked with depending on the proportion of tree visible from an aerial perspective.

To begin, we must first define the function that will segment the trees. Here "zq" is the threshold for the height segmentation and can either be set in the polys_to_las function (if you would like it to be the same value each time the function is run) or defined as a variable prior to initiating the function (use this option if the function will be called for different sites that have different zq values). 

The polys_to_las function works by:
1. Identifying points that fall within a tree crown and labeling the points with the unique treeID for that crown
2. Labels points within the crown that fall below the defined height threshold to have a treeID of zero 
3. Filters to only keep points with a treeID value greater than zero

Important:
- If zq = 0.75, then the points in the bottom 75% of the tree will be removed, leaving a point cloud for the top 25% only.
- "treeID" is a column in our dataset that functions as a unique identifier per tree within the site, ensure you change each instance of treeID to a column containing a unique identifying for your trees. The unique identifiers can come from the experimental site setup or the [crown delineation](09-Crown_delineation#Crown-delineation) step.

```{r, eval=FALSE, echo=TRUE}
# Function that will segment the LiDAR tiles to only retain points above a defined cutoff
polys_to_las = function(chunk, zq = zq, polygons = pols) { #edit zq = zq here if you would like the function to always use the same zq value, ie zq = 0.75, otherwise leave zq = zq as you will be able to define zq in the next code chunk
  
  las = readLAS(chunk)                  
  if (lidR::is.empty(las)) {
    return(NULL) }# If the LAS file is empty, return NULL
  
  las2 = merge_spatial(las, polygons, "treeID") # Merge the LAS points with the polygons based on the treeID attribute, change the "treeID" attribute to a unique identifier for the trees you are working with
  las_df = las2@data %>%
    dplyr::group_by(treeID) %>%
    dplyr::mutate(Zq999 = quantile(Z, 0.999)) %>% # compute Zq999 (the 99.9th percentile of Z) for each tree 
    dplyr::mutate(treeID = if_else(Z > quantile(Z, 0.999) * zq, as.numeric(treeID), 0))# assign points below the top zq% of the tree height a treeID of zero to filter them out later

  las3 = las2 %>% 
    add_lasattribute(las_df$treeID, name = "treeID", desc = "treeID") %>% #add treeID las a lasattribute
    filter_poi(treeID > 0) #filter for points with a treeID greater than 0 
  
  if (lidR::is.empty(las3)) {
    return(NULL)#return NULL is the las3 is empty
  } else {
    return(las3)#return the las file that has a treeID associated with it and only contains points in the top zq%
  }
  
}
```

Next we read in the normalized files, set our zq threshold and run the tiles through the polys_to_las segmentation function using the catalog_apply function.

```{r, eval=FALSE, echo=TRUE}

# Read in the normalized cloud from the end of the last step
NORM = readLAScatalog(paste0(dir, "\\06_NORM_clean\\"))
crs(NORM) <- st_crs(26910) # setting CRS
opt_chunk_buffer(NORM) = 0 # no buffer
opt_laz_compression(NORM) = TRUE # compress output files to .laz objects
opt_output_files(NORM) = paste0(dir, "\\07_SEGMENTED\\{*}_SEGMENTED") # write output files to "07_SEG..." folder and name with suffix "_SEGMENTED"
opt_progress(NORM) = TRUE # show progress
  
zq = 0.75 # Height percentile to drop, here we will be filtering for the top 25% of the tree 
# New tiles have only segmented portions of tree crowns
SEGMENTED = catalog_apply(NORM, polys_to_las) # apply the poly_to_las function to the NORM catalog

```

### Merging segmented tiles to create one large point cloud containing the top defined height % of each tree

This is done to facilitate clipping out individual tree point clouds in the next step.  

**Note:**  

- The *opt_chunk_size(SEGMENTED) = 10000* step sets the new chunk size to 10,000 points, meaning that chunks of 10,000 points will be managed at a time. If there are more than 10,000 points in your point cloud than data will be handled in chunks of 10,000 points at a time to not overload memory. 

- The *catalog_retile* function merges the point clouds into one large point cloud 

```{r, eval=FALSE, echo=TRUE}
SEGMENTED = readLAScatalog(paste0(dir, "\\07_SEGMENTED\\"))
opt_chunk_buffer(SEGMENTED) = 0 #zero buffer
opt_chunk_size(SEGMENTED) = 10000 #set the chunk size for the LAScatalog object to 10000 points so that all laz files are merged into one large laz file
opt_laz_compression(SEGMENTED) = TRUE
opt_progress(SEGMENTED) = TRUE
opt_output_files(SEGMENTED) = paste0(dir, "\\08_MERGED\\", site, "_HULLS_merged") #write output files to "08_M..." with the suffix "_HULLS_merged"
  
# merge all the segmented trees into a single point cloud 
MERGED = catalog_retile(SEGMENTED) #apply the above opt_ commands
print("merged") #print statement to show where the code is at
```

### Clipping the point cloud into individual point clouds per tree

This step results in individual tree point clouds that represent the top #% of the tree, where # is the height percentile defined in the segmentation step above. 

```{r, eval=FALSE, echo=TRUE}
opt_output_files(MERGED) = paste0(dir, "\\09_CROWNS\\", site, "_fam{fam}_rep{rep}_tag{tag}_treeID{treeID}") #where the individual crown point clouds will be written to and the suffix they will have. Note values associated with the laz file will replace the names in {}
CROWNS = clip_roi(MERGED, pols) #clip the MERGED laz file to individual crowns using the crown polygons 
```

Below we define the clean_crowns function that ensures that only points:
- with the correct treeID remain in each individual tree point cloud. 
- that pass the defined height percentile threshold remain in the point cloud

This function was added to double check that only the points that meet the criteria remain in the point clouds

```{r, eval=FALSE, echo=TRUE}
clean_crowns = function(chunk) {
  las = readLAS(chunk) #reading in las file                 
  if(lidR::is.empty(las)) return(NULL) #removing empty
  
  #ensuring all points in the cloud have the same/right tree ID
  treeID_true = as.numeric(names(sort(table(las@data$treeID), decreasing = TRUE))[1])
  las2 = filter_poi(las, treeID == treeID_true)
  
  #(added to Sam's code) filter points below the 75% threshold that were not dropped previously
  las_df = las2@data %>%
    dplyr::group_by(treeID) %>%
    dplyr::mutate(Zq99 = quantile(Z, 0.99)) %>%
    dplyr::mutate(Zq999 = quantile(Z, 0.999)) %>%
    dplyr::ungroup()
  
  las3 = filter_poi(las2, Z >= quantile(Z, 0.99)*zq) #only keeping points above 99%*zq height percentile
  las4 = filter_poi(las3, Z <= quantile(Z, 0.999)) #removing points too high
}

```




```{r, eval=FALSE, echo=TRUE}
## CLEANING crowns
CROWNS = readLAScatalog(paste0(dir, "\\09_CROWNS\\"), filter = "-drop_withheld") #read in the clipped crown point clouds
opt_chunk_size(CROWNS) = 0 # processing by files
opt_laz_compression(CROWNS) = TRUE
opt_chunk_buffer(CROWNS) = 0 # no buffer
opt_wall_to_wall(CROWNS) = TRUE # disable internal checks to ensure a valid output
opt_output_files(CROWNS) = paste0(dir, "\\10_CROWNS_clean\\{*}") #location for output files
print("cleaning crowns")
CROWNS_clean = catalog_apply(CROWNS, clean_crowns) #applying the clean_crowns function on the CROWNS
```



## Individual Tree Metrics

This script calculates the following metrics for each tree: 

- **Height percentiles**: 99, 97.5, 95, 92.5, mean height  
  - To create new height percentiles, change the “X” here:  
  - as.numeric(quantile(Z, X, na.rm = TRUE)) 

- **Volumes**: convex and two variations of concave (α = 1, α = 0.5) 
  - Volume calculations are dependent on the alpha shape created. The larger the alpha parameter used to create the alpha shape the more convex the hull shape, whereas the smaller the alpha the more concave the shape. 
  - Very small alpha values can result in capturing holes in the point cloud as it will hug tightly to the points 
  - The alpha shape for convex (lines curving outward) hull volume requires an α = inf and represents the tightest convex boundary around all points in the point cloud 
  - Concave volume often uses α = 1, however below we also calculate concave volume with α = 0.5 for a more defined shape 
  - To create a new alpha shape with a new alpha value change the “α_value” below:
    alphashape3d::ashape3d(x = a3d, alpha = “α_value, pert = TRUE,eps = 1e-09) 

- **Crown complexity**: rumple, canopy rugosity ratio (CRR), coefficient of variation of height  
  - Rumple measures the ratio of canopy surface area to ground surface area to give an idea of complexity. 
  - Canopy Rugosity Ratio (CRR) quantifies the vertical complexity of the canopy or crown. 
  - Coefficient of variation of height quantifies the variability of points in the point cloud. This metric is often used to look at structural diversity of a stand, however we added it to look at variation in point distribution within individual trees, to give an idea of variation in vegetation density between crowns. 
  
```{r, eval=FALSE, echo=TRUE}
Z = las@data$Z # Z values of each point in the .laz cloud
chm = grid_metrics(las, func = ~max(Z,na.rm = TRUE), res = 0.05) # 5cm CHM with max Z values #grid_metrics, replaced by pixel_metrics :https://github.com/r-lidar/lidR/releases
      # Extract X, Y, and Z coordinates from the LAS data and create a matrix 'a3d'
a3d <-  cbind(las@data$X, las@data$Y, las@data$Z)
      
# Center the points around the origin (0,0,0) by subtracting the mean of each dimension
a3d[,1] = a3d[,1] - mean(a3d[,1],na.rm = TRUE) #center x values
a3d[,2] = a3d[,2] - mean(a3d[,2],na.rm = TRUE) #center y values
a3d[,3] = a3d[,3] - mean(a3d[,3],na.rm = TRUE) #center z values # sams orginal code did not center the Z values, but that is the only way we found to not get an error thrown in the ashape3d function.
      
# Generate different shapes using the alphashape3d package
shape_convex = alphashape3d::ashape3d(x = a3d, alpha = Inf, pert = TRUE,eps = 1e-09)# Compute a convex hull using alpha = Inf (convex hull) #USED PERT= TRUE : https://cran.r-project.org/web/packages/alphashape3d/alphashape3d.pdf
shape_concave = alphashape3d::ashape3d(x = a3d, alpha = 1, pert = TRUE,eps = 1e-09)# Compute a concave hull using alpha = 1 (concave hull)
shape_a05 = alphashape3d::ashape3d(x = a3d, alpha = 0.5, pert = TRUE,eps = 1e-09)# Compute a shape for alpha = 0.5 (balanced between convex and concave)
      
structural_metrics_df <- data.frame(
  treeID = unique(las@data$treeID), 
  tag = tag_value,
  n_points = length(las@data$Z),
        
  # Crown height
  Zq99 = as.numeric(quantile(Z, 0.990,na.rm = TRUE)),# 99th percentile
  Zq975 = as.numeric(quantile(Z, 0.975,na.rm = TRUE)), # 97.5th percentile
  Zq95 = as.numeric(quantile(Z, 0.95,na.rm = TRUE)),# 95th percentile
  Zq925 = as.numeric(quantile(Z, 0.925,na.rm = TRUE)), # 92.5th percentile
  Z_mean = mean(Z,na.rm = TRUE), #mean crown height
        
  # Crown volume
  vol_convex = alphashape3d::volume_ashape3d(shape_convex), # convex volume
  vol_concave = alphashape3d::volume_ashape3d(shape_concave), # concave volume
  vol_a05= alphashape3d::volume_ashape3d(shape_a05), #volume with alpha = 0.5 (balance between concave and convex)

  # Crown complexity
  CV_Z = sd(Z,na.rm = TRUE) / mean(Z,na.rm = TRUE),# Compute the coefficient of variation (CV) of Z values, representing the variability of heights relative to the mean height
  rumple = lidR::rumple_index(chm), #rumple: ratio of canopy outer surface area to ground surface area as measured by the CHM and DTM
  CRR = (mean(Z,na.rm = TRUE) - min(Z,na.rm = TRUE)) / (max(Z,na.rm = TRUE) - min(Z,na.rm = TRUE))# Compute the Canopy Rugosity Ratio (CRR), 
  # representing the ruggedness or roughness of the canopy surface
)
      
#Create structural metrics folder
if (!dir.exists(paste0(dir,"\\10_CROWNS_clean\\Structural_metrics\\"))) {
      dir.create(paste0(dir,"\\10_CROWNS_clean\\Structural_metrics\\"), recursive = TRUE)
}

#writing out .rds files
saveRDS(structural_metrics_df, paste0(dir,"\\10_CROWNS_clean\\Structural_metrics\\",name,"_", date,"_structuralMetrics_tag",tag_value,".rds")) #USED PERT= TRUE : https://cran.r-project.org/web/packages/alphashape3d/alphashape3d.pdf
    
```

Below are a individual tree point cloud (left), clipped to the top 25% of the tree and its corresponding alphashape used for volume (right).
   
```{r echo=FALSE, results='asis'}
# Define the image paths
img_paths <- c(
  "C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\Individual_tree_point_cloud_resize_bookdown.png", 
                          "C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\Individual_tree_volume_resized_bookdown.gif"
)

# Create the HTML for the images
html <- paste0(
  '<div style="display: flex; flex-direction: row; justify-content: center; align-items: center;">',
  paste0('<img src="', img_paths, '" style="height: 300px; margin: 0 10px;" />', collapse = ""),
  '</div>'
)

# Print the HTML
cat(html)
```
  
.

<!--chapter:end:0_LiDAR_processing.Rmd-->

# MicaSense Irradiance Correction


## Background
The MicaSense system measures radiance at the camera, which needs to be transformed into surface reflectance. The basic relationship is given by:

$$
R = \frac{L}{I_{\text{hor}}}
$$

where \(L\) is the at-camera radiance and \(I_{\text{hor}}\) is the horizontal irradiance.

### Deriving Horizontal Irradiance

Below we go through the derivation to compute \(I_{\text{hor}}\).

The MicaSense system includes both the cameras and a downwelling light sensor (DLS). Since the DLS is tilted at the angle of the drone during flight and the sun is rarely directly overheard the sensor, the DLS measures non-horizontal irradiance, referred to as spectral irradiance (\(I_{\text{spec}}\)). Spectral irradiance is a function of direct irradiance (\(I_{\text{direct}}\)), scattered irradiance (\(I_{\text{scattered}}\)), and the angle between the sun and the sensor ( \(\theta_{\text{sun-sensor}}\)):

$$
I_{\text{spec}} = I_{\text{direct}} \cdot \cos(\theta_{\text{sun-sensor}}) + I_{\text{scattered}}
$$

If we take  \( r\) to be the ratio of scattered to direct irradiance, \( r = \frac{I_{\text{scattered}}}{I_{\text{direct}}} \), we can substitute \( I_{\text{scattered}} = r \cdot I_{\text{direct}} \) into the equation:

$$
I_{\text{spec}} = I_{\text{direct}} \cdot \cos(\theta_{\text{sun-sensor}}) + r \cdot I_{\text{direct}}
$$

Next, by factoring out \( I_{\text{direct}} \) from the equation:

$$
I_{\text{spec}} = I_{\text{direct}} \cdot \left( \cos(\theta_{\text{sun-sensor}}) + r \right)
$$

We can solve for \( I_{\text{direct}} \) by dividing both sides of the equation by \( \cos(\theta_{\text{sun-sensor}}) + r \):

$$
I_{\text{direct}} = \frac{I_{\text{spec}}}{\cos(\theta_{\text{sun-sensor}}) + r}
$$

If we assume that \( I_{\text{hor}} \) can be expressed in terms of \( I_{\text{direct}} \) with a similar function involving the zenith angle, we can express this as:

$$
I_{\text{hor}} = I_{\text{direct}} \cdot \left( \cos(\theta_{\text{zenith}}) + r \right)
$$

Substituting the equation for \( I_{\text{direct}} \) into the equation for \( I_{\text{hor}} \) gives:

$$
I_{\text{hor}} = \frac{I_{\text{spec}}}{\cos(\theta_{\text{sun-sensor}}) + r} \cdot \left( \cos(\theta_{\text{zenith}}) + r \right)
$$

### Calculating Reflectance
Finally, substituting \( I_{\text{hor}} \) into the equation for reflectance, we obtain:

$$
R = L \cdot \frac{\cos(\theta_{\text{sun-sensor}}) + r}{I_{\text{spec}} \cdot \left( \cos(\theta_{\text{zenith}}) + r \right)}
$$

Thus we see that reflectance is highly dependent on the ratio of scattered to direct irradiance as well as the sun-sensor angle. This is important for understanding the issue we have found when working with the DLS2 and the MicaSense MX Dual camera system.

## The Problem
The issue we have noticed is that the sun-sensor angle can be absurdly high or low, especially in strong illumination conditions. This leads to equally absurd, sometimes impossible, values for direct and scattered irradiance, that can be multiples higher than the direct solar irradiance.

To begin, let us first look at some main differences between the legacy DLS1 and the current DLS2 system. 

The DLS1:

- calculates the sun zenith angle using the GPS location of the drone and the time of acquisition 

- calculates the sun-sensor angle from the yaw/pitch/roll of the drone

The DLS2:

- directly measures the sun sensor angle and the direct and diffuse irradiance components using a proprietary method that takes in information from the light sensors on the surface of the DSL2
 
## Is Correction Needed?

Deciding whether data needs to be corrected can be challenging. We have found that looking at these three questions can be helpful:

*1) Do the sun sensor angles make sense?*

*2) Does the relationship between direct and horizontal irradiance make sense?*

*3) Does the scattered: direct ratio look right for the lighting conditions of that day?*

Below we have plotted the sun sensor angle (top left), the position of each photo (bottom left) and the irradiance values (right) from the exif data of the Micasense images. We have used the yaw values to identify flight lines (blue and green) and roll to filter for images taken when the drone is turning (red). 

This data was captured on June 25th, 2024 on Vancouver Island in British Columbia, Canada. As seen by the black horizontal line in the above figure, the solar zenith angle at the time of the flight was around 30°. 

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\Irradiance_flight_lines_with_turning_all_with_spatial_plot_CROPPED.PNG")

```


### Do the sun sensor angles make sense?

Solar zenith angle is the angle between the normal (vertical from the Earths surface) at the location of interest and the position of the sun. Given the tilt of the Earth, the solar zenith reaches its maximum (i.e. greatest angle between the sun and the normal to the Earths surface) in the winter, as the northern hemisphere is tilted away from the sun, and its minimum in the summer when the northern hemisphere is tilted towards the sun. 

The solar zenith also changes throughout the day, with its minimum occurring at solar noon, a.k.a. when the sun is at its highest. This is why it is suggested to fly within +/- 2 hours of solar noon to avoid shadows in your imagery.

Below we have graphed solar zenith angles over the winter solstice (December 24, 2024), summer solstice (June 21, 2024) and the date of the flight acquisition (June 25th, 2024) that will be used in this example to demonstrate the daily and seasonal change in values. 

The gray vertical bar highlights the flight time, which was just within the +/- 2 hours of solar noon bounds and shows the solar zenith at around 30°, as we saw in the above figure.

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\Solar_zenith_solstice_flight.PNG")
```

Now that we've refreshed our understanding of the solar zenith, let's dive into the sun sensor angle. The sun sensor angle is, as it sounds, the angle between the normal (perpendicular) of the sensor and the direction of the sun. Given that drone flights occur relatively close to the ground compared to the distance from the Earth to the sun, this difference is often negligible. Therefore, the solar zenith angle can serve as a rough estimate of the sun sensor angle for a sensor that is parallel to the Earth's surface - making it a helpful check to see if the sun sensor angle recorded by the DLS2 is within a reasonable range.


```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\SSA_verse_solarZenith_diagram.PNG")
```
However, drones often fly at an angle, which can cause a cyclical pattern in sun sensor angles depending on flight direction. For example, let's take a look at the diagram below. When the drone is flying in the general direction of the sun (left), the forward tilt of the drone mid-flight results in a smaller sun sensor angle compared to when the drone is flying away from the general direction of the sun, where the tilt of the drone away from the sun increases the sun sensor angle (right).


```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\SSA_flight_direction_diagram.PNG")
```

This pattern of sun sensor angle dependence on drone orientation relative to the sun can be see for the June 25th flight. Below are the uncorrected sun sensor angles, derived from the DLS2 sensor located on the top of the drone, colored by flight line direction. Here red indicates turning and blue and green are opposite flight directions.

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\SSA_flightlines.PNG")
```
Now that we understand the reason behind the cyclical pattern of the sun sensor angles, we can use the solar zenith value (plotted as the horizontal black line) to make an informed decision on whether or not the values need correcting. 

To help this decision we recommend plotting the DLS2 sun sensor values and the sun sensor values calculated with the DLS1 method [LINK] which will be referred to as the "corrected" values.

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\SSA_DLS1_verse_DLS2.PNG")
```
The dashed horizontal lines are the moving averages for the DLS1 and DLS2. Here the moving average of the DLS2 is at times almost 10°lower than the solar zenith, whereas the moving average of the DLS1 (corrected method) is more or less consistently 2-3° lower than the solar zenith. 

Given this data, we would conclude that the DLS1 method provides more accurate sun sensor angles compared to the DLS2. We will now continue to look at irradiance to confirm if correction is needed.

### Does the relationship between direct and horizontal irradiance make sense?

First, lets define direct and horizontal irradiance with respect to the DLS:

**Direct Irradiance**: The direct component of the sunlight reaching the sensor's surface that is not being scattered. On a clear sunny day, this component is high.

**Scattered Irradiance**: The sunlight that is scattered by particles in the atmosphere. This component is generally weaker on sunny days and higher on overcast days.

**Horiztonal Irradiance**: The total irradiance on a horizontal surface, including both direct sunlight (projected onto the horizontal plane) and diffuse light. Hence, if the sun is directly above the sensor and it is a clear sunny day, the direct irradiance component will likely be quite close to the horizontal irradiance, however still lower given that horizontal irradiance inlcudes diffuse light as well. 

Below, we have plotted the direct and horizontal irradiance from the DLS2 and values calculated with the [DLS1 method](0_Micasense_Irradiance_Correction##Correcting Irradiance Values for Micasense Cameras). We can see that the direct irradiance from the DLS2 is greater than the horizontal irradiance, which is physically impossible. In contrast, the direct irradiance from the DLS1 method is approximately 50 W/m²/nm less than the horizontal irradiance. 

Explanations for the lower direct irradiance from the DLS1 compared to the horizontal irradiance include:

- The ~30° sun sensor angle, which reduces the effective direct component captured by the sensor.

- The portion of scattered irradiance in the horizontal irradiance signal that is not included in the direct irradiance value.


```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\Hor_dir_irradiance_DLS1_v_DLS2.PNG")
```
From this graph, we would recommend correcting the data using the [DLS1 method](0_Micasense_Irradiance_Correction##Correcting Irradiance Values for Micasense Cameras) outlined below 


### Does the scattered: direct ratio make sense for the lighting conditions of that day?

For a clear sunny day when the sun is around its peak, direct radiation generally accounts for ~85% of the total insolation with scattered radiation accounting for the remaining ~15%. For an completely overcast day, scattered radiation contributes to 100% of solar radiation.<!-- https://www.ftexploring.com/solar-energy/direct-and-diffuse-radiation.htm -->

Hence for a clear sunny day in the summer a ratio of 1:6 scattered to direct irradiance is often used as an estimate. This means that direct irradiance is around 6 times stronger than the scattered irradiance.

The data plotted below from June 25th was captured on a sunny day with occasional clouds.
```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\Dir_Scat_irradiance_DLS1_v_DLS2.PNG")
```

The scattered: direct ratio for the data plotted above is 0.09 for the DLS2 and 0.46 for the DLS1. 

**DLS2:** The ratio of 0.08 scattered:direct components from the DSL2 states that the direct irradiance during the flight was 12.5 time greater than the scattered irradiance, or in other words that 91% of the total radiation reaching the earth was direct radiation and 9% was scattered radiation. 

**DLS1:** The ratio of 0.46 scattered:direct components from the DLS1 states that the direct component was 2.17 time greater than the scattered component, hence 46% of the solar radiation was from the scattered component and the remaining 54% from the direct component.

Given that the flight on June 25th was flown just at the end of the 2 hour solar noon window under a sunny sky with the occasional cloud, we would assume that the scattered component would be greater than the minimum ~15% for fully clear skies at solar noon. Hence a value of 46% scattered radiation could be the result of extra scattering by the occasional clouds and a slight increase in scattering from a greater solar zenith angle. Overall the value of 0.46 is more logical than the value of 0.09, since 0.09 implies less scattered radiation than the generally accepted minimum scattered component percentage on a clear sunny day. 

Overall, we recommend correcting this flight using the DLS1 method. The DLS1 corrections are done on individual photos, if the orthomosaic has already been built, no need to rebuild the dense cloud, you can re-upload the imagery, re-calibrate, and re-build the orthomosaic. 

## Correcting Irradiance Values for Micasense Cameras

This chapter explains the process of correcting irradiance values for Micasense cameras using R. The correction involves several steps, including filtering and mutating data, calculating solar angles, and estimating scattered to direct light ratios. Each step is crucial for ensuring accurate irradiance readings, which are essential for downstream analyses. The code was taken largely from [Micasense's GitHub](https://github.com/micasense/imageprocessing/blob/master/MicaSense%20Image%20Processing%20Tutorial%203.ipynb) and converted into R to use in our workflow.


To fix the sun-sensor angle and irradiance values we have taken the methodology used by the DLS1. The process involves:

- Using the GPS location and time of acquisition to determine the solar zenith angle, and the yaw/pitch/roll of the drone to determine the sun sensor angle

- Generating a rolling regression of the relationship: \(I_{\text{spec}} = I_{\text{direct}} \cdot \cos(\theta_{\text{sun-sensor}}) + I_{\text{scattered}}\) to determine the direct/ scattered irradiance ratio, \(r\), and only keeping realistic models with good fits

- Computing the horizontal irradiance using \(I_{\text{hor}} = I_{\text{direct}} \cdot (cos(\theta_{\text{zenith}}) + r) \)

- Editing the exif data of the images so that the imagery can be processed by Metashape with the corrected irradiance values

### Reading Exif Data

First, we read in the exif data of the imagery we want to correct using the exifr package.


```{r eval=FALSE, include=TRUE}
# This code was written to be within a for loop that looped through many acquisition dates. Hence the path names used will be in the form " paste0(dir, date,"\\1_Data\\",MS_folder_name,"\\"...etc)". Change these path names to match your data
library(exifr)
dir <- "directory to folder holding folders representing each flight aquisition date" #change to match your data
MS_folder_name <- "Micasense" #change to match the name of your folder holding the folders of imagery from the Micasense camera
band_length <-  10 # 10 bands for Micasense RedEdge-MX Dual
#Loops through bands
 for (j in 1:band_length){ # bands 1 through 10 
   pics = list.files(paste0(dir, date,"\\1_Data\\",MS_folder_name,"\\"), #path the original data that will NOT be written over
                     pattern = c(paste0("IMG_...._", j, ".tif"), paste0("IMG_...._", j, "_", ".tif")), recursive = TRUE, 
                     full.names = TRUE) # list of directories to all images
        
  if(!dir.exists(paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\"))){ #creating CSV folders
      dir.create(paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\"))}
        
  mask_images <- list.files(paste0(dir,date,"\\2_Inputs\\metashape\\MASKS\\",MS_folder_name,"\\")) #path to folder with masks that are exported from metashape 
  mask_img_names <- gsub("_mask\\.png$", "", mask_images) # removes the suffix "_mask.png" from each element in the 'mask_images' list, ie : IMG_0027_1_mask.png becomes : IMG_0027_1 
  print(paste0("example of mask name: ",mask_img_names[[1]])) #print to ensure your naming is correct, the list of mask_img_names will be used to identify panel images in the micasense image 
  # Loops through images in each band
  for (i in 1:length(pics)){ #for each image in Micasense_Cleaned_save, one at a time
    pic = pics[i]
    pic_root <- gsub(".*/(IMG_.*)(\\.tif)$", "\\1", pic)# The pattern captures the filename starting with "IMG_" and ending with ".tif".Ie: IMG_0027_1.tiff becomes IMG_0027_1
    print(pic_root)
    if (substr(pic_root,11,11) == "0"){ # Distinguishes between _1 (band 1) and _10 (band 10), ie IMG_0027_1 verse IMG_0027_10
        band = substr(pic_root,10,11) # for _10
        } else {
          band = substr(pic_root,10,10) # for _1, _2, _3, ... _9 (bands 1-9)
          } 
    img_exif = exifr::read_exif(pic) # read the XMP data 
    print(paste0(date, " band ", j, " ", i, "/", length(pics))) # keep track of progress
    
    # Creating df with same column names as exif data
    if (i == 1){ # If it's the first image, make new df from XMP data 
      exif_df = as.data.frame(img_exif)%>%
        mutate(panel_flag = ifelse( gsub(".*/(IMG_.*)(\\.tif)$", "\\1", SourceFile) %in% mask_img_names, 1, 0)) # Give a value of 1 if the pic root name is in the list of mask root names and is therefore a calibration panel, give 0 otherwise (ie not a panel) 
      } else {   # Add each new image exif to dataframe
        exif_df = merge(exif_df, img_exif, by = intersect(names(exif_df), names(img_exif)), all = TRUE)%>%
          mutate(panel_flag = ifelse( gsub(".*/(IMG_.*)(\\.tif)$", "\\1", SourceFile) %in% mask_img_names, 1, 0)) # Give a value of 1 if the pic root name is in the list of mask root names and is therefore a calibration panel, give 0 otherwise (ie not a panel) 
        }
    }
  saveRDS(exif_df, paste0(dir,date, "\\1_Data\\",MS_folder_name,"\\CSV\\XMP_data_", date, "_", j, ".rds")) # save output for each band, set path
  #binding all bands into one dataframe
  if (j == 1){
    df_full = exif_df
    }else{
      # Add missing columns to xmp_all and fill with NA values
      df_full= rbind(df_full, exif_df)
    }
  saveRDS(df_full, paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\XMP_data_", date,"_AllBands.rds")) #.rds file containing the original exif data if all images for the flight
 } 
```

Read in the exif data for all bands.

```{r eval=FALSE, include=TRUE}
xmp_all <- readRDS(paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\XMP_data_", date,"_AllBands.rds"))
```

### Calculating Solar Zenith Anlge

Below we convert the DateTimeOrginal column from the exif data to a date object and use the date object and average lat and lon to calculate solar angle.

```{r eval=FALSE, include=TRUE}

xmp_all_filtered <- xmp_all %>%
  mutate(Date_time = ymd_hms(DateTimeOriginal),
         BandName_Wavelength = paste0(BandName, "_", CentralWavelength),
         Date = as.Date(Date_time),
         Time = format(Date_time, format = "%H:%M:%S"),
         img_name = str_split(FileName, "\\.tif", simplify = TRUE)[, 1],
         img_root = sub("_(\\d+)$", "", img_name)) %>%
  drop_na(DateTimeOriginal) %>% 
  dplyr::mutate(
    site_avg_lat = median(GPSLatitude, na.rm = TRUE),
    site_avg_long = median(GPSLongitude, na.rm = TRUE),
    solar_angle = photobiology::sun_zenith_angle(time = ymd_hms(DateTimeOriginal),
                                                 geocode = tibble::tibble(lon = unique(site_avg_long),
                                                                          lat = unique(site_avg_lat),
                                                                          address = "Greenwich")))
```

Next, we check for any missing timestamps in image metadata as this usually indicates that the image was not saved properly and can't be opened which will throw errors later in the workflow. In our experience, these images are few and far between, so we remove them from the analysis. We recommend manually checking that the image is in fact corrupted before removing it from analysis.

```{r eval=FALSE, include=TRUE}
missing_imgs <- xmp_all[is.na(as.Date(xmp_all$CreateDate, format = "%Y:%m:%d"))]$FileName
missing_imgs_roots <- sub("_[^_]*$", "", missing_imgs)

xmp_all_filtered_mis <- xmp_all_filtered %>% #removing images with missing date information
  filter(!FileName %in% c(missing_imgs))

```

<!--
### Caluclating DNI

The solar position and irradiance values are necessary to understand whether or not DLS acquired values are reasonable. We calculate the direct normal irradaiance using the average latitude and longitude from the GPS coordinates of the imagery and the time of the acquisition. 

```{r eval=FALSE, include=TRUE}
(lat <- unique(xmp_all_filtered$site_avg_lat))
(lon <- unique(xmp_all_filtered$site_avg_long))
(time <- mean(xmp_all_filtered$Date_time, na.rm = TRUE))

sun_pos <- getSunlightPosition(date = time, lat = lat, lon = lon)
elevation_angle <- sun_pos$altitude
turbidity <- 2.5
solar_constant <- 1367

air_mass <- 1 / cos(pi/2 - elevation_angle)
dni <- solar_constant * exp(-0.2 * air_mass * turbidity)

print(dni)
```
-->


### Caluclating Sun-Sensor Angles

Here we explain the difference between the archived DLS1 method and the current DLS2 method. 

*DSL1:*
The following code has been taken from [Micasense's GitHub](https://github.com/micasense/imageprocessing/blob/master/MicaSense%20Image%20Processing%20Tutorial%203.ipynb) and coded in R to match our workflow

```{r eval=FALSE, include=TRUE}
compute_sun_angle <- function(SolarElevation, SolarAzimuth, Roll, Pitch, Yaw) {
    ori <- c(0, 0, -1)
    SolarElevation <- as.numeric(SolarElevation)
    SolarAzimuth <- as.numeric(SolarAzimuth)
    Roll <- as.numeric(Roll)
    Pitch <- as.numeric(Pitch)
    Yaw <- as.numeric(Yaw)
  
    elements <- c(cos(SolarAzimuth) * cos(SolarElevation),
                  sin(SolarAzimuth) * cos(SolarElevation),
                  -sin(SolarElevation))
  
    nSun <- t(matrix(elements, ncol = 3))
  
    c1 <- cos(-Yaw)
    s1 <- sin(-Yaw)
    c2 <- cos(-Pitch)
    s2 <- sin(-Pitch)
    c3 <- cos(-Roll)
    s3 <- sin(-Roll)
  
    Ryaw <- matrix(c(c1, s1, 0, -s1, c1, 0, 0, 0, 1), ncol = 3, byrow = TRUE)
    Rpitch <- matrix(c(c2, 0, -s2, 0, 1, 0, s2, 0, c2), ncol = 3, byrow = TRUE)
    Rroll <- matrix(c(1, 0, 0, 0, c3, s3, 0, -s3, c3), ncol = 3, byrow = TRUE)
  
    R_sensor <- Ryaw %*% Rpitch %*% Rroll
    nSensor <- R_sensor %*% ori
  
    angle <- acos(sum(nSun * nSensor))
    return(angle)
}

SSA_xmp_all_filtered <- xmp_all_filtered %>%
  rowwise() %>% 
  mutate(SunSensorAngle_DLS1_rad = compute_sun_angle(SolarElevation, SolarAzimuth, Roll, Pitch, Yaw),
         SunSensorAngle_DLS1_deg = SunSensorAngle_DLS1_rad * 180 / pi)

saveRDS(SSA_xmp_all_filtered,paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\XMP_", date,"_with_SSA.rds")) # Save XMP rds file with sun sensor angle added

```


*DLS2:*
```{r eval=FALSE, include=TRUE}
SSA_xmp_all_filtered$SunSensorAngle_DLS2_rad <- sapply(xmp_all_filtered$EstimatedDirectLightVector, function(vec) acos(-1 * as.numeric(vec[[3]])))
SSA_xmp_all_filtered$SunSensorAngle_DLS2_deg <- as.numeric(SSA_xmp_all_filtered$SunSensorAngle_DLS2_rad) / pi * 180
```

Check that the sun-sensor angles are within a reasonable range. They should range from ~30 degrees mid summer to ~80 degrees mid winter.

```{r eval=FALSE, include=TRUE}
xmp_all_ssa = SSA_xmp_all_filtered %>%
  # Converting from radians to degrees
  mutate(Yaw_deg = as.numeric(Yaw)*180/pi,
         Roll_deg = as.numeric(Roll)*180/pi,
         Pitch_deg = as.numeric(Pitch)*180/pi) %>% 
  # Grouping images by Date and band
  group_by(Date, BandName) %>% 
  arrange(ymd_hms(DateTimeOriginal)) %>% # Converting DateTimeOriginal to a Date and Time object and arranging in order 
  mutate(GPSLatitude_plot = scale(as.numeric(GPSLatitude)), # These are for clean ggplotting, no other reason to scale
         GPSLongitude_plot = scale(as.numeric(GPSLongitude)),
         cos_SSA = cos(SunSensorAngle_DLS1_rad),
         Irradiance = as.numeric(Irradiance),
         Date2 = ymd_hms(DateTimeOriginal)) # this is the date/time we will use moving forward 

(SSA_plot <- xmp_all_ssa %>%
    ggplot(aes(Date_time)) +
    geom_line(aes(y = SunSensorAngle_DLS2_deg, color = "DLS2 (from metadata)"), linewidth = 1) +
    geom_line(aes(y = SunSensorAngle_DLS1_deg, color = "DLS1 (calculated)"), linewidth = 1) +
    geom_vline(data = subset(xmp_all_ssa, panel_flag == 1), aes(xintercept = as.numeric(Date_time)), color = "grey", alpha = 0.3) +
    geom_line(aes(y = solar_angle), color = "black", linewidth = 1, alpha = 1) +
    scale_x_datetime(date_breaks = "10 min", date_labels = "%H:%M") +
    labs(x = "Time", y = "Sun Sensor Angle", title = paste0("SSA, Site: ",site_to_plot,", Date: ", date_to_plot, ", Camera: ", camera_to_plot))+
    labs(subtitle = "Grey vertical lines are calibration panel images, black hoirzontal line is the solar angle (calculated from lat, long, and flight time)", size = 10) + 
    scale_color_manual(values = c("blue", "red"),
                       labels = c("DLS2 (from metadata)", "DLS1 (calculated)"),
                       name = "Sun Sensor Angle Comparison")+
    facet_wrap(. ~BandName_Wavelength, 
               scales = "free_x", ncol = 2)+ # this will plot each band as its own plot as a check for all data
    theme_bw()
)
```

<!--
```{r eval=FALSE, include=TRUE}
ggsave(plot = SSA_plot, #save out the plot for reference
       filename = paste0(plot_dir, "0_SSA_plot.jpeg"), #change plot_dir to the location you would like to save to
       device = jpeg,
       width = 6,
       height = 6,
       units = 'in',
       dpi = 300,
       bg = 'white')
```
-->

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\0_SSA_plot.PNG")
```

### Scattered:Direct Ratio Estimate

Here we estimate scattered:direct ratio for each flight by relating cosine of the sun-sensor angle to spectral irradiance measured by the DLS2. This relationship, in a perfect world, should give you the scattered irradiance as the intercept, which is independent of angle, and the direct irradiance, which is the slope, and therefore perfectly proportional to sun angle. In reality, these relationships are extremely messy and most data needs to be discarded.

Below are three main steps to find the estimated scattered:direct ratio:

1) First, generate a rolling regression of this linear relationship over a certain time window

2) Second, eliminate all models with poor fits using the R\(^2\) value

3) Third, drop any models with negative slopes or intercepts (physically impossible)

First, we generate a rolling regression of the linear relationship \(I_{\text{spec}} = I_{\text{direct}} \cdot \cos(\theta_{\text{sun-sensor}}) + I_{\text{scattered}}\) via a regression of irradiance on \(\cos(\theta_{\text{sun-sensor}})\) over a specified time window (we used 30 seconds here). 

```{r eval=FALSE, include=TRUE}
# via a regression of Irradiance on cos_SSA over a specified time window (30s here)
mod_frame = xmp_all_ssa_site_date %>%
  drop_na(Date2) %>% 
  drop_na(cos_SSA) %>% 
  drop_na(Irradiance) %>% 
  # Fit a rolling regression
  # for each image, fit a linear model of all images (of the same band) within 30 seconds of the image
  tidyfit::regress(Irradiance ~ cos_SSA, m("lm"),
                   .cv = "sliding_index", .cv_args = list(lookback = lubridate::seconds(30), index = "Date2"),
                   .force_cv = TRUE, .return_slices = TRUE)
# df : summary of models, adding R sqaured and Dates
df = mod_frame %>% 
  # Get a summary of each model and extract the r squared value
  mutate(R2 = map(model_object, function(obj) summary(obj)$adj.r.squared)) %>% 
  # Extract the slope and intercept
  coef() %>% 
  unnest(model_info) %>% 
  mutate(Date2 = ymd_hms(slice_id)) 
# df_params : adding slope (direct irradiance) and y-intercept (scatterd irradiance) values
df_params = df %>%
  dplyr::select(Date:estimate, Date2) %>% 
  # we will have to go from long, with 2 observations per model, to wide
  pivot_wider(names_from = term, values_from = estimate, values_fn = {first}) %>% 
  dplyr::rename("Intercept" = `(Intercept)`,
                "Slope" = "cos_SSA")
# Cleaning up the df
df_p = df %>%
  filter(term == "cos_SSA") %>% 
  dplyr::select(Date:model, R2, p.value, Date2)
# Joining model info, parameter (slope, y intercept) info and XMP data with sun sensor angle and using the linear relationship 
# (spectral_irr = direct_irr * cos(SSA) + scattered_irr) to create % scattered and scattered/direct ratios
df_filtered = df_params %>% 
  left_join(df_p) %>% 
  left_join(xmp_all_ssa_site_date) %>% 
  mutate(percent_scattered = Intercept / (Slope + Intercept),
         dir_diff = Intercept/Slope)
```

Next, we eliminate all models with poor fits. In this case we eliminate models with R\(^2\) < 0.4 and drop any models with negative slopes or intercepts (physically impossible)

```{r eval=FALSE, include=TRUE}
df_to_use = df_filtered %>% 
  mutate(R2 = as.numeric(R2)) %>% 
  filter(R2 > .4 
         & Slope > 0 & Intercept > 0) %>% 
  group_by(Date) %>% 
  mutate(mean_scattered = mean(percent_scattered),
         dir_diff_ratio = mean(dir_diff))
#save out dataframe as an rds
saveRDS(df_to_use,paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\",date,"_rolling_regression_filteredModels_used_plot1.rds"))  #SET PATH to save the rds to 
```

Below we plot the pattern in the rolling regression and make sure you're keeping enough models. If you are loosing too many models, adjust the filters.

```{r eval=FALSE, include=TRUE}
(RR_params <- df_to_use %>%
    group_by(Date, BandName) %>% 
    ggplot(aes(x = Date2, y = percent_scattered, color = R2)) +
    geom_point(data = df_filtered, color = "grey") +
    geom_hline(yintercept = 1, linetype = 2) +
    geom_hline(yintercept = 0, linetype = 2) +
    geom_hline(aes(yintercept = mean_scattered), color = "red4", linewidth = 1) +
    scale_y_continuous(breaks = seq(0, 1, .2), limits = c(0,1)) +
    ggnewscale::new_scale_color() +
    labs(title = paste0("Site: ",site_to_plot,", Date: ", date_to_plot, ", Camera: ", camera_to_plot),
         x = "Time (UTC)")+
    theme_bw(base_size = 16) +
    facet_wrap(. ~ Date, 
               scales = "free"))
```

<!--
```{r eval=FALSE, include=TRUE}

ggsave(plot = RR_params,
       filename = paste0(plot_dir,"1_CheckRollingRegressionParams_plot.jpeg"),
       device = jpeg,
       width = 15,
       height = 10,
       units = 'in',
       dpi = 300,
       bg = 'white')
```
-->

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\1_CheckRollingRegressionParams_plot.PNG")
```

Next, check that the linear relationships you're keeping look realistic. The slope should be steeper for sunny days (i.e. more direct irradiance) and shallower for overcast days. 

```{r eval=FALSE, include=TRUE}
(linear_plots <- df_to_use %>%
    filter(BandName == "Blue") %>% 
    ggplot(aes(x = cos_SSA, y = Irradiance, color = R2)) +
    geom_point(data = filter(df_filtered, BandName == "Blue"), color = "grey30", alpha = .4) +
    geom_point(data = filter(df_filtered, BandName == "Blue" & R2 > .4), aes(color = as.numeric(R2))) +
    geom_smooth(method = "lm", se = FALSE, aes(group = BandName)) +
    lims(x = c(0, 1),
         y = c(0, max(df_to_use$Irradiance))) +
    geom_abline(aes(slope = Slope, intercept = Intercept, color = R2), alpha = .3) +
    labs(title = paste0("Site: ",site_to_plot,", Date: ", date_to_plot, ", Camera: ", camera_to_plot))+
    theme_bw(base_size = 16) +
    scale_color_viridis_c() +
    facet_wrap(. ~ Date, 
               scales = "free_y"))
```

<!--
```{r eval=FALSE, include=TRUE}
ggsave(plot = linear_plots,
       filename = paste0(plot_dir,"2_LinearRegression_plot.jpeg"),
       device = jpeg,
       width = 15,
       height = 10,
       units = 'in',
       dpi = 300,
       bg = 'white')
```

-->

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\2_LinearRegression_plot.jpeg")
```

We recommend checking that there is no excessive spatial pattern in the data by plotting the locations of imagery used in the models. Here you want to ensure that imagery throughout the plot is being used rather than only images from a certain location. The code to do this is below:

```{r eval=FALSE, include=TRUE}
(photos_kept <- df_to_use %>%
    filter(GPSLongitude != 0 & GPSLatitude != 0) %>% # Filter out imgs with GPSLongitude and GPSLatitude of zero, 
    # this is rare and in my experience were corrupted imgs where the XMP could not be properly read
    ggplot(aes(x = GPSLongitude, y = GPSLatitude, color = SunSensorAngle_DLS1_deg)) +
    geom_point(data = df_filtered, color = "grey60") +
    geom_point(size = 3) +
    labs(title = paste0("Site: ",site_to_plot,", Date: ", date_to_plot, ", Camera: ", camera_to_plot))+
    theme_bw() +
    scale_color_viridis_c() +
    facet_wrap(. ~ Date, scales = "free"))

```

Lastly, we compute the scattered/ direct irradiance ratios dataframe from the dataframe of filtered models 

```{r eval=FALSE, include=TRUE}
(ratios = df_to_use %>% 
    dplyr::select(Date, mean_scattered, dir_diff_ratio,
                  GPSLatitude, GPSLongitude, Date2) %>% 
    group_by(Date) %>% 
    mutate(Lat_mean = mean(GPSLatitude),
           Long_mean = mean(GPSLongitude),
           Date_mean = mean(Date2)) %>% 
    distinct(Date, mean_scattered, dir_diff_ratio, Lat_mean, Long_mean, Date_mean))

saveRDS(ratios,paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\",date,"_ratios.rds"))  #SET PATH to save the rds to 

# Print the values to use in the calibration as a check (does this ratio looks reasonable?)
round(ratios$dir_diff_ratio, 2)
```

The ratio for the DLS1 was 0.46 and the mean_scattered component was ~32 W/m\(^2\).

### Computing Horizontal (Corrected) Irradiance 
This is the Fresnel correction, which adjusts for the DLS reflecting, rather than measuring, some of the irradiance that hits it. We aquired this code from [Micasense's GitHub](https://github.com/micasense/imageprocessing/blob/master/MicaSense%20Image%20Processing%20Tutorial%203.ipynb) and converted to R to use in this workflow.

```{r eval=FALSE, include=TRUE}
fresnel_transmission = function(phi, n1, n2, polarization) {
  f1 = cos(phi)
  f2 = sqrt(1 - (n1 / n2 * sin(phi))^2)
  Rs = ((n1 * f1 - n2 * f2) / (n1 * f1 + n2 * f2))^2
  Rp = ((n1 * f2 - n2 * f1) / (n1 * f2 + n2 * f1))^2
  T = 1 - polarization[1] * Rs - polarization[2] * Rp
  T = pmin(pmax(T, 0), 1)  # Clamp the value between 0 and 1
  return(T)
}

multilayer_transmission = function(phi, n, polarization) {
  T = 1.0
  phi_eff = phi
  for (i in 1:(length(n) - 1)) {
    n1 = n[i]
    n2 = n[i + 1]
    phi_eff = asin(sin(phi_eff) / n1)
    T = T * fresnel_transmission(phi_eff, n1, n2, polarization)
  }
  return(T)
}

# Defining the fresnel_correction function 
fresnel_correction = function(x) {
  
  Irradiance = x$Irradiance
  SunSensorAngle_DLS1_rad = x$SunSensorAngle_DLS1_rad
  n1=1.000277
  n2=1.38
  polarization=c(0.5, 0.5)
  
  # Convert sun-sensor angle from radians to degrees
  SunSensorAngle_DLS1_deg <- SunSensorAngle_DLS1_rad * (180 / pi)
  
  # Perform the multilayer Fresnel correction
  Fresnel <- multilayer_transmission(SunSensorAngle_DLS1_rad, c(n1, n2), polarization)
  return(Fresnel)
}

```

Now we put it all together to compute the horizontal irradiance. Here the horizontal irradiance can be thought of as a corrected value for the total irradiance that is reaching a point on the flat ground directly underneath the drone.

```{r eval=FALSE, include=TRUE}
xmp_corrected = xmp_all_ssa  %>% 
  group_by(Date, BandName) %>% 
  #filter(BandName == "Red") %>% 
  #slice_head(n = 900) %>% 
  nest(data = c(Irradiance, SunSensorAngle_DLS1_rad)) %>% # Creates a nested df where each group is stored as a list-column named data, containing the variables Irradiance and SunSensorAngle_DLS1_rad
  mutate(Fresnel = as.numeric(map(.x = data, .f = fresnel_correction))) %>% # Applies the fresnel_correction function to each group of nested data
  unnest(data) %>% #unnesting
  # Joining the ratios
  left_join(ratios, by = "Date") %>% 
  mutate(SensorIrradiance = as.numeric(SpectralIrradiance) / Fresnel, # irradiance adjusted for some reflected light from the DLS diffuser
         DirectIrradiance_new = SensorIrradiance / (dir_diff_ratio + cos(as.numeric(SunSensorAngle_DLS1_rad))), # adjusted for sun angle, 
         HorizontalIrradiance_new = DirectIrradiance_new * (dir_diff_ratio + sin(as.numeric(SolarElevation))), 
         ScatteredIrradiance_new = HorizontalIrradiance_new - DirectIrradiance_new)
saveRDS(xmp_corrected,paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\",date,"_xmp_corrected.rds"))  #SET PATH to save the rds to

```

Now we plot the sensor irradiance along with the corrected horizontal, direct, and scattered irradiance values

```{r eval=FALSE, include=TRUE}
(Sensor_irr <- xmp_corrected %>% 
    filter(BandName == "Blue") %>% 
    ggplot(aes(x = Date2)) +
    geom_point(aes(y = SensorIrradiance, color = "Sensor Irradiance"),size = 1,show.legend = TRUE) +
    geom_point(aes(y = HorizontalIrradiance_new, color = "Horizontal_DLS1"), size = 1, show.legend = TRUE) +
    geom_point(aes(y = DirectIrradiance_new, color = "Direct_DLS1"), size = 1, show.legend = TRUE) +
    geom_point(aes(y = ScatteredIrradiance_new, color = "Scattered_DLS1"), size = 1, show.legend = TRUE) +
    geom_hline(yintercept = 0) +
    scale_color_manual(values = c("Sensor Irradiance"= "black", "Horizontal_DLS1" = "red", "Direct_DLS1" = "orange", "Scattered_DLS1" = "purple")) +
    #lims(y = c(50, 150)) +
    labs(y = "Irradiance", title = paste0("Site: ",site_to_plot,", Date: ", date_to_plot, ", Camera: ", camera_to_plot))+
    theme_bw() +
    facet_wrap(. ~ Date, scales = "free",
               ncol = 3) +
    labs(color = "Irradiance Type"))
```

<!--
```{r eval=FALSE, include=TRUE}

ggsave(plot = Sensor_irr,
       filename = paste0(plot_dir,"4_SensorIrradiance_w_CorrectedIrradiance_plot.jpeg"),
       device = jpeg,
       width = 15,
       height = 10,
       units = 'in',
       dpi = 300,
       bg = 'white')
```

-->

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\4_SensorIrradiance_w_CorrectedIrradiance.PNG")
```

Now that we have corrected the data using the DLS1 method, we can compare the DLS1 values to the DLS2 values to see if correction is needed. The code below creates a dataframe that has the median scatted/direct ratios and median percent scattered irradiance values for the DLS1 method and from the DLS2 sensor. These values will be used to annotate the following comparison plots.

```{r eval=FALSE, include=TRUE}
avg_ratio_data <- xmp_corrected %>%
  filter(!is.na(BandName))%>%
  group_by(BandName) %>%
  summarize(median_ScatteredDirectRatio_DLS2 = median(as.numeric(ScatteredIrradiance) /as.numeric(DirectIrradiance), na.rm = TRUE),
            median_ScatteredDirectRatio_DLS1_calc = median(as.numeric(ScatteredIrradiance_new) /as.numeric(DirectIrradiance_new), na.rm = TRUE),
            median_percent_scat_DLS2 = median(100*as.numeric(ScatteredIrradiance)/(as.numeric(ScatteredIrradiance)+as.numeric(DirectIrradiance))),
            median_percent_scat_DLS1 = median(100*as.numeric(ScatteredIrradiance_new)/(as.numeric(ScatteredIrradiance_new)+as.numeric(DirectIrradiance_new))),
            max_Date_time = max(Date_time),
            max_dir = max(DirectIrradiance, na.rm = TRUE)) %>%
  ungroup()%>%
  mutate(
    Scattered_To_Direct_Ratio_DLS2 = as.character(round(median_ScatteredDirectRatio_DLS2,2)),
    Scattered_To_Direct_Ratio_DLS1 = as.character(round(median_ScatteredDirectRatio_DLS1_calc,2)),
    Percent_Scat_DLS2 = as.character(round(median_percent_scat_DLS2,2)),
    Percent_Scat_DLS1 = as.character(round(median_percent_scat_DLS1,2)),
    x = max_Date_time,
    y = max_dir,
  )%>%
  dplyr::select(c(Scattered_To_Direct_Ratio_DLS2,Scattered_To_Direct_Ratio_DLS1,Percent_Scat_DLS2,Percent_Scat_DLS1, x, y, BandName, camera))

unique(xmp_corrected$CenterWavelength) #make sure all wavebands are present

saveRDS(avg_ratio_data,paste0(dir,date,"\\1_Data\\",MS_folder_name ,"\\CSV\\",date,"_avg_ratio_data.rds"))  #SET PATH to save the rds to

```

Plotting original VS corrected exif irradiance data to compare values from the DLS2 to corrected values (DLS1 method).

```{r eval=FALSE, include=TRUE}
data = avg_ratio_data %>% filter(BandName %in% c("Blue")) # only need to look at one band

(Dir_scat_irr_plot <- xmp_corrected %>%
    filter(BandName %in% c("Blue")) %>%
    mutate(DirectIrradiance = as.numeric(DirectIrradiance),
           ScatteredIrradiance = as.numeric(ScatteredIrradiance),
           Irradiance = as.numeric(Irradiance),
           SpectralIrradiance = as.numeric(SpectralIrradiance),
           HorizontalIrradiance = as.numeric(HorizontalIrradiance),
           # Date_time = ymd_hms(CreateDate),
           Time = format(Date_time, format = "%H:%M:%S"),
           #scaled 
           DirectIrradiance_scaled = as.numeric(DirectIrradiance)*0.01,
           ScatteredIrradiance_scaled = as.numeric(ScatteredIrradiance)*0.01,
           SpectralIrradiance_scaled = as.numeric(SpectralIrradiance)*0.01) %>%
    
    ggplot(aes(Date_time)) +
    geom_line(aes(y = DirectIrradiance, color = "Direct"), linewidth = 1) +
    geom_line(aes(y = DirectIrradiance_new, color = "Direct_DLS1"), linewidth = 1, linetype = "dashed") +
    
    geom_line(aes(y = ScatteredIrradiance, color = "Scattered"), linewidth = 1) +
    geom_line(aes(y = ScatteredIrradiance_new, color = "Scattered_DLS1"), linewidth = 1,linetype = "dashed") +
    
    geom_line(aes(y = HorizontalIrradiance, color = "Horizontal"),linewidth = 1) +
    geom_line(aes(y = HorizontalIrradiance_new, color = "Horizontal_DLS1"),linewidth = 1, linetype = "dashed") +
    
    geom_line(aes(y = Irradiance, color = "Irradiance"), linewidth = 1) +
    geom_line(aes(y = HorizontalIrradiance, color = "Horizontal"),linewidth = 1) +
    geom_line(aes(y = SpectralIrradiance, color = "Spectral"), linewidth = 1.5, linetype = "dotted") +
    labs(x = "Time (UTC)", y = "Irradiance", title = paste0("DLS1 (corrected) vs. DLS2 (metadata) Irradiance, Site: ",site_to_plot,", 
                                                      \nDate: ", date_to_plot,", Camera: ", camera_to_plot,
                                                      "\nScattered_To_Direct_Ratio_DLS1:", data$Scattered_To_Direct_Ratio_DLS1,
                                                      "\nScattered_To_Direct_Ratio_DLS2:", data$Scattered_To_Direct_Ratio_DLS2,
                                                      "\nPercent_Scat_DLS1:", data$Percent_Scat_DLS1,
                                                      "\nPercent_Scat_DLS2:", data$Percent_Scat_DLS2
                                                      )) +
    scale_x_datetime(date_breaks = "10 min", date_labels = "%H:%M") +
    scale_color_manual(values = c("Direct" = "blue", 
                                  "Direct_DLS1" = "lightblue",
                                  "Scattered" = "black",
                                  "Scattered_DLS1" = "grey",
                                  "Horizontal" = "red",
                                  "Horizontal_DLS1" = "orange",
                                  "Spectral" = "forestgreen",
                                  "Irradiance" ="purple"
    ),
    name = "Irradiance (W/m2/nm)")+
    facet_grid(BandName ~ camera, scales = "free")+
    theme_bw()
)
```

<!--
```{r eval=FALSE, include=TRUE}

ggsave(plot = Dir_scat_irr_plot,
       filename = paste0(plot_dir, "5_Irradiance_DLS1_DLS2_plot.jpeg"),
       device = jpeg,
       width = 15,
       height = 10,
       units = 'in',
       dpi = 300,
       bg = 'white')
```

-->

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\5_Irradiance_DLS1_DLS2_plot.jpeg")
```

Refer to the section [Is Correction Needed?](0_Micasense_Irradiance_Correction##Is Correction Needed?) to help decide whether or not your data needs correcting.

Up to this point, we have calculated "corrected" irradiance and sun sensor angles however have not overwritten the exif data within the MicaSense imagery. In this next step, we will overwrite image exif data with the corrected DLS1 values. Before moving on to this step ensure that :

1) You have decided that your data needs correcting

2) You have created a backup of your original data, since you will be irreversibly overwriting the exif data of the imagery. Prior to running the code below we copied all images in our Micasense folder to a folder in the same directory as the Micasense folder however named named Micasense_cor. Imagery in the Micasense_cor folder will be the imagery we will be editing, and the Micasense folder will contain the unedited imagery

3) The MicaSense.config has been downloaded from [GitHub](https://github.com/owaite/GenomeBC_BPG), the file can be found in the "Scripts" folder under MicaSense


```{r eval=FALSE, include=TRUE}
# Writing over exif data with corrected SSA, horizontal irradiance, direct irradiance, and scattered irradiance

corrected_directory <- paste0(dir,date,"\\1_Data\\",MS_folder_name,"_cor\\") #i.e. this folder is called Micasense_cor and is a copied version of the Micasense folder, we will be correcting the exif data of images in the Micasense_cor folder and leaving the Micasense folder untouched with the original exif data
original_directory <- paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\") # path the folder of micasense images

xmp_corrected = readRDS(paste0(dir, date, "\\", MS_folder_name,"\\CSV\\",date,"_xmp_corrected.rds")) %>% #path to corrected xmp data .rds file
  mutate(SourceFile = str_replace(SourceFile,original_directory, corrected_directory), # Changing source file name from the original directory to the corrected_directory. This will result in imagery in the corrected direcotry only to be edited
         TargetFile = SourceFile) #the TargetFiles are the path names to the files to be edited

# As vectors
(img_list = xmp_corrected$FileName)
(targets = xmp_corrected$TargetFile)
(SSA = xmp_corrected$SunSensorAngle_DLS1_rad)

(horirrorig = xmp_corrected$HorizontalIrradiance)
(horirr = xmp_corrected$HorizontalIrradiance_new)
(dirirr = xmp_corrected$DirectIrradiance_new)
(scairr = xmp_corrected$ScatteredIrradiance_new)

targets[1] #checking its the right imgs

for (i in seq_along(targets)) {
  # given a micasense config file, overwrite tags with computed values
  # using exiftool_call from the exifr package
  call = paste0("-config G:/GitHub/GenomeBC_BPG/MicaSense/MicaSense.config", # SET PATH to the config file, this file is on the PARSER GitHub in the same folder as this R script
                " -overwrite_original_in_place",
                " -SunSensorAngle=", SSA[i],
                " -HorizontalIrradiance=", horirr[i],
                " -HorizontalIrradianceDLS2=", horirrorig[i],
                " -DirectIrradiance=", dirirr[i],
                " -ScatteredIrradiance=", scairr[i], " ",
                targets[i])
  
  exiftool_call(call, quiet = TRUE)
  print(paste0(i, "/", length(targets), " updated img:",img_list[i] ))
}
```

Lastly, check the exif data of the first and last image to ensure that the data has been properly overwritten.


<!--chapter:end:0_MicaSense_Irradiance_Correction.Rmd-->

