---
title: "Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials"
author: "Jake King*, Olivia Waite, Miriam Isaac-Renton, Nicholas C. Coops, Samuel Grubinger, Liam Irwin, Lise Van Der Merwe, Jon Degner, Alex Liu"
output: bookdown::gitbook
---

# Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials {-}

<!-- ```{r echo=FALSE,out.width="100%", fig.align = 'center'} -->
<!-- knitr::include_graphics(here("Photos_&_gifs\\Cover_photo_crowns.png")) -->
<!-- ``` -->

```{r echo=FALSE,out.width="100%", fig.align = 'center'}
knitr::include_graphics(here("Photos_&_gifs\\cropped_legend_labels_GenomeBC_cover_blackCrowns.png"))
```

Date last updated: `r Sys.Date()`


*Jake King^1^, Olivia Waite^1^, Samuel Grubinger^2^, Alex Liu^1^, Miriam Isaac-Renton^1^, Nicholas C. Coops^2^, Liam Irwin^2^, Lise Van Der Merwe^3^, Jon Degner^3^, Alvin Yanchuk^3^* 

^1^ Natural Resources Canada, Canadian Forest Services, Canadian Wood Fibre Center, 506 Burnside Road West, Victoria, British Columbia, V8Z 1MZ.  

^2^ Integrated Remote Sensing Studio, Faculty of Forestry, University of British Columbia,2424 Main Mall, Vancouver, BC V6T 1Z4, Canada.

^3^ BC Ministry of Forests, Cowichan Lake Research Station, 7060 Forestry Rd, Mesachie Lake, BC V0R 2N0.

## GitHub link {-}
Below is the link to the GitHub where you can access full scripts referenced in this document

**GitHub Link: ** [GBC-GitHub](https://github.com/owaite/GenomeBC_BPG)

```{r echo=FALSE, results='asis'}
# Define the image paths
img_paths <- c(
  here("Photos_&_gifs/IRSS.png"),
  here("Photos_&_gifs/NRCan_english.PNG"),
  here("Photos_&_gifs/GeneSolve.png"),
  here("Photos_&_gifs/GenomeBC.jpg")
)

# Create the HTML for the images
html <- paste0(
  '<div style="display: flex; flex-direction: row; justify-content: center; align-items: center;">',
  paste0('<img src="', img_paths, '" style="height: 60px; margin: 0 10px;" />', collapse = ""),
  '</div>'
)

# Print the HTML
cat(html)
```


## Acknowledgements {-}
These guidelines were made possible thanks for project funding from Genome British Columbia’s GeneSolve program, the Canadian Forest Service’s Fibre Solutions, 2 Billion Tree programs, and Assistant Deputy Minister’s Innovation Fund. For administrative assistance, we thank Adam Dick, Olivier van Lier, Marlene Francis, Annick Pelletier, Lise Carron, Guy Smith and Amélie Roberge. For practical input, we thank Bill Lakeland, Alec Wilson, Eric Saczuk, Keenan Rudichuk and David Huntley. 

## How to cite this report:{-}

<!--chapter:end:index.Rmd-->

# Abstract

There is significant potential for broad deployment of scalable drone-mounted remote sensing technologies in forestry. For example, forest genetics programs rely on large-scale phenotyping of individual trees throughout pre-selection, selection, and post-selection to inform seed choices that will promote resilient, productive forests. However, the number of sites and trees within sites that can be phenotyped is often constrained by the time and cost of labour. As climate change intensifies environmental stress for forest trees and forestry policies shift towards a focus on climate adaptation, there is increased need for efficient and cost-effective data collection methods to measure key phenotypic traits relating to climate adaptation and resilience. Drone-mounted sensors can be used to quantify a range of standard phenotypic data (i.e. height) and novel traits (i.e. vegetation indices) with data captured over entire trials within hours.

The information and workflows in this guide are informed by a collaborative project. Our team included the University of British Columbia (UBC), Natural Resources Canada (NRCan) and the BC provincial Ministry of Forests (MOF). The project objective was to use remote sensing tools to better understand how trees respond to climatic stressors by analyzing phenotypic (i.e. spectral indices and structural metrics) and genetic correlations at the tree level to guide genetics programs in the selection of trees better able to cope with future climates.

This guide is intended as a resource for the forestry community interested in incorporating remote-sensing tools for phenotyping. While this platform was built on gridded forest genetics trials, the data collection and processing on any gridded forest research trial capturing individual tree data would essentially remain the same. This guide covers ideal site selection for the remote sensing tools, flight preparations, equipment considerations and parameters, data management, step-by-step LiDAR and photogrammetric (multispectral and RGB imagery) workflows, tree georeferencing and crown delineation workflows, and a breakdown of equipment cost. All workflows are illustrated step-by-step with example images and code. This information has been published as a living bookdown document to allow for updates and the incorporation of edits by readers. It is designed to be a start to finish workflow with the final product being individual tree metrics and vegetation indices for analysis.

See the index for a more detailed breakdown of the information provided

<!--chapter:end:0_Abstract.Rmd-->

# Background and basis of knowledge

In Canada and other countries with managed forest industries, operational forestry includes reforestation of harvested or naturally deforested land. In many widespread forest tree species, populations are locally adapted to their climates of origin, meaning that jurisdictions must determine which seed sources can be used in which environments to ensure the health and productivity of planted forests. These decisions are made using common-garden trials, experiments which test different genotypes in a common environment to isolate the genetic component of their physical traits, or phenotype.  For many decades, measurements of height and diameter have been used to identify desirable genotypes for a given environment, in other words, the trees that grow tallest in a climate represent the best seed sources to plant in that climate. 
However, the measurement of traits in common-garden trials, or phenotyping, represents a bottleneck in forest genetics because (1) it is time-consuming and expensive to phenotype thousands of large trees in remote field environments, and (2) simple traits of height and diameter may not capture signals of adaptation and resilience. Drone remote sensing technology has the potential to alleviate the forest genetics phenotyping bottleneck through (1) more efficient phenotyping of traditional target traits representing height and diameter growth, and (2) novel traits, such as spectral vegetation indices, representing foliar physiology and responses to stress.
 
In the western Canadian province of British Columbia (BC), where this research took place, remote sensing tools provide the opportunity for hundreds of forest genetics trials to be assessed for traditional traits like tree size and for novel climate-adaptive traits and tree health assessments. However, before these powerful tools can be adopted, their suitability in different environments must be assessed (i.e. sloped verse flat terrain, open canopy verse crown-closure, etc.).  
 
Over 2022 and 2023 we conducted more than 400 flights on six MOF forest genetics research trials collecting multispectral, LiDAR and thermal data. The six trials, located in two regions near Jordan River and Powell River in coastal BC, represent cool-moist and dry-warm coastal environments. Our repeated field campaigns over two years allowed us to capture several biologically relevant climatic windows, including heat and drought events, and to look at practical considerations on applying these tools by including two morphologically different conifer species at pre-selection and mature ages.  As the project continues into 2024, the methodology will be further refined on new sites and updated throughout this guide.

These guidelines were developed using drone-acquired data collected on forest genetics field trials known as progeny trials. Progeny trials involve crossing specific parent trees and testing offspring in replicated trials across a wide geographic area. Due to known parent-offspring relationships and family structures, it is possible to infer the heritability of a trait, or the proportion of phenotypic variance attributable to transmissible genetic variance. This information is key for forest geneticists since the gains from genetic selection rely on trait heritability and variability (selection intensity). Knowledge of relatedness also makes it possible to infer the value of parent trees and offspring for future crosses for the trait, known as breeding values. Evaluating the correlation of genetic variations among traits is another important component in tree breeding since positive genetic correlations may allow simultaneous selection for two desirable traits (indirect selection) while negative genetic correlations require special treatment to mitigate trade-offs. It is also critical to understand how tree genotypes’ responses vary across environments (genotype-by-environment interactions), since stability of trait expression affects tree breeding and seed deployment.

<!--chapter:end:0_Background.Rmd-->

# Glossary {#glossary}

Census data – Traditional trial data in a spreadsheet with rows and columns denoting each position on a gridded site and a unique identifier for each tree.  It will also contain historical data like height and health assessments.

Common Garden Trial - A type of experiment where different genotypes (often from different locations) are grown in the same environmental conditions to assess how genetic differences influence the phenotype.  This helps isolate the genetic contribution to observed traits by minimizing environmental variation.

Crown delineation - Refers specifically to the process of defining the boundaries of individual tree crowns.  It focuses on outlining the exact perimeter of each tree's canopy.

Crown polygon - a delineated boundary that represents the outer edge or perimeter of a tree's canopy when viewed from above.

DAP – Digital Aerial Photogrammetry is the process of using overlapping aerial images to generate three-dimensional point clouds and orthorectified imagery (orthomosaics

DEM - Digital Elevation Models are 2-dimensional, single-channel images that represent elevation.  Lower values in a DEM correspond to lower elevations, while higher values correspond to higher elevations.  We reference three types of DEMs:

-   DSM – Digital Surface Model:DSMs capture the elevation values from sea level to the surface of the canopy, including buildings, trees, and other objects.

-   DTM – Digital Terrain Model:DTMs represent the bare ground elevation, typically, but not exclusively generated using LiDAR technology.  They exclude trees, shrubs, and other surface objects, showing the "ground" elevation.

-   CHM – Canopy Height Model:CHMs are created by normalizing the elevation values, representing the difference between the DSM and the DTM (CHM = DSM - DTM).  This model specifically highlights the height of the canopy above the ground.

EXIF data (Exchangeable Image File Format) refers to metadata embedded within digital image files that contains information about the image such as the camera settings, date and time the photo was taken, GPS location, and other technical details.

GCP – Ground Control Point.  GCPs are fixed points on the ground of known position that can be seen in the imagery  GCPs are used in photogrammetry data processing to tie images to known points, to aid in alignment and registration and to verify the accuracy of the results.

Genotype - The genetic makeup of an organism, representing the specific set of genes inherited from its parents. The genotype determines the potential traits an organism can exhibit, although the environment may influence the expression of these traits.

GIS – Geographic Information System.  A system designed to view, manage and analyze spatial data.  QGIS and ArcGIS are two commonly used GIS.

GNSS – global navigation satellite system. A broad term for all satellite-based navigation systems, including GPS, GLONASS, Galileo, and BeiDou, providing global positioning and timing data by combining signals from multiple satellite constellations.

GPS – Global Positioning System. A satellite-based navigation system developed by the U.S. government that provides location and timing information, using a network of satellites.

GSD - Ground Sampling Distance.  The distance between the centers of two consecutive pixels on the ground in a digital image. GSD is a measure of image resolution; a smaller GSD indicates higher resolution, providing more detailed imagery.

Orthomosaic – an orthorectified image produced by aligning images. A primary photogrammetric product.

Phenotype - the observable physical and physiological traits of an organism, such as height, leaf color, or drought tolerance. The phenotype results from the interaction of the genotype with the environment.

Progeny Trial **-** an experiment in forest genetics where the offspring (progeny) of specific parent trees are planted and evaluated across multiple locations. Progeny trials are used to assess the heritability of traits, estimate breeding values, and study the genetic variation within a population.

Registration(register) – aligning multi-temporal and or mutisensor orthomosaics to one another.

SOP – Standard Operating Procedures.  Documentation detailing workflows.

UAV – Unmanned Aerial Vehicle. Drone

<!--chapter:end:0_Glossary.Rmd-->

# Range of Sites Assessed

The original six trials this work is based on are three each of Western redcedar (Thuja plicata) and coastal Douglas-fir (Pseudotsuga menziesii var. menziesii). These are two iconic coastal BC tree species that are culturally and ecologically significant. These conifers also play a crucial role in the provincial forestry industry and economy. The sites are low elevation progeny trials in replicated single tree plots planted in a grid. After establishment a spreadsheet is made with each tree’s location recorded with a row and column denoting its position on the site and an identity linking parentage. These sites were chosen due to a wealth of historical phenotypic data available including tree size metrics and responses to biotic or abiotic stressors. They were also chosen for ease of access to enable repeated field campaigns and to comply with visual line-of-sight regulations for flying unmanned aerial vehicles. The trials vary in size, age, and site conditions, each of which plays a considerable role in the challenges involved in data collection and processing.  
Our approach to data collection and processing has evolved with experience.  In March 2024 we began working on a sister site of the East main site from the original six sites.   It is a MOF Genetics Combining Ability (GCA) Douglas-fir trial named “Big Tree Creek”, due to its location near Big Tree creek.  This site is used as an example to illustrate our workflow because it is the most recent site we have worked on and represents the cumulation of practical knowledge gained over two years.  
The Big Tree GCA trial site is located near Sayward on northeastern Vancouver Island, BC. It was planted in 2003 at 2m spacing and tests 84 families with 32 replications. It has a southern aspect with an even 25% slope. Mid-slope, the elevation is 210 m above sea level, with 20 m elevation change over the site. In 2024, during a drone-based data collection campaign, there were 2229 live trees visible in the imagery on 2900 positions. The site had reached canopy closure and our LiDAR derived measurements indicated a median tree height of 12.5m.
Although these guidelines were developed using forest genetics trials, the methodology could easily be applied to other gridded forestry trials or even to non-gridded trials with additional stem mapping workflows.

Figure \@ref(fig:Site-location-map)

```{r Site-location-map, echo=FALSE, out.width="100%", fig.cap = "This guide is informed by lessons learned from the drone-based phenotyping sites on the map and listed in the tables below. Imagery for each site is available through the corresponding links"}
knitr::include_graphics(here("Photos_&_gifs\\map_site_location.png"))
```

## Jordan River
```{r, echo = FALSE}
library(knitr)
library(kableExtra)

# Data as a dataframe
data <- data.frame(
  `Western Red cedar (Cw)` = c("[Cw_East](https://projects.spexigeo.com/cw-jr-east/map/1bf9b0e2-21b5-4e96-a96c-033db45424fd) – planted in 2000, 3980 live trees on 4480 positions ",
                               "[Cw_Sandcut](https://projects.spexigeo.com/cw-jr-sandcut/map/b4d345ed-a69a-43bf-b622-1431504ff041) – planted in 2012, 3500 live trees on 3841 positions"),
  `Douglas fir (Fdc)` = c("[Fdc_East](https://projects.spexigeo.com/fdc-east-gca/map/01275e24-4077-446a-9d31-f2c1f55dbb8e) – planted in 2003, 1612 live trees on 3019 positions ",
                          "[Fdc_W45](https://projects.spexigeo.com/fdc-jr-w45/map/86e4f176-d3d8-466f-895b-5ab21b396ca5) – planted in 2019, 2076 live trees on 2494 positions")
)

# Create the table and center it
kable(data, col.names = c("Western Red cedar (Cw)", "Douglas fir (Fdc)"))

```


## Powell River
```{r, echo = FALSE}
library(knitr)
library(kableExtra)

# Data as a dataframe
data <- data.frame(
  `Western Red cedar (Cw)` = c("[Cw_Rainbow](https://projects.spexigeo.com/cw-rainbow/map/884b78af-30b7-4158-93a4-7516c5c2568b) – planted in 2000, 3674 live trees on 4745 positions"),
  `Douglas fir (Fdc)` = c("[Fdc_Canoe](https://projects.spexigeo.com/9536b041-e269-4d4d-a02a-9311bda57ae8/map/dd07addc-a47c-485b-9bce-5b35f0508611) – Planted in 1999, 1739 live trees on 3164 positions")
)

# Create the table and center it
kable(data, col.names = c("Western Red cedar (Cw)", "Douglas fir (Fdc)"))

```
## 2024 Sites
```{r, echo = FALSE}
# Data as a dataframe
data <- data.frame(
  `Douglas fir (Fdc)` = c("[Big Tree GCA](https://projects.spexigeo.com/fdc-big-tree-gca/map/0c1d3c05-63ff-46a9-b7ba-b85fd3a429a8) – Planted in 2003, 2229 live trees on 2900 positions"),
  `Douglas fir (Fdc)` = c("[Hillcrest GCA](https://projects.spexigeo.com/fdc-hillcrest-gca/map/9a13807e-55ed-4993-b70a-f3a6697c5d7b) – Planted in 2003, 1514 live trees")
)

# Create the table and center it
kable(data, col.names = c("Douglas fir (Fdc)", "Douglas fir (Fdc)"))

```

<!--chapter:end:0_Sites.Rmd-->

# Preflight Planning

## Achieving positional accuracy on multi-temporal and multi-sensor data collections 
For the purposes of this document, we will refer to “alignment” as the process of aligning one flights pictures from one camera to build an orthomosaic. We will use the term “register” to refer to the process of aligning data from different times or sensors.
When applying drone-mounted remote sensing tools in the context of forest genetics field trials, high spatial (positional) accuracy and precision is crucial as individual tree remote sensing derived traits need to be linked to the genetic information of each tree. Correctly assigning remotely sensed phenotypes to individual trees is essential since each tree in a progeny trial has a unique identity associated with a known parentage. In turn, this information is used to estimate genetic parameters and to rank and select genotypes.
Spatial accuracy becomes even more critical when comparing data from multiple sensors across multiple timepoints to ensure accurate multi-temporal tree metrics. This involves choosing data collection and processing procedures that will allow for accurate image alignment to achieve centimeter-level registration between datasets.
The ability to align pictures and register data from different sensors across time is vital to ensure accurate multi-temporal tree metrics.  The alignment and registration occur during the data processing stages, however, decisions made in preflight setup and planning can facilitate these processes by minimizing subsequent manual interventions. 

### Kinematic processing: RTK/PPK 
To attain centimeter-level positional precision, either Real-Time Kinematic (RTK) or Post-Processing Kinematic (PPK) methods are employed. RTK involves real-time correction of positional data during the drone flight, providing immediate high-precision positioning.  To achieve RTK, an on-site GNSS receiver, referred to as a base station (we used either Emlid RS3 or DJI’s D-RTK 2), needs to be near the remote control to stream corrections to the drone. The sensor then writes the corrected, centimeter precise location to the image meta data. We recommend placing the base station in as open a space as possible with a clear skyline to limit connection issues. In cases where cellular service is available, RTK corrections can be streamed from a commercial provider.  
PPK, on the other hand, involves correcting positional data after the flight by processing it against reference data and can potentially achieve higher levels of precision, as it is not subject to communication delays.  It does require specific software, we use [Emlid Studio](https://emlid.com/emlid-studio/) or [RedCatch RedToolbox](https://www.redcatch.at/redtoolbox/).  PPK does not require a base station on site, but it does require a minimum flight time of approximately 10 minutes and a log of GNSS and precise photo timestamp data during the flight.  Before attempting PPK, it is recommended to verify that there is a nearby base station logging GNSS data, ideally within 15km for best results but a maximum distance of 60km.  In Canada, the [Canadian Active Control System (CACS)](https://webapp.csrs-scrs.nrcan-rncan.gc.ca/geod/data-donnees/cacs-scca.php) is a free service (with subscription) provided by NRCan that provides GNSS data from a network of regulated base stations. Many countries have similar options (American version: [NOAA CORS Network - National Geodetic Survey](https://www.ngs.noaa.gov/CORS/)) and commercial providers are available for some locations. 
We used RTK for our data-collection campaigns, however PPK is an excellent alternative for sites without adequate skyline above the base station. PPK is also a suitable alternative for flying multiple sites in one area, where a base station could be set up in the morning and used to fly several nearby sites in one day.  Another big draw for PPK is that it does not necessarily require the purchase of a base station.
RTK/PPK precision is reliant on the sensor being used.  For RTK, the sensor must be able to communicate with the drone and write the RTK precise corrections to the image metadata and for PPK the sensor must be able to log GNSS and timestamp data.   
Some sensors are unable to utilise RTK or PPK information to register the data. In these situations, we rely on image-to-image registration where high quality data is acquired from a sensor which has the RTK capacity and then we match the uncorrected data to the corrected data to ensure alignment and registration.

### Ground Control Points (GCP) 
GCPs are fixed points on the ground of known position that can be seen in the imagery.  With RTK/PPK procedures GCPs are not necessary to attain centimeter level spatial precision, however without GCPs we don’t have a centimeter precise gauge to measure the accuracy of data registration. 
We used 4-6 GCPs per site and built them ourselves using 2-foot square heavy-duty plywood painted as a four-quadrant checkerboard with black and white (Figure X).  Our aim was for them to be left on site and be long lasting, but lightweight versions can be purchased or made.
GCPs are used in photogrammetry data processing to tie images to known points, to aid in alignment and registration and to verify the accuracy of the results.  To use a GCP as a tiepoint, it needs to have a precisely surveyed location. We do this by using RTK, only instead of the base station streaming corrections to the drone, it is streaming corrections to a second GNSS receiver defined as the “rover”.  The rover we are using is a second Emlid RS3 on a staff, which can record a GPS position with centimeter accuracy. As with the base and the drone, both the base and rover must have unimpeded skylines to connect to enough satellites to obtain a reliable position (RTK centimeter precision). When a closed canopy prevents this, GCP coordinates can be leveraged from an RGB orthomosaic developed from an RTK/PPK corrected sensor. This alternative method is not as precise as using surveyed GCPs but nevertheless provides check points to measure error during imagery alignment and registry. Without GCPs, especially on sites where canopy closure has occurred, it becomes difficult to measure the precision of the registered images.  Some sites do have natural features such as rocks that can be used to estimate error. 

 Here in Figure \@ref(fig:GCP-figure) 

```{r GCP-figure, echo=FALSE, out.width="100%", fig.cap = "Aerial view of a GCP at the 12-year old Western redcedar Sandcut site taken with the P1 sensor."}
knitr::include_graphics(here("Photos_&_gifs\\Tree_tops_GCP.png"))
```

### Absolute and Relative Reference with Precise Point Positioning (PPP) 
It is important to consider the difference between relative and absolute reference accuracy when considering methodology.  When setting up a base station in remote locations, the absolute accuracy is meter-level.  This means, that if you set it up at the same spot next week or forty years from now it would only give you a position within approximately 1-5 meters.  Relative reference accuracy is the within-survey or flight precision and with RTK or PPK, it is centimeter-level precision. If the objective is to align and register flights under similar conditions within a short timeframe, precise relative accuracy alone may be sufficient. However, for long time series and legacy projects, it is more desirable to have both centimeter-level absolute and relative accuracy. 
Precise absolute accuracy can be obtained using a variety of methods. If cellular service and a subscription to a provider is available, accurate coordinates can be streamed via RTK.  Network RTK providers in Canada are mostly private enterprise, the NRCan [RTK Networks](https://webapp.csrs-scrs.nrcan-rncan.gc.ca/geod/data-donnees/rtk.php?locale=en) page has information on availability.  If within range of a base station as outlined above, PPK will provide absolute accuracy. For many experimental trials established in remote forest locations, however, neither of these conditions may be met. Accordingly, another method to secure precise absolute accuracy in remote locations is logging GNSS data from a base station in the field for more than 4 hours and later submitting the log to the free [NRCan Precise Point Positioning (PPP)](https://webapp.csrs-scrs.nrcan-rncan.gc.ca/geod/tools-outils/ppp.php?locale=en) website. The PPP website can provide 3 levels of accuracy depending on the latency period.  We have been accepting the rapid, which is accurate to approximately 5 cm when an Emlid RS3 log is submitted more than 12 hours after the end of each day (as quoted on the [Emlid site](https://docs.emlid.com/reachrs/tutorials/post-processing-workflow/nrcan-workflow/)).  Whether absolute accuracy and PPP has been a goal or not, if repeated flights are expected, before leaving the site it is important to mark the base station centre point with a metal survey pin as a “monument”, ensuring the base station can be set up over the same position for subsequent flights.  For example, if using the DJI DRTK2 system you can enter the known PPP’d base station position into the remote controller for absolute accuracy on all subsequent flights. Similarly, if you are using the Emlid you can enter the PPP’d position in the base station app called Emlid flow.  We have developed in house Standard Operating Procedures (SOP) for many of the workflows, and they can be accessed through a GitHub folder and are linked thoughtout this document.  See the [SOPs folder](https://github.com/owaite/GenomeBC_BPG/tree/main/SOPs) for SOP_Emlid_RS3 and SOP_M300_Flight for more details on the Emlid and DJI RTK2 setups.  Once a site has been set up using precise absolute and relative accuracy, even if the monument or GCPs become inaccessible, accurate positions can be reliably obtained by following similar processes.

### Terrain Following on Sites with Elevation Change 
Most flight planning software references the elevation at takeoff. In this scenario, the drone flies at a constant altitude relative to the takeoff point, which is acceptable if the terrain remains somewhat constant. However, a constant flight altitude over sloped or variable terrain increases the risk of collision and results in inconsistent imagery across the acquisition. Terrain following flights reference the altitude relative to a Digital Elevation Model (DEM).  A DEM is a 2-dimensional georeferenced raster with elevation values. This can be a Digital Surface Model (DSM), which references the surface (canopy) elevation, or a Digital Terrain Model (DTM) which references the bare ground.  In situations where the canopy is relatively level for the whole flight a recent DSM is more desirable as this provides the most accurate reference to the canopy, which is our area of interest.  This has not been possible for most of the coastal sites we have been using as there are often large (40-60 meter) “leave” trees near the site perimeter which would create greater changes in the flight altitude and reduce consistency therefore, we have been using DTMs for terrain following.  With terrain following, the drone automatically adjusts its altitude to remain constant above the DEM, regardless of changes in canopy elevation. This ensures the focal length and overlap remains consistent, preserving data quality and accuracy. Any flights with significant vertical change should be flown with terrain following enabled. For terrain following to be effective, an accurate DEM is required.  We use 1 meter resolution DTMs for our Terrain Following flights.  DTMs are becoming more available worldwide, and in BC we can access free LiDAR derived DTMs at 1m accuracy from [LiDAR BC](https://governmentofbc.maps.arcgis.com/apps/MapSeries/index.html?appid=d06b37979b0c4709b7fcf2a1ed458e03) for many areas of the province.  Remote sites outside of these areas require building your own DTM by first flying the site with LiDAR and processing the LiDAR to DTM ahead of the photogrammetry flights.  We have made our own DTMs for approximately half of the sites we have flown.  Figure \@ref(fig:terrain-following-figure) depicts the difference between a flight plan referencing the takeoff point, and a flight plan using terrain following.  The blue lines represent the field of view of the drone mounted sensor.  Note the red ovals where the overlap decreases as the ground rises on the top image when the flight elevation remains constant to takeoff.

Figure \@ref(fig:terrain-following-figure)
```{r terrain-following-figure, echo=FALSE, out.width="100%", fig.cap = "Terrain following on variable topography."}
knitr::include_graphics(here("Photos_&_gifs\\Terrain_following.PNG"))
```

## Site Selection Considerations
Preferred site conditions and considerations when selecting sites prior to undertaking a drone-based phenotyping trial.

**Ideal sites:**Ideal remote sensing sites are on well maintained, relatively flat ground with juvenile trees, just before the tree crowns begin encroaching on each other.  At this stage, competition between trees will not play a significant role, allowing for clearer data collection and analysis.  Douglas-fir trees at 4m tall on sites established at 2m spacing are a good example. The imagery below is of a five-year-old Douglas fir trial.

```{r ideal-site-figure, echo=FALSE, out.width="100%", fig.cap = "Aerial view of a 5 year old Douglas fir trial."}
knitr::include_graphics(here("Photos_&_gifs\\ideal_sites.png"))
```

**Effects of topography: Warning to take extra care**

- Sites on ground with significant elevation change require extra care registering imagery and may require repeated alignment steps with manual intervention. Figure \@ref(fig:GCP-elevation) below show processed orthomosaic imagery from a multispectral camera with GCP points.  The left is from a relatively flat site where all 4 GCPs were within 5cm of the template.  The right side shows data collected with the same parameters in similar conditions and processed with the same settings on a sister site with an elevation change of 25m across the site.  We flew the site with terrain following enabled to maintain consistant height above the canopy, but the initial error was measured at 28cm.  Repeated alignment steps were able to reduce this error to within 10cm for the final product, but this required hours of reprocessing and manual intervention.

Figure \@ref(fig:GCP-elevation) 

```{r GCP-elevation, echo=FALSE, fig.show="hold",fig.align='center', out.width="50%", fig.cap = "Effect of topograghy on alignment. Left: GCP alignment on flat ground (Error ~4cm). Right: GCP alignment on ground with elevation change (Error ~28cm)"}
# Include the pre-brushing and post-brushing images
knitr::include_graphics(c(
  here("Photos_&_gifs/GCP_flat_ground.png"),
  here("Photos_&_gifs/GCP_elevation_change_terrain.png")
))
```

<!--
```{r echo=FALSE, results='asis'}
# Define the image paths
img_paths <- c(
  here("Photos_&_gifs/GCP_flat_ground.png"),
  here("Photos_&_gifs/GCP_elevation_change_terrain.png")
)

# Define the captions
captions <- c(
  "a. GCP alignment on flat ground (Error ~4cm)",
  "b. GCP alignment on ground with elevation change (Error ~28cm)"
)

# Create the HTML for the images with captions
html <- paste0(
  '<div style="display: flex; justify-content: center; gap: 20px;">',  # Centered horizontally with a gap between
  paste0(
    '<div style="text-align: center;">',
    '<img src="', img_paths, '" style="height: 350px; margin: 0 10px;" />',
    '<p style="margin: 0;">', captions, '</p></div>', collapse = ""  # Captions are directly below the images
  ),
  '</div>'
)

# Print the HTML
cat(html)

```
-->

**Site Maintenance:Will produce cleaner data.**

- Sites should be maintained to remove competing herbaceous plants and ingress.  On younger sites, summer brush growth around smaller trees will affect the imagery, so should be cut back from the trees.  To better understand the impact of brush on metrics derived from imagery (RGB or multispectral) imagine viewing the trees from an aerial perspective and drawing a polygon around the crown: Any brush or ingress that is within that polygon could contribute to incorrect metrics. 

- Cutting and piling woody debris on heavily overgrown sites can be problematic.  To produce accurate tree metrics, we rely on LiDAR sensors to create an accurate DTM.  At a heavily overgrown Western redcedar site we made the decision to have the ingress cut out to enable us to see the crowns. The LiDAR derived height compared to traditional measurements was calculated before and after brushing and was found to be significantly affected by the heavy piled debris.  Hence the decision on whether to remove competing vegetation should be based on the desired outcomes. Ideally, choose sites that have been well maintained.

Below Figure \@ref(fig:brushingEffect) shows an example of the effect of brushing on crown visibility at a moist, high-productivity common-garden trial.

```{r brushingEffect, echo=FALSE, fig.show="hold",fig.align='center', out.width="50%", fig.cap = "Effect of brushing on crown visibility. Left: early August prior to brushing. Right: late August post brushing"}
# Include the pre-brushing and post-brushing images
knitr::include_graphics(c(
  here("Photos_&_gifs/pre_brush.jpg"),
  here("Photos_&_gifs/post_brush.jpg")
))
```

**Mature Sites:** 

- Mature sites with crown closure require more manual editing for crown delineation.

- Example: A 5-year-old site (2500 trees) took 2 days of manual processing, while a 25-year-old site took over a week due to neighbouring branch interference.

-	Competition in mature sites can obscure smaller trees, reducing visibility in imagery.

-	Example: At the East Main Douglas fir site, 1612 live trees were surveyed, but only 1514 crowns were delineated, with 96 trees partially obscured.

**Juvenile Sites:** 

-	Small, thin trees in young sites may reduce confidence in extracted values within crown delineations.

-	Example: A 3-year-old Douglas-fir trial had difficulty identifying trees under 1m due to nearby brush, with thin canopies further reducing reflectance accuracy.

-	Similar concerns arose for western redcedar trees under 1.5m.

**Site takeoff and landing requirements:**

- [Transport Canada regulations](https://tc.canada.ca/en/aviation/drone-safety/learn-rules-you-fly-your-drone) dictate that Visual Line of Sight (VLOS) must be maintained with the drone.  A landing zone must be carefully chosen and have the following characteristics.

- Take-off/landing safety:The area designated for take-off and landing should be free of overhanging branches and surrounding large trees, which could create connection and safety issues.

- Multispectral panel calibration:Multispectral sensors require calibration photos to be taken. These photos are required to be acquired in as large an open area as possible so that the data collected by the DLS correctly mimics the flight conditions. Panel Calibration procedures are detailed [here]((https://github.com/owaite/GenomeBC_BPG/tree/main/SOPs)) in our SOP_MS_field document.

- For RTK flight:The Remote Control (RC) and base station are connected via Wi-Fi for flight.  The effective range of this Wi-Fi requires set up near the landing zone and a clear skyline so connectivity can be established and maintained throughout the flight.  Consider PPK when this is not feasible.
  
- Should not be on a major road: Dust and traffic create difficulties, as do bystanders.
  
- It is recommended to fly sites from a higher vantage point. At one site, the only area with an acceptable line-of-site was approximately 500 meters uphill from the site. Therefore, we flew from this distance to maintain VLOS.

- Confirm that the site is in unrestricted airspace (in Canada) by checking the [Drone Site Selection Tool](https://nrc.canada.ca/en/drone-tool/) or be willing to apply for permissions.  Only pilots with an advanced license can apply for permission to fly in restricted airspace.  Permission is granted through the [NavDrone](https://map.navdrone.ca/) portal or app.


## Drone and Sensors: What will I need to purchase and how much will it cost?

The biggest capital investment will be for the sensors and drone.  Specific sensors and drones will be dictated by use case.  

Our research was looking for sensors with the following attributes:  


-   High quality RGB: capable of producing imagery that can differentiate branches from one tree to the next to delineate crowns and with high enough resolution to distinguish a Douglas fir plot tree from an ingress Western Hemlock.  It would also need to have centimeter level positional accuracy. 

- 	Multispectral sensor:   The multispectral is the most important sensor to purchase and the final project results are based on its performance across a range of conditions.  We wanted a multispectral sensor capable of calculating a broad range of Vegetative Indices (VI).  There are more options for multispectral sensors if 5 or 6 wavelengths are okay.

- 	LiDAR: for its ability to penetrate through canopy to create accurate DTMs and to calculate structural metrics for each tree.

- 	Thermal: to identify trees suffering from heat stress through thermal variations.

We wanted a drone capable of carrying each of these sensors with the added ability to carry extra weight if a drone mounted sampling system could be implemented.  We chose the DJI Matrice 300 RTK (M300), an enterprise drone.  The newer model released in 2023 is the M350 with minor upgrades.

We recommend the Emlid RS3 as a base station for RTK/PPK corrections, and a second RS3 as a rover to collect GCPs or stem mapping.

Other options:
The DJI Mavic 3 Multispectral is an all-in-one drone with a 4-band multispectral/RGB sensor capable of RTK. While this option does not support LiDAR or other sensors, it could be a viable solution if the limited vegetation indices available through this sensor are acceptable and the sites have enough ground exposure to allow for DTMs to be created with DAP if structural metrics are desired. It does not measure the downwelling radiance which may cause issues if correcting and comparing imagery over time.

The Senterra 6X, is a six-band (five plus RGB) sensor that is also RTK capable. Senterra offers the ability to custom-fit wavelengths. MicaSense also produces several RTK-enabled multispectral sensors worth considering, including the Altum PT, which features five multispectral bands with pan sharpening and an integrated thermal camera.

Table \@ref(tab:droneTable) shows a breakdown of drone and sensor costs, where newer versions are available, both are listed.  The * would be a “wishlist” full set up.  This is just the major purchases and doesn’t include general field equipment.  The section at the bottom details some options untested by our group.

### Hardware Costs

Table \@ref(tab:droneTable)
```{r droneTable, echo = FALSE}
library(knitr)
library(kableExtra)

data <- data.frame(
  Hardware  = c("*Zenmuse P1, RGB",
                "Micasense RedEdge-MX Dual",
                "*Micasense RedEdge-P Dual (Panchromatic)",
                "Zenmuse H20T, Thermal and RGB",
                "Zenmuse L1, LiDAR and RGB",
                "*Zenmuse L2 LiDAR and RGB",
                "DJI Matrice (M)300 RTK",
                "*DJI Matrice (M)350 RTK",
                "*DJI TB65 batteries",
                "*DJI TB65 charging station",
                "DJI DRTK2",
                "*Emlid RS3",
                "*Emlid RS3"
  ),
  Purpose = c("Sensor",
              "Sensor",
              "Sensor",
              "Sensor",
              "Sensor",
              "Sensor",
              "Drone",
              "Drone",
              "Drone batteries",
              "Charging station",
              "GNSS receiver",
              "GNSS receiver",
              "GNSS receiver"
  ),
  Cost = c("$9,000",
           "$16,000",
           "$22,500",
           "$13,350",
           "$11,600",
           "$16,660",
           "$12,000",
           "$13,500",
           "$1,910",
           "$1,300",
           "$4,200",
           "$3,600",
           "$3,600"
  ),
  Detail = c(
  "Positional accuracy - Horizontal: 3 cm, Vertical: 5 cm. Ground Sampling Distance (GSD) - 1cm at 80 m elevation",
  "Ten band multispectral camera, no longer in production",
  "Ten band multispectral camera with Panchromatic sharpening",
  "Thermal and RGB sensor",
  "LiDAR sensor released in 2020",
  "LiDAR sensor released in 2023, now with 5 returns and better accuracy",
  "2020 released DJI enterprise drone with RC, intelligent battery case and one set of batteries",
  "2023 released DJI enterprise drone with RC plus, intelligent battery case and one set of batteries",
  "Pair of the newer model M300/350 batteries. Good for one approximately 30-minute mapping flight including safety margins.  At least 4 pairs are recommended for continuous flight when paired with a small generator",
  "Intelligent batery charging station in a hard carrying case for the newer batteries.",
  "GNSS receiver for use as base to stream RTK to drone",
  "GNSS receiver for use as base to stream RTK to drone and rover",
  "GNSS receiver for use as rover for centimeter precise GCP or stem mapping."
  )
  
  
)
# Create the table and center it
kable(data, col.names = c("Hardware", "Purpose","Cost","Detail"),caption = 'A breakdown of drone and sensor costs, where newer versions are available, both are listed.  The * would be a “wishlist” full set up.  This is just the major purchases and doesn’t include general field equipment.')

```



Table \@ref(tab:Untested-droneTable) are some untested options:


```{r Untested-droneTable, echo = FALSE}
data <- data.frame(
  Hardware  = c("DJI Mavic 3 Multispectral0",
                "Senterra 6X",
                "Micasense Altum PT"
  ),
  Purpose = c("Sensor, Drone",
              "Sensor",
              "Sensor"
  ),
  Cost = c("$5,945",
           "$17,120",
           "$25,950"
  ),
  Detail = c("RTK capable drone, RGB and 4 multispectral wavelengths",
             "RTK capable, 5 multispectral plus RGB, potential to customize wavelengths, but extra price unknown",
             "Thermal, and multispectral panchromatic, RTK capable sensor"
  )
)
# Create the table and center it
kable(data, col.names = c("Hardware", "Purpose","Cost","Detail"), caption = 'Options that we would like to test in the future but have not yet had the chance to.')
```

An excellent guide for starting costs is detailed in [Getting your drone research off the ground](https://www.silva21.com/extensionnotes?lang=en)

### Drone Training in Canada
In Canada, drone laws stipulate that a license is required to fly any drone weighing over 249 grams. There are two types of drone licenses: basic and advanced. The details for each type are available on the [Transport Canada (TC)](https://tc.canada.ca/en/aviation/drone-safety/learn-rules-you-fly-your-drone/find-your-category-drone-operation) webpage. Flights conducted in uncontrolled airspace only require a basic license.  If you have experienced pilots on staff and are only flying basic operations, the extra training might not be needed, however, we found the extra training and work for the advanced course to be beneficial and we highly recommend the [Indro Robotics](https://indrorobotics.ca/) training which can be tailored to fit most use cases. To prepare for the basic license The [Don Drones On](https://www.youtube.com/watch?v=ut44cLyXWLk) website has an excellent video.  

<!--chapter:end:0_Preflight_Planning.Rmd-->

# Data Collection and Flights

## Hardware - DJI Matrice 3000 RTK (M300)

```{r M300-flying, echo=FALSE, out.width="100%", fig.cap = "DJI Matrice 300 RTK in flight over one of our Vancouver Island field sites, taken by Alex Liu with a Mavic3."}
knitr::include_graphics(here("Photos_&_gifs/M300_flying.png"))
```

A complete SOP for mapping flights with the M300 was developed in-house and can be found in the [SOPs folder](https://github.com/owaite/GenomeBC_BPG/tree/main/SOPs) on GitHub. Transport Canada (TC) regulations require logging all flights and to have the logs on-hand when flying. We have developed a simple spreadsheet that logs the details required by TC and other useful details for each flight. When undertaking a project with repeated data collection campaigns, we would recommend developing a similar approach to organizing these data. A copy of the one we use is available within the [Example Documents](https://github.com/owaite/GenomeBC_BPG/tree/main/Example_Documents) folder on GitHub.

While the DJI M300 drone can fly in heavy rain, wind and under any light conditions, flying under such conditions can damage your equipment or create artifacts in the resulting data and generally reduce data quality and usability. Each sensor has specific requirements for producing the best-quality data, outlined in greater detail below. It is highly recommended to keep the flight app and all hardware up-to-date and to familiarize oneself with the “Release Notes” before each trip as the DJI platform evolves quickly. App and hardware updates require Wi-Fi, so updating in a location with service is crucial, ideally you can do this before visiting the field.

## Battery Management

Managing battery life is an ongoing concern when flying drones. The [DJI M300 documentation](https://enterprise.dji.com/matrice-300/specs) claims 55-minute flight time without a payload on one set of batteries. We have found flight time with a payload on newer batteries to be 30-35 minutes and 25-30 on older batteries. To keep enough batteries charged for a field day we start the day with four sets of charged batteries in the battery station and bring a small generator into the field to charge batteries as they are used.

The M300 RTK system is compatible with the TB60 and TB65 batteries. However, it is important to note they cannot be mixed when used on the drone’s two battery sockets. The TB60 and TB65 batteries have different voltage ratings and internal configurations, and mixing them will lead to imbalanced power distribution, and potentially causing power fluctuations or damaging the drone's power system.

For each set of batteries, it is also important to differentiate between them. If batteries are used interchangeably, their charge and discharge rates will quickly begin to vary and so will their capacity. Therefore, it is required to mark them in pairs and only use them as such.

It is possible to manually set the discharge rate for each individual battery when it is slotted in the drone. The setting can be found under the battery logo of the triple dot menu in camera view. This can be helpful when you may have a few days before the next set of flights. By default, DJI batteries will automatically discharge to 95% after twelve hours, but after the user set time in days it will begin to discharge to 50-60%, the default setting is 2 days before safe discharge occurs.

## Workflow: Site Reconnaissance

Ahead of the first data collection flights we follow the reconnaissance workflow outlined in Figure \@ref(fig:Reconnasissance-flowDiagram). This is to assess for hazards, confirm site perimeter, layout GCPs and collect their coordinates (where possible), and to locate trees to enable georeferencing existing trial data to the imagery. We followed this workflow in March 2024 at the Big Tree creek trial where were able to find suitable locations to install GCPs and were able to collect precise GPS points for the GCPs. We also measured azimuth and distance from the GCPs to the nearest trees to enable us to tie the row column grid to the imagery (georeferencing) later in the processing. Please refer to the [SOP_Emlid_RS3](https://github.com/owaite/GenomeBC_BPG/tree/main/SOPs) for detailed instructions on using the Emlid GNSS receivers. The [Emlid RS3 documentation](https://docs.emlid.com/reachrs3/) is excellent and covers a wide range of applications.

```{r Reconnasissance-flowDiagram, echo=FALSE, out.width="100%", fig.cap = "Reconnaissance workflow."}
knitr::include_graphics(here("Photos_&_gifs\\Reconnasissance_flowDiagram.PNG"))
```

## Data Collection: Flight Planning

### Flight planning theory

In drone photogrammetry, Ground Sampling Distance (GSD) is a critical measure that represents the pixel size on the ground, directly influencing the resolution of the images captured. A smaller GSD indicates higher detail, meaning that more information is available in each image. The GSD is primarily determined by the drone's altitude and the camera's resolution—lower altitudes result in a smaller GSD, offering finer detail in the images.

The primary variables in the flight planning software are altitude, speed, and overlap (both front and side). Higher altitudes increase GSD, covering more area but reducing image resolution. Speed must be adjusted carefully; slower speeds ensure better image overlap and minimize motion blur, which is especially important in windy conditions. Overlap is critical for ensuring that each image captures enough common ground with adjacent images to allow for accurate 3D reconstruction and data processing. Higher overlap generally improves the quality of the final data but also increases the number of images required and the overall length of the flight. The required overlap is complicated by the complexity of the surface. We have found that attaining high quality output imagery on bare ground requires considerably lower overlap. This is due to two reasons, first, displacement of free moving objects on bare ground, such as sticks and rocks commonly found on recently logged trials, requires very strong winds and is overall a less complex surface than a closed canopy with thin coastal treetops moving in a breeze. The second reason is the elevation of the canopy itself. Unless you are using a DSM the overlap calculation is based on the elevation from the ground. When flying over mature sites with closed canopy the effective altitude is the flight altitude minus the canopy height. This will decrease the effective, or canopy, overlap which is an important consideration when collecting data over what is already a complex structure.

Figure \@ref(fig:effective-overlap) outlines the difference in ground overlap and canopy overlap. We have developed a calculator in which you enter the flight altitude, overlaps, and canopy height and it will output the canopy overlap. You can then adjust the inputs until you get the required overlap. It is available in the [Example Documents](https://github.com/owaite/GenomeBC_BPG/tree/main/Example_Documents) folder on GitHub. Figure \@ref(fig:effective-overlap) below is an example calculation detailed in the next paragraph.

```{r effective-overlap, echo=FALSE, out.width="100%", fig.cap = "Effective overlap calculation."}
knitr::include_graphics(here("Photos_&_gifs\\effective_overlap.png"))
```

### Flight planning with DJI Pilot

There are various flight planning software options. We chose the DJI Pilot app, which comes installed on the remote control (RC), for its ease of use. The Big Tree creek Douglas fir site has approximately 2900 gridded tree planting positions at 2 m spacing. Because there was no accurate polygon for the site prior to the project, we marked positions for GCPs and made waypoints for the corners of the site with the DJI Pilot app during the reconnaissance flight. We then built the flight plan polygon from the corner points and added an extra margin for the first flights. Once the site corners were confirmed, for subsequent flights, we remade the polygon in a Geographic Information System (GIS) and clipped to 15 meters outside the edge trees. This polygon is the flight shape and was 2 Ha for the Big Tree site. While doing this reconnaissance flight, the pilot scouted the site for hazards and noted that, within 20 meters of the site, there were mature wildlife trees left standing after the last harvest. The median tree height for plot trees was 12.5m however, the wildlife trees were approximately 45 m tall. To allow for a 10m safety margin above the wildlife trees the minimum elevation was set to 55 m. Additionally, the site is on a 25 percent slope, which required terrain following. Flight planning is sensor-specific, after entering the sensor, the app presents a list of options. See Figure \@ref(fig:P1-flightPlan) for a visual of the site polygon and flight parameters for a P1 flight. The recommended flight parameters for each sensor are quoted in the next section.

```{r P1-flightPlan, echo=FALSE, out.width="100%", fig.cap = "Flight planning a P1 flight on the remote controller. Colored lines represent terrain following elevation."}
knitr::include_graphics(here("Photos_&_gifs\\P1_flight_plan.png"))
```

## Data Collection: Sensors, Parameters, and Ideal Conditions

The parameters we have listed for each sensor are based on collecting data on gridded 1.5 - 2 Ha closed canopy coastal Douglas fir and Western redcedar sites. The parameters are conservative and should allow for less than ideal flight conditions.

### MicaSense: 10-Band Spectral Sensor for Vegetative Indices

The MicaSense RedEdge-MX Dual (MS) and its replacement, the Micasense RedEdge-P Dual (MS_P), (where P represents panchromatic), are 10-Band multispectral sensors that detect reflectance in ten specific wavebands of reflected light that provide useful information on vegetation health. The MS_P includes an 11th camera that is senstive to a wide range of wavelengths across the visible and NIR spectra, allowing it to capture a much larger amount of light (photons) per capture, and thus producing a higher spatial resolution product that allows for pansharpening.

The MicaSense (MS) cameras are powered by the M300, but the gimbal mount only provides power; the camera itself is not integrated and does not communicate with the drone or remote controller. The camera is started via a laptop or phone though a Wi-Fi port connection housed on the MS and runs until the camera is linked again and stopped or the drone is powered down.

The multispectral cameras are passive sensors and as such are sensitive to changing light conditions. To detect these changes Micasense cameras use a Downwelling Light Sensor (DLS), which is mounted to the top of the M300, to measure ambient light conditions during flight and sun to sensor angles. These measurements are written into the metadata of each photo. We are using the DLS2 which measures the ambient light and sun angle using 10 small circular sensors spread across the top of the DLS. Hence, it is imperative that these sensors are clean to avoid erroneous measurements.

Multispectral sensors also require calibration photos to be taken at the start and end of each flight. This involves taking pictures of a specialized MicaSense calibration panel with known reflectance values. These pictures need to be taken in an open area for the DLS to correctly mimic the current flight conditions, ideally the location is large enough to avoid scattering from nearby vegetation or other objects. Ensure there are no shadows being cast onto the panel or over the DLS as these calibration pictures are taken, either from nearby objects, vegetation, or the person taking the photos.

As seen in Figure \@ref(fig:DLS-Micasense-connection) the MicaSense Dual cameras are two sets of cameras, each captures 5 unique wavebands. The cameras write images as .tif with an \_(x) for each of (x) wavelengths and organizes the images into folders with 1000 images per folder, in as many folders as needed. Image capture rate can be set as high as one image per second. A 25–30-minute flight that covers a 1.5-2 Ha site using the flight parameters outlined below will yield 5-6 folders per camera system producing 10-12 thousand images.

The MS sensors are particularly prone to difficulties writing to data cards and the data must be carefully checked and reflown if necessary. We keep a spare set of SD cards for each sensor and will not overwrite any cards until the data has been confirmed by loading into the processing software. For the MS sensors, ensure SD cards used are formatted FAT32, they will not work in EXFAT32 or other formats. Formatting cards should be done through the MicaSense wireless connection using the built in format card button.

Figure \@ref(fig:DLS-Micasense-connection)

```{r DLS-Micasense-connection, echo=FALSE, out.width="100%", fig.cap = "Micasense RedEdge-MX Dual and the DLS2 mounted on the M300. Here we are connecting the DLS2 to the Micasense camera."}
knitr::include_graphics(here("Photos_&_gifs\\DLS2_Micasense_connection.png"))
```

**Sensor Specifications:**

-   **Camera Type:** MicaSense Dual cameras

-   **Image Format:** .tif with a suffix indicating the band (e.g., \_1, \_2)

-   **Bands/Wavelengths:**

```{r MS-Table, echo = FALSE}
data <- data.frame(
  Band_Name  = c("Blue",
                 "Green",
                 "Red",
                 "Red Edge",
                 "NIR",
                 "Panchromatic",
                 "Coastal Blue",
                 "Green",
                 "Red",
                 "Red Edge",
                 "Red Edge"
                 
  ),
  Center_Wavelength = c("475",
                        "560",
                        "668",
                        "717",
                        "842",
                        "634",
                        "444",
                        "531",
                        "650",
                        "705",
                        "740"
  ),
  Bandwidth = c("32",
                "27",
                "14",
                "12",
                "57",
                "463",
                "28",
                "14",
                "16",
                "10",
                "18"
  ),
  Camera = c("Red",
             "Red",
             "Red",
             "Red",
             "Red",
             "Red",
             "Blue",
             "Blue",
             "Blue",
             "Blue",
             "Blue"
  )
  
  
)
# Create the table and center it
kable(data, col.names = c("Band Name", "Center Wavelength (nm)","Bandwidth (nm)","Camera"),caption = 'Band names, center wavelengths, and bandwidths collected by the Red and Blue cameras within the Micasense Dual camera systems. Panchro* is only available in the pan chromatic models.')

```

-   **Image Capture Rate:**

    -   The default setting for timed intervals is one image capture every two seconds, with simultaneous capture for all bands.

    -   The capture rate can be adjusted to a maximum of one image per second

-   **Output:** 5-6 folders per camera (10-12 thousand images) for a 25–30-minute flight covering a 1.5-2 Ha site

-   **GSD:** 2.5-4cm at 40m above canopy. 1.5-2cm when processed pansharpened with the panchromatic band.

**Flight Parameters:**

-   **Flight Elevation:**

    -   40 m above canopy

-   **Flight Speed:** \~2 m/s (slower speeds reduce motion blur, especially in windy conditions)

-   **Image Overlap:** Extra high to ensure sufficient coverage and allow for exclusion of poor-quality images during processing.

    -   Front: \~86% (speed and timed interval is the limiting factor)

    -   Side: \~86%

-   **Margins:** 10 m around the site perimeter is sufficient for flight planning due to the wide-angle shot of the MS camera

-   **Flight Time:** Approximately 30 minutes for a 1.5-2 Ha area site. This allows for a single battery flight, which helps to maintain even light conditions.

**Best Practices:**

-   **Weather Conditions:** Avoid flying in any precipitation as MicaSense cameras are unprotected from moisture, with exposed data and power connections.

-   **Lighting:** Conduct flights within two hours of solar noon to minimize shadowing in the imagery.

    -   The DLS calibration helps to reduce the effect changing light conditions, but extreme changes in light conditions will leave residual effects that are difficult to process out. Figure \@ref(fig:drone-variable-light) below shows a flight in variable conditions (left) in which the calibration has failed to correct the changing light conditions. The flight on the right was flown on an ideal day with even diffuse light.

Figure \@ref(fig:drone-variable-light)

```{r drone-variable-light, echo=FALSE, fig.show="hold",fig.align='center', out.width="50%", fig.cap = "Left: MS orthomosaic in variable light. Right: MS orthomosaic in even diffuse light."}
knitr::include_graphics(c(here("Photos_&_gifs\\variable_light.png"),
                          here("Photos_&_gifs\\diffuse_light.png"))
)
```

-   **Wind Conditions:** Lower wind speeds yield better results; however, the impact of wind will vary depending on species and site-specific factors. Consider flying at a higher altitude and overlap in windier conditions.

-   **Flight Elevation Considerations:** While lower flight elevations produce higher resolution images, they increase flight time, making calibration more challenging.

Click [here](https://github.com/owaite/GenomeBC_BPG/tree/main/SOPs) to access our Micasense field SOP housed on GitHub.

[Best practices: Collecting Data with MicaSense Sensors – MicaSense Knowledge Base](https://support.micasense.com/hc/en-us/articles/224893167-Best-practices-Collecting-Data-with-MicaSense-Sensors) is an important resource.

### Zenmuse P1: High-Quality Natural-Colour (RGB) Imagery

The P1 is RTK/PPK enabled and has the highest resolution and positional accuracy of the sensors available. It is used for georeferencing, crown delineation, and to aid in the registration of data from sensors with less spatial accuracy.

**Sensor Specifications:**

-   **Sensor Type:** Natural-colour (RGB)

-   **Resolution:** 45MP

-   **Pixel Size:** 1cm per 80m altitude, 7-9 mm at the paramaters detailed below.

-   **Positional Accuracy:** 3-5cm without GCPs when using RTK/PPK.

-   **Waterproof:** No

**Flight Parameters:**

-   **Flight Elevation:** 65m above ground or canopy.

-   **Flight Speed:** \~4m/s.

-   **Overlap:** 85% front and side.

-   **Margin:** 20 m to reduce edge error and ensure higher image quality.

-   **Flight Time:** Approximately 15 minutes for a 2-Hectare site.

**Best Practices:**

-   **Timing:** Conduct template flights before spring flush or "greenup" in order to easily detect experimental trees from ingress and brush.

-   **Wind Conditions:** Ideal on days with little or no wind; \<10 km/h for coastal species (e.g., western redcedar, Douglas-fir); up to 20 km/h may be suitable for other species (e.g., juvenile interior spruce).

-   **Lighting:** Best results are achieved on overcast days due to diffuse lighting conditions which reduces shadows and enhances detail. We have found that of full sun days using the “Auto” settings will produce oversaturated imagery. The [sunny 16 rule](https://en.wikipedia.org/wiki/Sunny_16_rule) of photography does help with the P1 on full sun days.

Figure \@ref(fig:P1-RGB)

```{r P1-RGB, echo=FALSE, fig.show="hold",fig.align='center', out.width="50%", fig.cap = "These images are the same area, the left was taken with the above parameters with the P1 before greenup. The right was taken with the wide angle RGB on the Zenmuse H20T, a lower resolution camera, mid-August."}
knitr::include_graphics(c(here("Photos_&_gifs\\P1_pre_greenup.png"),
                        here("Photos_&_gifs\\RGB_mid_Aug.jpg")
                        ))
```

### Zenmuse L1: LiDAR and RGB

```{r RC-L1, echo=FALSE, out.width="100%", fig.cap = "L1 LiDAR screenshot from the remote controller during a flight at Big Tree Creek site on July 7, 2024. On the left is the RGB image instantaneously acquired for the frame and the right shows the LIDAR data acquisition from the scanner as it acquired the data up the frame."}
knitr::include_graphics(here("Photos_&_gifs\\RC_L1.png"))
```

LiDAR (Light Detection and Ranging): LiDAR is an active sensor that emits laser pulses at specific wavelengths and measures the time and intensity of the returns to calculate distances. This allows it to create precise 3D models of the terrain or objects. LiDAR relies on a highly accurate Inertial Measurement Unit (IMU) and GNSS to calculate positional data, including roll, pitch, yaw (orientation), and X, Y, Z coordinates (spatial position), enabling accurate georeferencing of the collected data.

The L1 LiDAR is used to build DTMs for terrain following when one is not available and is flown again with terrain following to calculate tree metrics.

**Sensor Specifications:**

-   **Echo Mode:** Triple returns for penetrating vegetation and tree canopies. Ideally this will record three distinct returns per pulse: First Return- Typically represents the top of the canopy. Second Return- Represents mid-canopy layers or understory vegetation. Third Return- Represents the ground

-   **RGB Imagery:** The L1 also includes a 20 MP RGB camera, which can be used as a lower resolution substitute for a P1 camera.

-   **System Accuracy:** 10cm Horizontal, 5cm Vertical (50m AGL)

-   **Moisture Resistance:** Rated for rainy or foggy conditions, but moisture in the air can result in noise and overall degrade point cloud quality; not recommended to fly in such conditions.

**Flight Parameters:** These settings have been effective for coastal open and closed canopy sites.

-   **Repetitive Scan Mode:** Recommended for detailed terrain and vegetation profiling. This is being further tested.

-   **Inflight Automatic Calibration:** Highly recommended for accuracy; the IMU recalibrates every 100 seconds during the flight.

-   **Overlap:** 85% front and side (85x85) for RGB imagery.

-   **Flight Elevation:** 60-85 m, depending on site conditions.

-   **Flight Speed:** Approximately 4 m/s.

-   **Point Density:** Greater than 1500 points per square meter.

-   **Elevation Optimization:** Should be turned OFF to avoid inconsistencies in the point cloud dataset.

-   **Timing of Flight:** When possible, it is best to collect LiDAR before greenup in the spring. LiDAR is typically taken before or after multispectral flights as it is not constricted to the -/+ 2 hours of solar noon window.

**Zenmuse L2 LiDAR:** The recently released L2 LiDAR sensor offers improved positional accuracy due to a smaller beam footprint and more precise IMU. It also supports up to five returns per pulse, enhancing the within canopy detail of the point cloud and potentially defining a more accurate DTM for areas with understory.

### Zenmuse H20T: Thermal and RGB

Thermal Imaging (H20T DJI Thermal Camera): The H20T is a multi-sensor camera with thermal imaging, zoom, and wide-angle RGB (visible light) cameras in one sensor, allowing it to capture detailed visual and thermal data. The thermal sensor detects infrared radiation and converts it into temperature data, which can then be used to analyze heat signatures of objects, terrain, or structures. The H20T also integrates a laser rangefinder for accurate distance measurements, which can be useful for checking and confirming terrain following flight mission altitudes.

**Note:** The H20T was not designed for thermal mapping and the raw thermal imagery needs to be converted to single band temperature tiffs. Our conversion method is detailed in the [Thermal Conversion](#Thermal-Conversion) chapter.

GCP's are very helpful with thermal imagery.  It is dificult to confirm that the crown polygons are positionally accurate without them, especially in closed canopy sites.

**Sensor Specifications:**

- **GSD:** Thermal \~6cm at 70m, RGB wide \~2cm at 70m.

- **Zoom:** The H20T has a 23x hybrid optical zoom with digital zoom capabilities for a combined zoom of up to 200x in RGB mode.

- **Thermal Sensitivity:** \<50 mK at f/1.0, providing high sensitivity for detecting slight differences in temperature.

- **Spectral Band:** The thermal sensor operates in the 7.5 - 13.5 µm range, which is ideal for detecting heat sources in various environmental conditions.

- **Thermal Accuracy:** The temperature measurement accuracy is ±2°C or ±2%, whichever is greater.

- **Gain Mode:** The High Gain mode measures temperatures from -40°C to 150°C at a finer scale than the regular gain mode.  It is highly recommended to ensure the high gain mode is set for this application.

**Flight Parameters:** 

- **Optimal Flight Altitude:** The size of our sites required \~70 m altitude. Flying as low as possible based on system and terrain restrictions yields more detailed thermal data.

- **Flight Speed:** Recommended at 1.5-4m/s depending on site size, aim for slower speeds if possible to maximize thermal data quality.

- **Overlap:** Thermal: \>85% front and side overlap (85x85) to ensure comprehensive coverage

- **Environmental Conditions:** Best in low wind clear conditions.

- **Timing of Flight:** Best results from early afternoon.  We found that imagery taken later in the day can be overwhelmed by radiant heat and ealier has heavy “shadows” from overnight temperatures.

<!--chapter:end:0_Data_Collection_Flights.Rmd-->

# Agisoft Photogrammetry pipeline {#processing-orthomosaics}

**Processing P1 data and registering multispectral to it**

Software: Agisoft LLC. (2024). _Agisoft Metashape Professional, Version 2.1.1_. 

We use Agisoft Metashape Pro to process aerial photogrammetry into orthomosaics. The following is a complete workflow for processing a P1 RGB that is geometrically accurate, registering the less accurate multispectral imagery to it, and producing calibrated ten band orthomosaics for further processing into vegetative indices.  We process all DAP flights from the same site on the same day in one Agisoft project. Each flight is referred to as a “chunk” by Agisoft. In this case we are processing both a MS and MS_P chunk in order to test and provide examples, but normally only one would be used.  The settings used will vastly affect processing times and quality and each setting has been carefully considered.

Agisoft processes can be started using GUI tools or scripted using the python console.  We don’t use a full scripted workflow as there are several manual steps outlined below, but we do use snippets of code for some steps.

The workflow is based on a site approximately 1.5 Ha buffered to 2Ha and the raw data would be approximately:

-	P1 (RGB): 500 photos and 12 GB of data
-	MS_P (10 band plus panchro): 12,000 photos equaling 42 GB
-	MS (10 band): 10,500 photos equaling 25GB 

## Overview.

1.	Import all photos for each chunk and locate the calibration panel captures for the multispectral chunks. Then save to the project file structure.
2.	Manually assess all flight lines and disable outliers.
3.	Build the P1 RGB through to orthomosaic using the settings detailed below.
4.	Confirm the P1 is aligned to the GCPs and template P1 if this is not the primary flight.
5.	For each of the other DAP sensors, specifically MS and MS_P chunks, build through to orthomosaic following the steps outlined below paying particular attention to the chunk align.
6.	Check the ortho registration and reflectance values.
7.	Run the closing script for each multispectral sensor after entering the variables into the script.  This script will:
    -	Normalize the 16bit pixel values.  
    -	Import the 15m buffered shape of the trial and export the orthomosaic clipped to this shape in NAD83 to the project specific folder. 

Approximate processing times:

  -	 ~12 hours for the P1 RGB.
  -	 ~18-38 hours for each of the Micasense Dual sensors, with the MS_P on the higher end. 
  
This process produces an Agisoft folder with the P1 and MS chunks containing ~80GB of files. 
The exported orthomosaic tifs contain 10 (MS) or 11 (MS_P) bands and are ready for analysis.


```{r aligned-agisoft, echo=FALSE, out.width="100%",fig.align='center', fig.cap = "A dense point cloud processed in Agisoft Metashape with the camera locations turned on and disabled flight trajectories shown."}
knitr::include_graphics(here("Photos_&_gifs\\Aligned_agisoft.png"))
```

**Coordinate Reference Systems:**

- 	DJI drones and GNSS units work in the geodetic coordinate system (GCS) WGS84,  EPSG:4326
- 	We chose to work in WGS84 during photogrammetry processing until exporting orthomosaics.  We export orthomosaics for further processing into the coordinate reference system NAD83/UTM.  

**Agisoft Preferences Setup:**
Prior to commencing processing, we recommend the following settings in Tools-Preferences.

- GPU tab: Uncheck the Use CPU when performing GPU accelerated processing.  This will free up your CPU during the heavy processing steps.
- Advanced tab: The settings in Figure \@ref(fig:metashape-pref) are recommended. 

```{r metashape-pref, echo=FALSE, fig.show="hold",fig.align='center', out.width="50%", fig.cap = "Recommended Metashape preferences setup in Tools, Metashape Preferences, under the Advanced tab."}
# Include the pre-brushing and post-brushing images
knitr::include_graphics(c(
  here("Photos_&_gifs/metashape_pref.png"),
  here("Photos_&_gifs/metashape_pref_2.png")
))
```

## Detailed Workflow

1. **Load Photos and locate calibration panels** - Script (or GUI). 15 minutes.

  - Agisoft processes all ten bands as a multicamera with one primary channel. We import imagery using a python script located [here](https://github.com/owaite/GenomeBC_BPG/blob/main/Scripts/TheOpener_BPG.py).  However, you can also import folders containing imagery in metashape via the import tool or by dragging and dropping the folder into the specified chunk. 

  - The script will import all photos for each chunk, then locate the calibration panel captures for the multispectral and move them to a “Calibration images” folder in the Agisoft Workspace tab. If you chose to manually import imagery, you can locate calibration panels using the Calibrate Reflectance tool in the Tools GUI as seen below in Figure \@ref(fig:cal-ref-agisoft).


```{r cal-ref-agisoft, echo=FALSE, out.width="100%", fig.align='center', fig.cap = "Calibrate reflectance tool found under Tools in Metashape."}
knitr::include_graphics(here("Photos_&_gifs\\cal_ref_metashape.png"))
```

**Note:** To leverage the higher resolution imagery available with the MS_P sensor you are required to set the panchromatic camera as the primary channel for processing.  Go to Tools – Set Primary Channel – select Panchro.

2. **Micasense Corrections outside of Agisoft(if necessary)** – R Script. 2 hours.

  - We have encountered incorrect irradiance data in metadata of MicaSense imagery due to incorrect measurements by the DLS2. To correct the irradiance data prior to processing we use methods from MicaSense’s DLS1 system. See the chapter on [MicaSense Irradiance Correction](#MicaSense-Irradiance-Correction) for checks on whether your data needs correction and a detailed explanation of the correction process.

3.	**Visually Assess Flight Trajectory and Disable Outliers** – Manual. 10 minutes.

  - The MS cameras start taking pictures as soon as image capture has been started via the webpage on the ground and don’t stop until we power off at the end of the flight. Hence, we examine the flight trajectory and disable photos taken during flight to and from the site.

```{r trajectory, echo=FALSE, out.width="100%",fig.align='center', fig.cap = "Flight trajectory with light blue disabled and dark blue as enabled cameras prior to photo alignment."}
knitr::include_graphics(here("Photos_&_gifs\\trajectory.png"))
```

4.	**Select the Best Calibration Panels and Calibrate Reflectance** – Manual and Processed. 15 minutes.

  - Each panel has a unique set of calibration values provided by the manufacturer in a csv.  The Select Panel button in the Calibrate Reflectance tool will direct you to import the csv. 
  - Agisoft Metashape takes the latest before the flight and the earliest after the flight panel captures and linearly interpolates between them.
  - We select one panel capture from the start and one from the end of the flight, by visually selecting a centered capture without shadow or glare and moving the rest to an “Unused_Cal_Images” folder (this folder needs to be created in the GUI). 
  - We run the calibration process using both the DLS and Calibration Panels checked in . However, Micasense recommends only calibrating with panels on full sun days. Please refer to the [Micasense Knowledge Base - Best practices: Collecting Data with MicaSense Sensors](https://support.micasense.com/hc/en-us/articles/224893167-Best-practices-Collecting-Data-with-MicaSense-Sensors#:~:text=Hold%20the%20aircraft%20at%20chest,offset%20slightly%20to%20prevent%20shadows.) for more details.

```{r cal-ref-sunPanel-agisoft, echo=FALSE, out.width="100%",fig.align='center', fig.cap = "Calibrate reflectance tool found under Tools in Metashape with sun sensor and reflectance panels enabled for calibration."}
knitr::include_graphics(here("Photos_&_gifs\\cal_ref_sun_panel_metashape.png"))
```

5.	**Align Photos** – GUI - Processed. 3-10 hours.

  - Photogrammetric alignment is the process of matching and orienting overlapping images by identifying common features known as tie points, across multiple images. This alignment generates a geometrically accurate sparse point cloud from the tie points.
  - We use the batch processing tool and settings as seen below.
  - The sparse clouds vary greatly in density, the P1 clouds on a closed canopy can be as low as 150,000 points.  The MS_P sparse clouds could be 48 million points on the same flight plan.  


```{r align-photo-agisoft, echo=FALSE, out.width="40%", fig.align='center', fig.cap = "Photogrametric camera alignment settings using the alignment tool in Metashape"}
knitr::include_graphics(here("Photos_&_gifs\\align_photos_metashape.png"))
```

6.	**Import and Align GCPs** – Manual. 15 minutes.

  - The GCP coordinates are imported in WGS 84.
  -	**Note** GCPs can be used either actively as control points in which case they will “pull” the images into alignment or passively as check points to measure error.
  - We are only using GCPs as check points now, but they will be implemented if registration fails.
  -Instructions to align GCPs if necessary
    -	In the Reference pane check GCP_1 right click on it and filter photos by marker
    - Open the photos tab, and double click the first photo.  Drag the marker to the center of the photo.  Do this for several photos - then click on the update transform tool found in the Reference pane.
    - Scroll through at least 3 more pics to make sure they are aligned.
    - Repeat for each GCPs hitting update transform tool located in the reference panel once you have set the photos.
    - Leave at least one GCP for a “checkpoint”.

7.	 **Sparse Cloud - Filter and Optimize Camera Alignment** - manual. 30-45 minutes.

  - This iterative step filters the sparse cloud for outliers and optimizes the camera calibration.  Most literature suggests more aggressive filtering, but on closed canopy sites this tends to cause loss.  Visually assess the cloud at each stage to confirm the sparse cloud is not becoming too sparse.
  - In the GUI select Model – Gradual Selection – Reconstruction Uncertainty: Adjust the slider until the value is 25-75, while selecting a Max of %10 of the points.
  - In Figure \@ref(fig:sparseCloud-agisoft), there are 400,000 points selected out of 40 million and the uncertainty is at 30.
  - Hit okay and delete to remove points.
```{r sparseCloud-agisoft, echo=FALSE, out.width="100%",fig.align='center', fig.cap = "Sparse cloud created in image alignment. Pink points are those selected by the Gradual Selection tool that have a reconstruction uncertainty greater than or equal to 30.4608. These points will be filtered out."}
knitr::include_graphics(here("Photos_&_gifs\\sparse_cloud_metashape.png"))
```

  - Next we optimize the camera alignment. Tools - Optimize Camera Alignment: default settings as in Figure \@ref(fig:opt-align-agisoft).
     
```{r opt-align-agisoft, echo=FALSE, out.width="60%",fig.align='center', fig.cap = "Settings used for the Optimize Camera Alignment tool"}
knitr::include_graphics(here("Photos_&_gifs\\opt_camera_align_metashape.png"))
```
  
  - We now go through the same steps again filtering points and optimizing the cameras for the following settings.
  - Model – Gradual Selection – Projection Accuracy: 5-20. Max%10
  - Optimize Camera Alignment at stock - check estimate tiepoint covariance
  - Model – Gradual Selection – Reprojection Error: 0.3-1. Max%10
  - Optimize Camera Alignment at stock with fit additional corrections and estimate tiepoint covariance.
  - Finally using the select tool we manually clip any edge points not cohesive to the flight.

8.	**Point based chunk alignment:** multispectral to P1 - (requires manual verification). 2-6 hours. Batch processing

  - Leveraging the accuracy of the P1 as a reference, the MS sparse cloud is aligned to the higher accuracy P1 sparse cloud using the batch processing steps according to the settings in Figure \@ref(fig:chunk-align-agisoft).
```{r chunk-align-agisoft, echo=FALSE, out.width="60%", fig.align='center',fig.cap = "Settings used for point based chunk alignment when aligning the multispectral data to the P1."}
knitr::include_graphics(here("Photos_&_gifs\\align_chunks_metashape.png"))
```

  - Visually confirm each multispectral has in fact aligned by seeing the [T] for transposed as in Figure \@ref(fig:chunk-align-agisoft).  If the chunk align on medium setting fails, try again on low.  If this also fails, you will need to align with GCP's or attempt georeferencing the output orthomosaics in a GIS.

```{r chunk-align-confirm-agisoft, echo=FALSE, out.width="60%",fig.align='center', fig.cap = "Visual of the [T] that will appear next to chunks that have successfully aligned via chunk alignment."}
knitr::include_graphics(here("Photos_&_gifs\\transposed_metashape.png"))
```

9.	**Build Depth Maps and Dense Cloud** – Script or Batch Process. 5-24 hours.

  - A dense point cloud, a high-resolution 3D representation with significantly more detail than the sparse cloud is built in this step.  First depth maps are created for each image, capturing detailed information about the distance to surfaces in the scene. These depth maps are then combined to generate the dense point cloud. This is a primary product and can be exported.  We have been building these to the highest settings as outlined in Figure \@ref(fig:DC-settings-agisoft), but if this is not required significant time can be saved by lowering the setting.  An example dense cloud is seen in Figure \@ref(fig:aligned-agisoft)
     
```{r DC-settings-agisoft, echo=FALSE, out.width="60%",fig.align='center', fig.cap = "Metashape settings for building the depth maps and the dense cloud."}
knitr::include_graphics(here("Photos_&_gifs\\DC_settings_metashape.png"))
```

10.	**Build DEM** – Script or Batch Processed. ~1 hour.

  - Generates a Digital Elevation Model from the dense cloud data. Batch settings in Figure \@ref(fig:DEM-settings-agisoft)

```{r DEM-settings-agisoft, echo=FALSE, out.width="60%",fig.align='center', fig.cap = "Metashape settings for DEM generation."}
knitr::include_graphics(here("Photos_&_gifs\\build_DEM_metashape.png"))
```

11.	**Build Orthomosaic** – Script or Batch Processed. 1-4 hours.

  - Creates an orthomosaic by stitching together the aligned images and the DEM according to batch processing steps in Figure \@ref(fig:ortho-settings-agisoft).
  - **NOTE: We do NOT enable hole filling for multispectral** – only used for RGB orthomosaics that were needed strictly for visual purposes and not analysis.

```{r ortho-settings-agisoft, echo=FALSE, out.width="60%", fig.align='center', fig.cap = "Metashape settings for building the orthomosaic."}
knitr::include_graphics(here("Photos_&_gifs\\build_ortho_metashape.png"))
```

12.	**Check Orthomosaic Registration* - Manual. 15 minutes to redo from GCP alignment.

  - The orthomosaic is checked for alignment to the GCPs. If it is not close on each GCP, we go back and try alignment with GCPs. It is very important to check all corners, as occasionally three out of four will align beautifully, and the fourth will be 25-50cm out of alignment.
  - In the cases where we were unable to establish GCPs the orthomosaic is exported and checked against the P1 in a GIS. 
    
13.	**Sanity Check Reflectance Values** – Manual. 15 minutes – 2 hours if remaking orthomosaics.

  - The orthomosaic is sanity checked for reflectance values. We look at the histogram of three different bands to ensure the values make sense. If they do not, another calibration panel is chosen, the reflectance is recalibrated, and the orthomosaic is rebuilt.
  - To check values - go into Raster Transform - Enable Transform
  - Select B1 (blue band). Click the circular arrows to the left of the “interpolate colors” button, then click auto and then apply. B1 should be around ~0.02 for the trees. The range it gives will be for the entire ortho so roads and other vegetation will increase the range so I suggest zooming into the ortho so you can see approximately what values correspond to trees. Repeat for B4 (green) and B10 (NIR). If the values are too large or small, i.e. B10 (NIR) > 1, choose another calibration panel, delete the ortho, recalibrate, remake the ortho and recheck the values. 
  -**Note** these values are general estimates of what a “tree” should look like however, different vegetation will have different ranges, for example values for cedar may be a little lower compared to Douglas fir.
  
**General benchmarks:**

  - B1 should be around ~0.02.
```{r b1-check-ref-agisoft, echo=FALSE, fig.align='center',out.width="100%", fig.cap = "Raster calculator tool in Metashape to look at reflectance values of trees for band 1 (Blue - 444nm) prior to exporting the orthomosaic."}
knitr::include_graphics(here("Photos_&_gifs\\reflectance_values_metashape.png"))

```

  - B4 should be around ~0.06
```{r b4-check-ref-agisoft, echo=FALSE, fig.align='center',out.width="100%", fig.cap = "Reflectance values of trees for band 4 (Green - 560nm)."}
knitr::include_graphics(here("Photos_&_gifs\\b4_reflectance_values_metashape.png"))

```

  - B10 should be around ~0.3 – 0.5, but shouldn’t exceed 0.7
```{r b10-check-ref-agisoft, echo=FALSE, fig.align='center', out.width="100%", fig.cap = "Reflectance values of trees for band 10 (NIR - 842nm)."}
knitr::include_graphics(here("Photos_&_gifs\\b10_reflectance_values_metashape.png"))

```

14.	Normalize and export – Script 5 minutes.

  - Run the closing script for each multispectral sensor after entering the variables into the scripts.  [MS](https://github.com/owaite/GenomeBC_BPG/blob/main/Scripts/BPG_closing_MS.py), [MS_P](https://github.com/owaite/GenomeBC_BPG/blob/main/Scripts/BPG_closing_MS_.py) these scripts will:
    - Normalize the 16bit pixel values for the MS and MS_P. **Note** We do not recommend pan sharpening, but if this is part of your workflow you would do it now as per the micasense knowledge base [here](https://support.micasense.com/hc/en-us/articles/4416696717847-Pan-sharpening-processing-data-from-Altum-PT-and-RedEdge-P-cameras-in-Agisoft-Metashape).
    - Import the 15m buffered shape of the trial and export the orthomosaic clipped to this shape in NAD83 to the project specific folder. **Note** Importing and clipping the orthomosaic will reduce the size of the export and help speed up further processing.
      
The following are helpful Agisoft processing references:

  - [Agisoft Metashape User Manual: Professional Edition, Version 2.1.](https://www.agisoft.com/downloads/user-manuals/)
  
  - [USGS_ Processing Coastal Imagery With Agisoft Metashape Professional Edition](https://pubs.usgs.gov/publication/ofr20211039)
  
  - [Micasense - Process MicaSense sensor data in Agisoft Metashape](https://support.micasense.com/hc/en-us/articles/360002693373-Process-MicaSense-sensor-data-in-Agisoft-Metashape)
  
  - [Processing Multi-spectral Imagery with Agisoft MetaShape Pro](https://pressbooks.bccampus.ca/ericsaczuk/)

The knowledge base at Agisoft also has many other useful articles.

<!--chapter:end:0_Photogrametric_processing.Rmd-->

# Georeferencing Plot Trees {#Georeferencing-Plot-Trees}

At this stage in the workflow we have processed high quality imagery into orthomosaics ready for analysis. The next step is to georeference the row and column census data and the high quality P1 template to geolocate the trees, which will be used later on to create individual tree polygons. Figure \@ref(fig:genetic-layout) is an image of the spreadsheet map from the Big Tree creek site which is typical of many BC genetics trials. Figure \@ref(fig:P1-genetic-layout) is the P1 orthomosaic template flight we georeference the census data to.

```{r genetic-layout, echo=FALSE, fig.align='center', out.width="100%", fig.cap = "The spreadsheet map of the layout of the site, each tree is located on a grid with a unique row and column value assigned to it."}
knitr::include_graphics(here("Photos_&_gifs\\genetic_layout2.png"))

```

```{r P1-genetic-layout, echo=FALSE, fig.align='center', out.width="100%", fig.cap = "P1 orthomosaic of the site with GCPs and georeferenced trees marked with red dots"}
knitr::include_graphics(here("Photos_&_gifs\\genetic_layout_P1.png"))

```

We have recently developed a “Grid Georeferencer” plugin in QGIS that does the initial rough georeferencing of the census data. We also have a version that works in R. Both are detailed below.

## Workflow in QGIS

1.  The census data is examined, and a unique identifier column is found or created.

    a.  This column is “treeID” and will be used through the rest of the data processing pipeline.

    b.  The census data must also contain “row” and “col” (row and column identifiers).

2.  The information from the reconnaissance workflow is used to locate reference trees.

    a.  The distance, Azimuth and treeID from each GCP to the nearest plot tree was recorded as a reference tree.

    b.  In a GIS (in this case QGIS) a shapefile is made with the reference trees. Here they are visualized as red dots. The shapefile is saved in the “working” reference system NAD83 UTM’s.

3.  The “Grid Georeferencer” plugin is run in QGIS as seen in Figure \@ref(fig:Qgis-ref).

    a.  Enter the Input csv, choose the extra fields to import, enter the grid spacing, enter the export location.

```{r Qgis-ref, echo=FALSE, out.width="80%",fig.align='center', fig.cap = "Visual of the Grid Georeferenced plugin in QGIS."}
knitr::include_graphics(here("Photos_&_gifs\\Qgis_grid_georef.png"))
                         
```

4.  The plugin builds the grid based on the row and col fields at the spacing input.

    4a. Then rotates the grid based on the input reference trees and saves a copy of this rotated version as \_rotated in the export folder.

    4b. Then applies an affine transformation to better fit the grid to the reference trees. It saves this as \_transformed and loads it into QGIS.

## Alternate Workflow in R

1.  The census data is examined, and a unique identifier column is found or created.

2.  This column is “treeID” and will be used through the rest of the data processing pipeline.

3.  The information from the reconnaissance workflow is used to locate corner trees.

    a.  The distance, Azimuth and treeID from each GCP to the nearest plot tree was recorded as a reference tree.
    b.  In a GIS (in this case QGIS) a shapefile is made with the reference trees. Here they are visualized as red dots.
    c.  We want to identify the tree that is in the first row and first column (first-first), which here is the bottom left corner, and the first row last column (first-last), which is the bottom right and coincidentally a reference tree.
    d.  Ideally, we would have a GCP at the bottom left corner of the site, but this was the densest section, so the columns and rows were carefully counted in either direction from the nearest reference tree.
    e.  We made shapefiles for the first-first and first-last trees.

4.  The “Treetop_georeference.R” script is opened in R and site-specific variables entered including spacing, which on this site is 2m X 2m. The code then goes through the following steps. \*\*Note this requires North up rows and East up columns to work as scripted. Modifications will need to be made to sites with different orientations. This code will install the required libraries, set up the file structure as we use it and folow these steps:

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
library(tidyverse)
library(sf)
library(spatial)
library(rgdal)
library(raster)
library(lidR)
library(sp)
library(nngeo)
library(future)

dir = "Q:\\SNC\\Data\\Big_Tree_GCA\\"
name <- "Big_Tree"
csv_path = paste0(dir, "Census_Data\\#4_bigtree_gca_masterfile.csv")

trees <- paste0("Site_Info\\03_GIS\\shapefiles")

#shapfiles for the below trees were done in a GIS off high res imagery
first_first = st_read(paste0(dir, trees, "\\first_first.shp")) %>% 
  st_geometry()

first_last = st_read(paste0(dir, trees, "\\first_last.shp")) %>% 
  st_geometry()

# column spacing in m 
c_space = 2

# row spacing in m 
r_space = 2

# read in the CSV
csv_B = read.csv(csv_path) %>% 
  dplyr::select(treeID, row, col, ht12, c12)
  
csv_B
# produce a grid using row and column numbers, to be rotated
grid = csv_B %>% 
  mutate(
    x_coord = st_coordinates(first_first)[1] + (col * c_space) - c_space,
    y_coord = st_coordinates(first_first)[2] + (row * r_space) - c_space)

# to sf object
grid_sf = st_as_sf(grid, coords = c("x_coord", "y_coord"), crs = 26910)

# get the coordinates of the first-last
first_last_sf = grid_sf %>% 
  filter(row == 1) %>%
  filter(col == max(.$col)) %>% 
  st_geometry()

# define a FUNCTION which rotates coords for a given angle
rot = function(a) matrix(c(cos(a), sin(a), -sin(a), cos(a)), 2, 2)

# get the angle of rotation between the true (first, last) and
# the current position of (first, last)
a = st_coordinates(first_last - first_first)
b = st_coordinates(first_last_sf - first_first)

theta = acos( sum(a*b) / ( sqrt(sum(a * a)) * sqrt(sum(b * b)) ) )

# rotate all points 
grid_rot = (st_geometry(grid_sf) - first_first) * rot((theta)) + first_first

# in order to merge the new coordinates and the census info,
# copy the sf object created from the master csv
grid_rot_sf = grid_sf
# then replace its geometry with the rotated coords
st_geometry(grid_rot_sf) = grid_rot
# and set its CRS to the correct one
st_crs(grid_rot_sf) = 26910

#save rotated grid
st_write(obj = grid_rot_sf,
         dsn = paste0(dir, trees, "\\", name, "_grid_manual_bpg.shp"),
         append = FALSE,
         crs = 26910)

plot(grid_rot_sf)
```
</details>
         
a.  Read Shapefiles: Read the shapefiles for the first-first and first-last trees.
b.  Read and Filter Data: Load the CSV file, filter for the required columns. At Big Tree we want the row, col, treeID and the most recent assessments, which here are a 2012 remeasurement(ht12) and assessment(c12).
c.  Generate Initial Grid: Create a grid of tree locations using the row and column numbers, starting from the initial points (first-first). Adjust the coordinates to account for column and row spacing.
d.  Convert to Spatial Object: Convert the grid data to an sf (simple features) object in R.
e.  Identify Reference Point: Extract the coordinates of the tree in the first column and the last row (first_last_sf) from the grid.
f.  Calculate Rotation Angle: Define a function to rotate coordinates. Calculate the angle of rotation needed to align the grid with the actual positions of the initial points.
g.  Rotate Grid: Apply the rotation to all points in the grid.
h.  Merge and Save Results: Merge the rotated coordinates with the original grid data. Set the CRS and save the resulting grid as a shapefile.
i.  Plot Final Grid: Plot the final rotated grid to visualize the tree locations.

## Final Steps in QGIS

The grid is loaded into the GIS and examined. The first-first corner with row 1 column 1 looks great, but the top right corner is at least 2 meters off as you can see below. This is one of the better ones we have georeferenced. The site is on even ground and was quite square. Some of the sites were 6 meters off from one corner to the other.

```{r ref-trees, echo=FALSE,  fig.show="hold",fig.align='center', out.width="50%", fig.cap = "Left: treeID 1, reference for the bottom left corner of the plot. Right: treeID 2757, reference for the top right of the plot"}
knitr::include_graphics(c(here("Photos_&_gifs\\ref_treeID_1.png"),
                          here("Photos_&_gifs\\ref_treeID_2757.png")))
```

We now use the reference trees and QGIS’ Georeferencer plugin if needed to tighten up the fit of the grid to the Orthomosaic. Open the Georeferencer and select the Transformation Settings icon as seen to the left. The polynomial 2 transformation works well, but another transformation can be tried if it doesn’t fit. Once sufficient, add the vector layer that we exported from R and zoom in on the first point that matches a reference tree as below. Then add a point and select the From Map Canvas button.

```{r Qgis-ref-trees, echo=FALSE, fig.show="hold", fig.align='center', out.width=c("32.5%","67.5%"), fig.cap = "Left: Transformation settings. Right: zoom in on vector layer and place a point on the Map Canvas"}
knitr::include_graphics(c(here("Photos_&_gifs\\Qgis_transformation_settings.png")))

knitr::include_graphics(c(here("Photos_&_gifs\\Qgis_map_cor_setting.png")))
```

Work through the reference trees. With 6 reference trees spread evenly through the site the fit was quite good as seen here.

```{r Qgis-ref-tree-fit, echo=FALSE, out.width="100%",fig.align='center', fig.cap = "Zoomed in visual of the georeferenced tree grid for 6 trees overlain on the P1 orthomosaic."}
knitr::include_graphics(here("Photos_&_gifs\\Qgis-ref-tree-fit.jpg"))
                         
```

From here we will manually move each tree position to its visible top. This is a time-consuming task, as care must be taken. In the above imagery the tree between 1601 and 1680 is ingress (a Western hemlock, not planted as part of the trial), the P1 imagery helps to make the distinction visible. On sites that have been recently maintained as pictured below, and if you already have a CHM, a time saving method is to buffer 30-40cm circles around your tree points, then snap the tree point to the local maximum.  This can be done in R or a GIS.

```{r Qgis-maintained-ref-tree-fit, echo=FALSE, out.width="100%",fig.align='center', fig.cap = "Georeferenced grid overlain on P1 orthomosaic for a site that was well maintained. The georeferenced grid has been snapped to the highest point on the CHM within a given radius to pull the grid to the treetops."}
knitr::include_graphics(here("Photos_&_gifs\\maintained-geo-ref-trees.png"))
                         
```

<!--chapter:end:0_Tree_Georeferencing.Rmd-->

# Crown Delineation {#crown-delineation}

[Supercells](https://jakubnowosad.com/supercells/) work to spatially group pixels with similar characteristics making it a great tool for crown delineation. The follow workflow shows how to create and filter supercells into delineated crowns and requires:

1)  A shapefile (.shp) with a grid of tree locations derived by georeferencing the census data to the imagery. This will be referred to as the “grid”. Detailed steps for creating the grid can be found in the chapter on [georeferencing plot trees](#Georeferencing-Plot-Trees).

2)  Co-registered CHM made from normalized maximum elevation (Z) values, MicaSense orthomosaic, and P1 orthomosaic, ideally from around the same date.

This workflow goes over:

-   Loading in manually edited treetops (i.e. the "grid") and creating circles proportional to a defined % of the tree's height.

-   Creating a supercell segmentation raster using the CHM and MicaSense and P1 orthomosaics

-   Creating delineated crowns by filtering the supercells and merging remaining cells

-   Cleaning the delineated crowns

We then highly suggest the crowns be reviewed and edited where necessary in a GIS to ensure the accuracy of future metrics calculated using the crowns.

## Circles Proportional to Tree Height

The following code takes the grid and a registered CHM and creates a circle around each grid point that is proportional to a defined percent of its height. The percent will be dependent on the size and spacing of the plot trees.

In this example we decided to use 10% given the tight spacing and areas of crown closure. We recommend trying a range of percentages and choosing the one that gives proportional circles that contain as much of the visible portion of the crown as possible without also catching neighbouring branches.

Depending on the project, circles proportional to height may serve as adequate tree crowns however, if not, these circles will be used later in this workflow to filter the supercells to create defined crown boundaries.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
unique_id <- "TAG" # change this to the unique ID you have per tree that was used to create the grid
crown_dir <- paste0("path to save crown delineation producst to")# This was a path to a folder called Crowns for us

ttops <- st_read("path to your grid")# read in the treetops as an sp object
ttops<- ttops%>%
  st_centroid()

# Buffering circles around grid points to get heights for each tree:
buffer_radius <- 0.1  # buffer radius in meters
ttops_buffered <- st_buffer(ttops, dist = buffer_radius)# Create a buffer around each centroid with a radius of 10cm (0.1 meters)

#Check geometries, this should be  = 0
sum(!st_is_valid(ttops_buffered))

ttops_buffered <- ttops_buffered[!st_is_empty(ttops_buffered), ]

# Height Quantile from CHM
CHM_L1 <- "path to CHM" #path to CHM
CHM_max_L1 <- rast(CHM_L1) #read in CHM as a raster

extracted_values = ttops_buffered %>% #extract height percentiles
  exact_extract(x = CHM_max_L1,
                y = .,
                fun = "quantile",
                quantiles = c(0.975, 0.99),
                force_df = TRUE,
                append_cols = c(unique_id)) 

ttops_zq99 <- left_join(ttops_buffered, extracted_values, by = c(unique_id)) # Assign the pixel IDs to each polygon

# Saving out shp of buffered points
if(!dir.exists(paste0(crown_dir,"Buffer_radius_ttops_",buffer_radius))){
  dir.create(paste0(crown_dir,"Buffer_radius_ttops_",buffer_radius))
}
st_write(ttops_zq99, paste0(crown_dir,"Buffer_radius_ttops_",buffer_radius,"\\",site,"_ttops_",buffer_radius,"mBuffers_.shp"), append = FALSE)

# Proportional hulls based on 10% of the 99th percentile height value
percent <- 10
(buffer_dist <- ttops_zq99$q99 * (percent/100)/2)

buffer_dist[is.na(buffer_dist)] <- 0 

prop_hulls <- ttops_zq99%>%
  st_as_sf(crs = 26910)%>% #change to match your CRS
  st_buffer(dist = buffer_dist)%>% #value here is the radius
  mutate(buff = buffer_dist)

(prop_hulls$crownArea <- st_area(prop_hulls))

#Below we remove columns with mainly NAs as they will cause errors for the segmentation raster
prop_hulls_forSEG <- prop_hulls %>%
  dplyr::select(c(TAG, row, col, q99, q97, crownArea)) #select columns you would like to keep in your crown shapefile, if the columns contain NA values, remove them for the following segmentation steps and re-join the columns at the end


st_write(prop_hulls_forSEG, paste0(crown_dir,"Buffer_radius_ttops_",buffer_radius,"\\",site,"_proportional_Hulls_Diam",percent,"percent_zq99_forSEG_.shp"), append = FALSE) #Save shapefile of circles proportional to height for each tree
```
</details>

See Figure \@ref(fig:prop-circle-crowns) below for an example of the above output.

```{r prop-circle-crowns, echo=FALSE,fig.align = 'center',out.width="100%", fig.cap= "A section of the P1 orthomosaic with red circles outlining the circles proprotional to 10% of the 99th height percentile for each plot tree and grid points representing the top of the plot trees shown in blue."}
knitr::include_graphics(here("Photos_&_gifs\\prop_cirlce_Crowndelin.PNG"))
```

## Supercell Raster

To generate supercells, we use a CHM, MicaSense orthomosaic, and a P1 orthomosaic. When possible we provide rasters from the same flight date. However, if that is not feasible we select a CHM, MicaSense, and P1 orthomosaic from flight acquisitions flown as closely together as possible. This ensures that the aerial perspectives of the tree crowns remain highly consistent across the three rasters, facilitating more accurate supercell segmentation.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
library(future) 
library(supercells)
library(RStoolbox)

crown_dir_folder <- "directory to save files to" #change to match yours
ms_path <- "path to MicaSense orthomosaic"
p1_path <- "path to P1"
prop_hulls <- prop_hulls_forSEG #created in the step above

Hulls = prop_hulls

bound = Hulls %>% #buffer hulls by 5m to get a bounding box to trim rasters with
  st_buffer(5)

# Reading in CHM, MS, and P1 rasters to make supercells
CHM_max_L1 = rast(CHM_L1) %>%
  crop(bound) %>%
  trim()
ms_ortho = rast(ms_path)[[c(1,6,9)]] %>% 
  resample(CHM_max_L1, method = "near") %>% 
  crop(bound)%>%
  trim
rgb = rast(p1_path)[[1:3]] %>% 
  resample(CHM_max_L1,method = "near") %>% 
  crop(bound) %>%
  trim

# Ensure crs match between rasters prior to stacking
crs(ms_ortho) <- crs(CHM_max_L1) 
crs(rgb)<- crs(CHM_max_L1) 
crs(CHM_max_L1) == crs(ms_ortho)

use = c( # Creating a SpatRaster, crs must match for this step
  CHM_max_L1,
  ms_ortho,
  rgb)

use[use > 60000] = NA # Filtering out noisy data

use2 = terra::scale(use)

writeRaster(use2, paste0(crown_dir_folder,"scaled_for_segments.tif"),
            overwrite = TRUE)

use3 = use2 %>% 
  ifel(CHM_max_L1 < .25 , 
       NA, .)

plan(multisession, workers = 6L)

seg = supercells(use3, step = 6, compactness = 5, iter = 50)

if(!dir.exists(paste0(crown_dir_folder,"HULLS"))){ #creating "HULLS" folder to save the below supercell .shp to
  dir.create(paste0(crown_dir_folder,"HULLS"))
}
st_write(obj = dplyr::select(seg, supercells, geometry), 
         dsn = paste0(crown_dir_folder,"HULLS\\",site,"_SEGS_step6_c5.shp"),
         driver = "ESRI Shapefile",
         append = FALSE)
```
</details>

Figure \@ref(fig:seg-parts) shows the supercells (outlined in green) created by the above R code. These outlined areas indicate groups of pixels that the supercell algorithm has deemed similar between the CHM, MicaSense and P1 orthomosaics.

```{r seg-parts, echo=FALSE,fig.align = 'center',out.width="100%", fig.cap= "Supercells shown in green, with circles proportional to the 99th height percentile in red and grid points representing plot trees in blue"}
knitr::include_graphics(here("Photos_&_gifs\\seg_Crowndelin.PNG"))
```

## Filter & Merge Supercells

Above we found supercells across the site however now we need to filter for supercells that belong to tree crowns only. We do this by using the previously made circles proportional to height where supercells must be within the circle or intersect the perimeter in order to be kept, see Figure \@ref(fig:seg-intersection) for a visual. In our case, given that the trees are mature and tightly spaced we are not concerned with capturing lower branches in our crowns as they are not visible due to occlusion and would result in inaccurate spectral and structural metrics. Hence we created circles that were proportional to 10% of the 99th percentile per tree to help refine supercells to areas that were 1) visible from an aerial perspective and 2) had limited overlapping branches with neighbouring trees.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
library(smoothr) #for fill_holes
#Creating a folder to save files to, this was set up so that multiple tests could be run with different "percent" values 
if(!dir.exists(paste0(crown_dir_folder,"Prop_Diameter",percent,"%OfZq99"))){
  dir.create(paste0(crown_dir_folder,"Prop_Diameter",percent,"%OfZq99"))
}

chm_dat = c(CHM_max_L1)
names(chm_dat) = c("CHM_max_L1")

seg_extract = seg %>% 
  exact_extract(x = chm_dat,
                y = .,
                fun = "max",
                force_df = TRUE,
                append_cols = c("supercells"))

class(seg_extract)

seg1 = st_join(seg, prop_hulls_forSEG) %>% #joins supercells that are within or intersect circles to the circle dataset
  drop_na() %>% 
  left_join(seg_extract, by = c("supercells"))

saveRDS(seg1, paste0(crown_dir_folder,"Prop_Diameter",percent,"%OfZq99\\SEG_intersection.rds"))

st_write(obj = seg1, 
         dsn = paste0(crown_dir_folder,"Prop_Diameter",percent,"%OfZq99\\SEG_intersection.shp"),
         driver = "ESRI Shapefile",
         append = FALSE)

```
</details>

```{r seg-intersection, echo=FALSE,fig.align = 'center',out.width="100%", fig.cap= "Supercells (outlined in green) filtered to only retain those that are within or intersect the cirlces propotional to height (red)"}
knitr::include_graphics(here("Photos_&_gifs\\seg_intersection_Crowndelin.PNG"))
```

Below we show an example of further filtering the supercells. First we filter for supercells that overlap areas with a max height greater than a set threshold of 50% of the 99th height percentile using the CHM. This filtering step is mainly if you want to ensure you are only including data from above a certain height percentile in your crowns that was not already removed by the above filtering step using the proportional circles. In this example, no supercells were filtered out from this filter.

We then filter out supercells that are within two proportional circles (a.k.a. in two different crowns). The "supercells" column represents a unique ID for each supercell however in the st_join step above if a supercell intersected more than one proportional circle it will be recorded twice, once for each circle. Hence, we group the supercells by their unique ID (i.e. "supercells" here) and filter out supercells with more than 1 row. For example, in Figure \@ref(fig:seg-initial) supercells highlighted in red are those where "count" = 2 ( so intersects two circles) and those highlighted in purple are cells where "count" = 1, we filter to only keep supercells where "count" = 1.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
seg2 = seg1 %>% 
  mutate(z50_L2 = q99 * .5) %>% #filtering out supercells where the CHM value in that super cell as less than 50% of the 99th percentile (~50th height percentile)
  mutate(TAG_archive = TAG,
    TAG = if_else(
    max > z50_L2, 
    TAG, 0)) %>% 
  group_by(supercells) %>% #group by supercells so you can filter for supercells within two circles
  mutate(count = n()) %>%  #number of rows in each supercell - this value should be 1, if it is two it means that the supercell intersected two circles 
  mutate(TAG = if_else(count > 1, 0, TAG))

st_write(obj = seg2, 
         dsn = paste0(crown_dir_folder,"Prop_Diameter",percent,"%OfZq99\\SEGS_initial.shp"),
         driver = "ESRI Shapefile",
         append = FALSE)
```
</details>

```{r seg-initial, echo=FALSE,fig.align = 'center',out.width="100%", fig.cap= "Supercells belonging to one proportional circle (purple) or two proportional circles (red). Those belonging to two proportional circles are highlighted in red and will be removed in the next step."}
knitr::include_graphics(here("Photos_&_gifs\\seg_filter_count.PNG"))
```

Next, we filter for supercells where TAG (the unique tree ID) is not zero, which implements the filtering from the above step. We then create center points, or centroids, for each supercell and filter for supercells with centroids within the proportional crowns. See Figure \@ref(fig:seg-keep) for a visual of the supercell centroids that fall within the circles (yellow), boundaries of supercells with centroids that **did not** fall within the circle (green) and those that **did** fall within the circle (orange). This code filters to only keep the supercells outlined in orange.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
seg3 = seg2 %>% 
  filter(TAG != 0) %>% 
  st_centroid() %>% 
  st_join(y = prop_hulls_forSEG, join = st_within) %>% 
  drop_na() %>% 
  dplyr::select(supercells, geometry)

st_write(obj = seg3, 
         dsn = paste0(crown_dir_folder,"Prop_Diameter",percent,"%OfZq99\\SEGS_centroids.shp"),
         driver = "ESRI Shapefile",
         append = FALSE)

seg4 = seg2 %>% 
  st_join(seg3) %>% 
  drop_na() 

st_write(obj = seg4, 
         dsn = paste0(crown_dir_folder,"Prop_Diameter",percent,"%OfZq99\\SEGS_keep.shp"),
         driver = "ESRI Shapefile",
         append = FALSE)
```
</details>

<!--  ```{r seg-centroids, echo=FALSE,fig.align = 'center',out.width="100%", fig.cap= "                 "} -->

<!--  knitr::include_graphics(here("Photos_&_gifs\\seg_centroids_Crowndelin.PNG")) -->

<!--  ``` -->

```{r seg-keep, echo=FALSE,fig.align = 'center',out.width="100%", fig.cap= "Filtered supercells (green outline) that do not have a centroid that falls within it's respective proportional circle. Supercells highlighted in orange do have a centorid that falls within it's respective proportional circle, these are the supercells that will be retained and merged to form crowns in the following step."}
knitr::include_graphics(here("Photos_&_gifs\\2_seg_keep_outline_Crowndelin.PNG"))
```

Next we merge the supercells by TAG to create individual tree crowns, as seen in Figure \@ref(fig:seg-merge) where the yellow outlines are the outwards boundaries of the merged supercells that form the tree crown.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
seg5 = seg4 %>% 
  group_by(TAG) %>% 
  summarise()
st_write(obj = seg5, 
         dsn = paste0(crown_dir_folder,"Prop_Diameter",percent,"%OfZq99\\SEGS_merge.shp"),
         driver = "ESRI Shapefile",
         append = FALSE)
```
</details>

```{r seg-merge, echo=FALSE,fig.align = 'center',out.width="100%", fig.cap= "Retained supercells (orange) merged to form crown boundaries (yellow)."}
knitr::include_graphics(here("Photos_&_gifs\\seg_merge_Crowndelin.PNG"))
```

The following two steps help clean the crowns. First, define a threshold value that represents the maximum area of a hole that you would like to fill within the crowns, here we chose 2000cm$^2$. Figure \@ref(fig:seg-merge-smooth) shows the crowns in yellow and the smoothed version of the crowns that have had holes filled in red. As you can see, the crowns are the same. This is because this section of crowns did not contain any holes that required filling.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
seg5_smooth <- seg5 %>% 
  fill_holes(threshold = units::set_units(2000, "cm^2"))

st_write(obj = seg5_smooth, 
         dsn = paste0(crown_dir_folder,"Prop_Diameter",percent,"%OfZq99\\SEGS_merge_smooth.shp"),
         driver = "ESRI Shapefile",
         append = FALSE)
```
</details>

```{r seg-merge-smooth, echo=FALSE,fig.align = 'center',out.width="100%", fig.cap= "Yellow crown boundaries are overlaid on top of crowns that have been edited to remove holes smaller than 2000 cm² (red). In this portion of the plot, no holes were present within the crowns, resulting in no visible differences between the original crowns (yellow) and those that have been refined through hole filling (red)."}
knitr::include_graphics(here("Photos_&_gifs\\seg_merge_smooth_Crowndelin.PNG"))
```

Next, we cast the crowns as polygons to remove instances of multipolygons associated with the same tree ID. Figure \@ref(fig:seg-smooth-np) displays the new polygons, outlined with blue dotted lines. Here we can see an instance of a smaller polygon shown in yellow and red that was originally grouped with the larger crown polygon within the circle, however has been filtered out in this step.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
seg5_smooth_NM <- st_cast(seg5_smooth, "POLYGON")
unique(st_geometry_type(seg5_smooth_NM)) # CHECK: should only have "Polygon" no more multipolygons
st_write(obj = seg5_smooth_NM, #saving out the crowns for manual editing
         dsn = paste0(crown_dir_folder,"Prop_Diameter",percent,"%OfZq99\\HULLS\\SEGS_merge_smooth_NoMultipolygon.shp"),
         driver = "ESRI Shapefile",
         append = FALSE)
```
</details>

```{r seg-smooth-np, echo=FALSE,fig.align = 'center',out.width="100%", fig.cap= "Blue dotted lines represent crowns that have been converted to polygon class, overlaid on the original crowns in yellow and those with holes removed in red. In the upper center of the figure, a polygon (highlighted in red and yellow) that was initially part of a multipolygon within its proportional circle has been removed during the conversion to polygon in this step."}
knitr::include_graphics(here("Photos_&_gifs\\seg_merge_smooth_NMP_Crowndelin.PNG"))
```

Lastly, we join the attributes from the original grid that contained information on each tree to the crowns shapefile. 

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
att <- ttops %>%
  as.data.frame()%>%
  dplyr::select(-c(geometry))

(seg5_smooth_NM_attributes <- left_join(seg5_smooth_NM,att, by = unique_id )) ## adding in attributes back

st_write(obj = seg5_smooth_NM_attributes, 
         dsn = paste0(crown_dir_folder,"Prop_Diameter",percent,"%OfZq99\\HULLS\\SEGS_merge_smooth_NoMultipolygon_Attributes.shp"),
         driver = "ESRI Shapefile",
         append = FALSE)
```
</details>

## Manually Edit Crowns

Figure \@ref(fig:crown-seg) is the final shapefile of delineated crowns from the above process. We then highly suggest the crowns be examined in a GIS and edited to remove sections where neighbouring branches intersect the crown to ensure each crown contains only data from the correct tree or to add in missed branches that are above your height cutoff. We recommend also adding a "crown confidence" column, or similar descriptive column, that can be populated throughout this process to mark partially obscured trees or any other instances that would lead to poor data quality down the line. This allows for these potential "problem" trees to be easily highlighted in future analysis. This was the most manually time-consuming process in the whole project.

```{r crown-seg, echo=FALSE,fig.align = 'center',out.width="100%", fig.cap= "Final crowns created using supercells and filtered with circles proportional to the height of each individual tree."}
knitr::include_graphics(here("Photos_&_gifs\\seg_crowns_Crowndelin.PNG"))
```

<!-- ```{r crown-circ, echo=FALSE,fig.align = 'center',out.width="100%", fig.cap= "                 "} -->

<!-- knitr::include_graphics(here("Photos_&_gifs\\seg_crowns_w_propcircles_Crowndelin.PNG")) -->

<!-- ``` -->

<!-- ```{r 2-delin-crowns, echo=FALSE,fig.align = 'center',out.width="60%", fig.cap= "Delineated crowns at Bit Tree Creek with partially obscured plot trees in purple."} -->

<!-- knitr::include_graphics(here("Photos_&_gifs\\crowns.PNG")) -->

<!-- ``` -->

<!--chapter:end:0_Crown_delineation.Rmd-->

# Shadow Masks {#shadow-mask}

Pixels containing shadows and openings in the canopy can introduce error in vegetation indices as reflectance values are often lower in shaded pixels (Malenovský et al., 2013; Zhang et al., 2024).  Masking out shaded pixels and openings within the crown is necessary to calculate accurate vegetation indices. We found masking with NIR reflectance to be a consistent and effective approach for masking out shadows and gaps. We use the NIR reflectance band (842nm) from the Micasense RedEgde-MX Dual. NIR thresholds are determined using the shape of the NIR distribution. If the NIR distribution is bimodal, the threshold is set to the local minimum, whereas if the distribution is unimodal, the threshold is set to the local maximum (Chen et al., 2007; D’Odorico et al., 2021; Otsu et al., 2019).

Below we define a function *find_local_min* that takes a list of first derivatives and returns a dataframe containing:

- **neg_value**: The value of the first derivative directly before the minimum (i.e. the negative derivative value to the left of the minimum)

- **pos_value**: The value of the first derivative directly after the minimum (i.e. the positive derivative value to the right of the minimum)

-	**pos_def**: The summation of the positive gradient values (a.k.a. slope values) in the first 15 gradient values following the local minimum. This value provides an idea of how great the increase in slope is after the local minimum.

-	**neg_def**: The summation of the negative gradient values in the first 15 gradient values preceding the local minimum. This value provides an idea of how great the decrease in slope is before the local minimum.

-	**Index**: Index position of the negative gradient value bordering the local minimum. This is the index of our threshold value since we do not get a true zero slope value at the local minimum but rather a change from a very small negative gradient value to a very small positive gradient value.

-	**definition**: The summation of the absolute value of pos_def and neg_def values. The greater this number, the more defined the minimum. 
The above parameters were added to describe each minimum found by the function so that the true local minimum could be filtered for.

This function will be used in the next few steps to isolate minimums in the NIR distribution.

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
# This function finds local minimums and ranks how defined they are by the 'definition' attribute
find_local_min <- function(values) { #input "values" is a list of 1st derivatives of NIR values
  #initializing variables
  neg_slope <- numeric() 
  pos_slope <- numeric()
  index_of_closest <- numeric()
  definition = numeric()
  neg_sum = numeric()
  pos_sum = numeric()
  
  k <- 1 #initializing iterator 
  
  for (i in 2:length(values)) { #skip index 1 so following "values[i-1]..." can properly index
    #print(i)
    if (values[i - 1] < 0 & values[i] >= 0 & i > 15) { 
      #finds a change from negative to positive 1st derivative (local min)
      #i>15 because local min will not be in first 15 values and this stops an error occurring where a local min is found in the first 15 values and the def_positive indexing does not work
      def_positive <- c(values[i:(i+15)]) # vector of next 15 gradient values
      def_pos <- def_positive[def_positive > 0] #only taking positive 1st derivative values (aka positive slopes)
      pos_sum[k] <- sum(def_pos) #adding up the positive 1st derivative values
      
      def_negative <- c(values[(i - 15):i]) # vector of the 15 values to the left of the local min (negative 1st derivatives)
      def_neg <- def_negative[def_negative < 0] #only taking the negative slopes in the list
      neg_sum[k] <- sum(def_neg) #adding up negative values
      
      neg_slope[k] <- values[i - 1] #1st derivative value at i-1 (right before sign change from neg to pos)
      pos_slope[k] <- values[i] #1st derivative at i (at the switch from neg to positive)
      index_of_closest[k] <- i - 1 #index value of the last neg 1st derivative before the local min
      definition[k] <- sum(abs(def_neg), abs(def_pos)) # higher the value, more pronounced the local min
      k <- k + 1
    }
  }
  return(data.frame(neg_value = neg_slope, pos_value = pos_slope, Index = index_of_closest, definition = definition, neg_def = neg_sum, pos_def = pos_sum))
}
```
</details>

To begin, we load the necessary packages, the multispectral orthomosaic that contains the NIR (842nm) band, and the delineated crowns shapefile and create the folder where the shadow mask will be saved to:

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
#Required Packages
library(ggplot2) # to plot 
library(terra) # to work with the orthomosaics (rasters)
library(dplyr) #for data manipulation
library(tidyverse) # for data manipulation
library(sf) # to work with the delineated crown polygons
library(pracma) # for gradient/ derivative function
library(LaplacesDemon) # is.multimodal function

# Setting directory 
dir = "Change to match your folder strucutre" #directory where shadow mask folder will be made. This dir is also used in path names for the orhtomosaics. Change up paths throughout the code to call your data.

# Reading in the multispectral orthomosaic
ms_temp = rast(list.files(paste0(dir, "metashape\\3_MS_ORTHO\\"), pattern = ".*MS_Calibrated.*_bestPanel.tif$", full.names = TRUE)) # here we read in the ortho that is in the set directory and contains "MS_Calibrated" in the name and end in "_bestPanel.tif". We had multiple orthos in this folder, so did this to ensure the proper one was called. Change to match your ortho name, or remove the pattern if you only have one ortho in the defined path.
  
# Reading in the shapefile containing delineated crowns and buffering inward by 5cm to limit any mixed pixels from neighboring vegetation
dir_crowns <- "D:\\Sync\\Fdc_PR_Canoe\\Crowns.shp" #path to crowns shapefile
pols_spat = st_read(paste0(dir_crowns)) %>% # reading in crown shp
  filter(!st_is_empty(.)) %>% #removing empty
  st_buffer(dist = -.05) %>% #buffering inward by 5cm
  vect()

# Creating a folder for shadow masks
Nir_shadow_folder_baseName <- "NIR_shadow_mask_localMinOrMax" #name of the folder shadow masks will be written to. We have the folder name defined outside the folder creation step below so that we could change the folder name once without having to change it throughout the code.
if (!dir.exists(paste0(dir, Nir_shadow_folder_baseName,"\\"))) {
  dir.create(paste0(dir,Nir_shadow_folder_baseName,"\\"))
  }
```
</details>

Next, the NIR band is selected from the multispectral orthomosaic. Here you can choose to:

1) Crop the mulispectral othomosaic to the extent of the crowns shapefile. We have found this to be a good option for mature sites with minimal exposed ground.

2) Mask the mulispectral othomosaic to the individual crowns. We have found this to be a good option for younger sites with lots of ground exposure. 

Below we crop the raster to the extent of the polygons and reformat from a raster to a vector of values:

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
#Create a vector of near-infrared (NIR) values from the 10th multispectral band (842nm)
NIR = ms_temp[[10]] %>% #isolating the NIR (842nm) band
  crop(pols_spat) %>% #cropping to the extent of the crown polygon shp
  clamp(upper = 50000, values = FALSE) %>% 
  as.vector()

```
</details>

Density of the NIR values is calculated and plotted, after removing NA values.
<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
NIR_na <- na.omit(NIR) #remove NA values from the NIR vector
density_values <- density(NIR_na) # calculates density of NIR values
# Plot to see the distribution of NIR values
plot(density_values) # check to see the distribution of NIR values (ie a visual check for whether or not a local min exists)
```
</details>

At this point the threshold can be estimated manually from the graph however, this is not a feasible method when you have many data acquisitions, nor is it the most accurate method. Below we describe the method we used to create shadow masks for many acquisitions withouth having to visually inspect the NIR distributions. To computationally find the threshold we first need to decipher whether the NIR distribution is unimodal (one peak) or multimodal (more than one peak). To do so we use the *is.multimodal* function from the LaplacesDemon package. 

If the distribution was multimodal, we found the local minimum by:

1) calculating the first derivatives fo the NIR density values and storing the derivatives in dy_dt

2) applying the *find_local_min* function to the list of first derivatives to find minimums

3) filtering out small minimums that are not true minimums but rather dips on a larger slope

4) Identifying false multimodals and assigning the threshold value to be the local maximum

5) Identifying the largest minimum (a.k.a. the local minimum) and setting that as the threshold value

If the distribution was not multimodal (and therefore unimodal), we found the local maximum by:

1) locating the NIR value that corresponds to the greatest density of values in the NIR distribution

**Note:** for each method there are **mode** and **thresh_name** variables defined. These variables are used to annotate the histrogram made in the next step 

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
if(is.multimodal(NIR_na)){ # if the NIR distribution is multimodal, continue to the below steps
    
    dy_dt <- pracma::gradient(density_values$y) #list of first derivatives of NIR vector
    
    zeros <- find_local_min(dy_dt) # Finds index locations where slope switches from neg to post (local min) and ranks the intensity of each local min
    zeros_filtered <- zeros[(zeros$pos_def > 0),] # Filters rows with pos_def > 0, filtering out small minimums on negative slopes (not true local mins)
    
    if(nrow(zeros_filtered)==0){#if empty, then detected a false multimodal distribution and defaulting to max as threshold value
      print("It is a false multimodal")
      max_density = max(density_values$y, na.rm = TRUE)
      threshold = density_values$x[which(density_values$y == max_density)]
      mode <- "Unimodal (False Multi)"
      thresh_name <- "LocalMax"
      
      }else{ 
      zeros_local_min <- zeros_filtered[which.max(zeros_filtered$definition), ] #isolating the largest local min
      # x_zeros <- density_values$x[zeros_local_min$Index] #selecting NIR (aka x_mid) value that corresponds to the index value of the most defined local min 
      threshold <- density_values$x[zeros_local_min$Index] #selecting NIR (aka x_mid) value that corresponds to the index value of the most defined local min 
      
      # Catching cases of "inf" returns:
      if (threshold <= 0.7 & threshold > 0){#setting limits on the threshold to remove any thresholds from the tails of the distribution
        print(threshold)
        mode <- "Multimodal"
        thresh_name <- "LocalMin"
        
        }else{
        print("Multimodal with org thresh > 0.7") #This output usually indicates an error in the function, make sure to look at the NIR distribution to confirm this is in fact correct or if the code needs modification for a special case
        #This often indicates a false multimodal as well, and therefore the NIR value with the max frequency in the NIR distribution with be used as the threshold
        max_density = max(density_values$y, na.rm = TRUE)
        threshold = density_values$x[which(density_values$y == max_density)]
        mode <- "Unimodal (False Multi with org thresh > 0.7)"
        thresh_name <- "LocalMax"
      }
    }
  }else{ #if the NIR vector is NOT multimodal, continue to the below steps
    #Finding NIR value with the greatest frequency in the distribution - this max value will be the threshold for unimodal distributions 
    max_density = max(density_values$y, na.rm = TRUE)
    threshold = density_values$x[which(density_values$y == max_density)]
    print(threshold)
    thresh_name <- "LocalMax"
    mode <- "Unimodal"
  }
```
</details>

Next, we plot a histogram of the NIR values and annotate it to contain:

- a vertical line at the threshold

- the threshold value

- the mode (i.e. unimodal verse mulitmodal)

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
(hist = NIR %>% #plotting a histogram of NIR values with a vertical red line for the defined threshold value
    as_tibble() %>% 
    ggplot() +
    geom_histogram(aes(x = NIR), bins = 150) +      
    geom_vline(xintercept = threshold, color = "red3") +
    labs(title = paste0(mode, " , threshold: ",round(threshold, digits = 2)))+
    theme_bw()+
    theme(panel.grid.major = element_blank(),           
          panel.grid.minor = element_blank(),
          plot.title = element_text(size = 13,hjust = 0.75, vjust = -28)))
  
ggsave(hist, #saving out the plot
       filename = paste0(dir, Nir_shadow_folder_baseName,"\\", date_list[x], "_NIR_shadow_hist_localMin_orMax.jpeg"),
       device = jpeg,
       width = 8,
       height = 8)
```
</details>

```{r echo=FALSE, out.width="60%", fig.align = 'center'}
knitr::include_graphics(here("Photos_&_gifs\\multimodal_hist_canoe_2022_08_05.jfif"))
```

We then isolate the NIR band from the multispectral orthomosaic and filter the raster to set pixels with NIR values greater than the threshold to NA and those that are less than or equal to the threshold to 1.

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
shadow_mask = ms_temp[[10]] #isolating the NIR band of the multispectral ortho
shadow_mask[shadow_mask > threshold] = NA #for NIR values > threshold, make them NA
shadow_mask[shadow_mask <= threshold] = 1 #for NIR values < or = to the threshold value, make them 1
  
# Writing out the shadow mask that has values of 1 for all pixels that will be masked out
terra::writeRaster(shadow_mask, paste0(dir, Nir_shadow_folder_baseName,"\\",  date_list[x], "_NIR_shadow_thresh",threshold ,"_",thresh_name, ".tif"),
                     overwrite = TRUE)
```
</details>

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics(here("Photos_&_gifs\\shadow_mask_example_no_filter_2022_08_05.PNG"))
```


Lastly, we apply the *clump* function to group adjacent pixels that represent shadows creating "clumps" a.k.a. shadow patches. These shadow patches are then converted into a dataframe containing a unique ID per shadow patch and the number of pixels within each patch. 
<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
# Grouping adjacent pixels with the same value (i.e. representing shadowed areas) into distinct patches or clumps
shadow_patches = raster::clump(raster::raster(shadow_mask), directions = 4) %>%
  rast()
# Summarizing the clumps obtained from the shadow patches raster. It contains two columns: 
#1. value: representing the unique ID of each clump:
#2. count: representing the number of pixels within each clump
clumps = data.frame(freq(shadow_patches))
```
</details>

Here we filter the shadow patches by a threshold to remove insignificant patches and write out the cleaned shadow mask for future use.

The threshold **num_pix** is calculated by:

- setting an initial area threshold of 0.02m^2 (200cm^2)
 
- dividing the 0.02m^2 threshold by the area of a pixel to determine the threshold number of pixels a shadow patch must have to not be filtered out

In our case, the resolution of the raster was just over ~3cm, giving us a threshold of ~23 pixels.

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
# Calculating the threshold for the number of pixels that a clump must contain to be considered significant
# It is calculated based on the desired area threshold (200 cm²) divided by the area of a single pixel
num_pix = 0.02 / (res(shadow_patches)[1]^2) #0.02 represents 200cm² in meters
flecks = clumps[clumps$count > num_pix,] # remove clump observations with frequency smaller than the threshold
flecks = as.vector(flecks$value) # record IDs from clumps which met the criteria in previous step
  
new_mask = shadow_patches %in% flecks #keep clumps that have IDS in flecks
new_mask[new_mask == 0] = NA # make clumps that are zero, NA
  
#writing out a 'cleaned' shadow mask  
terra::writeRaster(new_mask, paste0(dir, Nir_shadow_folder_baseName,"\\",  date_list[x], "_NIR_shadow_thresh",threshold ,"_",thresh_name, "_mask2.tif"),
                     overwrite = TRUE)
```
</details>

Below you can see that in this case the filter did not result in a large change in the shadow mask as there were minimal small patches to begin with. Here the white areas in the filtered mask image on the right are areas that will not be masked given that the mask did not contain enough pixels. This will result in a less patchy shadow mask.

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics(here("Photos_&_gifs\\shadow_mask_example_filtered_2022_08_05.PNG"))
```



**References:**

Chen, Y., Wen, D., Jing, L., Shi, P., 2007. Shadow information recovery in urban areas from very high resolution satellite imagery. Int J Remote Sens 28. https://doi.org/10.1080/01431160600954621

D’Odorico, P., Schönbeck, L., Vitali, V., Meusburger, K., Schaub, M., Ginzler, C., Zweifel, R., Velasco, V.M.E., Gisler, J., Gessler, A., Ensminger, I., 2021. Drone-based physiological index reveals long-term acclimation and drought stress responses in trees. Plant Cell Environ 44. https://doi.org/10.1111/pce.14177

Malenovský, Z., Homolová, L., Zurita-Milla, R., Lukeš, P., Kaplan, V., Hanuš, J., Gastellu-Etchegorry, J.P., Schaepman, M.E., 2013. Retrieval of spruce leaf chlorophyll content from airborne image data using continuum removal and radiative transfer. Remote Sens Environ 131, 85–102. https://doi.org/10.1016/J.RSE.2012.12.015

Otsu, K., Pla, M., Duane, A., Cardil, A., Brotons, L., 2019. Estimating the threshold of detection on tree crown defoliation using vegetation indices from uas multispectral imagery. Drones 3. https://doi.org/10.3390/drones3040080

Zhang, C., Chen, Z., Yang, G., Xu, B., Feng, H., Chen, R., Qi, N., Zhang, W., Zhao, D., Cheng, J., Yang, H., 2024. Removal of canopy shadows improved retrieval accuracy of individual apple tree crowns LAI and chlorophyll content using UAV multispectral imagery and PROSAIL model. Comput Electron Agric 221, 108959. https://doi.org/10.1016/J.COMPAG.2024.108959

<!--chapter:end:0_Shadow_Mask.Rmd-->

# Crown-level Vegetation Indicies {#Vegetation-Indicies}

Vegetation indices (VIs) are important tools for near-real time monitoring and function as proxies for health, productivity, structural changes, and stress.  

Below we describe a few commonly used VIs that will be calculated in this workflow: 

- **Normalized Difference Vegetation Index (NDVI)** is of the oldest vegetation indices established in vegetation monitoring and is used as a metric of greenness. However, due to saturation of the chlorophyll absorption peak around 660-680nm at moderately low chlorophyll levels, NDVI is often not able to tease out small differences between healthy plants (Sims & Gamon, 2002). To solve this issue, the red reflectance band can be replaced with a red edge reflectance band creating the normalized difference red edge index mentioned below. 

- **Normalized Difference Red Edge index (NDRE)** normalizes a reflectance band in the red edge to a reflectance band in the near-infrared region to estimate chlorophyll content. This index is a derivative of NDVI that is sensitive to shifts in chlorophyll content at high concentrations (Clevers & Gitelson, 2013; Evangelides & Nobajas, 2020). 

- **Chlorophyll Carotenoid Index (CCI)** is a proxy for the ratio of chlorophylls to carotenoids in pigment pools. It is often used as a metric for tracking the onset of the growing season and photosynthetic activity (Gamon et al., 2016).  

- **Photochemical Reflectance Index (PRI)** can be used as a proxy for xanthophyll pigment epoxidation or changes in bulk seasonal pigment pool ratios depending on the timescale of analysis. Over a scale of milliseconds to minutes PRI can isolate the photoprotective conversion of violaxanthin into antheraxanthin and zeaxanthin within the xanthophyll cycle by normalizing the 531nm reflectance band to the 560nm reference reflectance band. However, on the seasonal scale the 560nm reflectance band no longer acts as a reference for isolating changes in xanthophyll pigments since it changes with bulk pigment shifts. Hence, seasonal PRI measurements are instead used as another proxy for the ratio of carotenoid to chlorophyll pigments and are used to show changes in photosynthetic activity (Wong et al., 2020). 

- **Red Edge (RE) slope** captures changes in chlorophyl content and structural health. Steep RE slopes generally indicate healthy vegetation while more gradual slopes often indicate stressed vegetation. This is due to the base of the RE slope sitting around a main chlorophyll absorption peak and the end of the RE slope sitting near the near-infrared (NIR) range. Hence, the more chlorophyll the lower the reflectance around the base of the RE slope and the greater the reflectance in the NIR, often attributed to healthy vegetative material, the steeper the overall slope of the RE (Sims and Gamon, 2002, Clevers and Gitelson)  

- **Green Chromatic Cordinate (GCC)** is a measure of green reflectance relative to the total reflectance in the visible light portion on the electromagnetic spectrum. It is a metric of greeness that is often used as a proxy for vegetation health (Reid et al., 2016). 

## VI Workflow 

Below are code chunks that take the [delineated crowns](#crown-delineation) and [multispectral orthomosaics](#processing-orthomosaics) and output a .rds file containing several popular vegetation indices at the crown-level. In this script we demonstrate how to calculate both mean and median values of vegetation indices per crown.

We begin by loading the necessary packages:
<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
library(terra) # to work with the orthomosaics (rasters)
library(dplyr) #for data manipulation
library(tidyverse) # for data manipulation
library(sf) # to work with the delineated crown polygons
library(exactextractr) # for the exact_extract function

```
</details>

Next, we read in the multispectral orthomosaic and shadow mask. The shadow mask will work to remove shadowed pixels so they do not skew the values for the vegetation indices. See the section on [shadow masks](#shadow-mask) for a detailed workflow.

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
# Load in multispectral orthomosaic 
ms_ortho = rast(list.files(paste0(dir, "metashape\\3_MS_ORTHO\\"), pattern = ".*MS_Calibrated.*_bestPanel.tif$", full.names = TRUE)) #path to multispectral ortho
# Reading in the NIR shadow mask
shadow_mask = rast(list.files(paste0(dir, Nir_shadow_folder,"\\"),  pattern = ".*_NIR_shadow.*_thresh.*_mask2.tif$", full.names = TRUE))# NIR shadow mask
# Below selects the root name of the multispectral ortho ie: "Name_of_multispectral_ortho" that will be used to name the multispectral shadow masked orthos created in the following steps
ms_ortho_name_root <- substr(names(ms_ortho)[1], 1, nchar(names(ms_ortho)[1]) - 2)
#names(ms_ortho)[1] grabs the name for band 1 of the ms_ortho: ie: "Name_of_multispectral_ortho_1", so that the substr function can selection the rootname as done above
```
</details>

To speed up processing, we mask the shadow mask to only contain areas inside delineated crowns.

The shadow mask is a raster containing values 1 (shadowed pixel) and NA (not shadowed pixel). The *mask* function below identifies pixels in the multispectral orthomosaic that align with shadowed pixels in the shadow mask and changes their value to NA. 

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
# Mask shadow mask to delineated crowns:
shadow_mask <- terra::mask(shadow_mask, pols)
  
# Mask the multispectral ortho using the shadow mask
ms_mask <- terra::mask(ms_ortho, shadow_mask, maskvalues = 1, updatevalue = NA)

#writing out the shadow masked multispectral ortho
terra::writeRaster(ms_mask, paste0(dir, Nir_shadow_folder,"\\",ms_ortho_name_root,"_NirShadow_masked.tif"),
                     overwrite = TRUE)
  
```
</details>

The shadow masked multispectral orthomosaic is then masked to the delineated crowns polygon to speed raster calculations in the next step.

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
# Mask shadow masked raster to delineated crowns
ms_mask <- terra::mask(ms_mask, pols)
# Write out the masked raster
terra::writeRaster(ms_mask, paste0(dir, Nir_shadow_folder,"\\",ms_ortho_name_root,"_NirShadow_ms_mask_to_pols.tif"),
                     overwrite = TRUE)
```
</details>

Here we calculate several vegetation indices using the multispectral orthomosaic from above with shadowed pixels and non-crown pixels removed.

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
# List of band reflectances (R444 - R842) and index values that will be calculated per tree crown
rast_list = list(
  # reflectance values
  R444 = ms_mask[[1]],
  R475 = ms_mask[[2]],
  R531 = ms_mask[[3]],
  R560 = ms_mask[[4]],               
  R650 = ms_mask[[5]],               
  R668 = ms_mask[[6]],               
  R705 = ms_mask[[7]],               
  R717 = ms_mask[[8]],               
  R740 = ms_mask[[9]],               
  R842 = ms_mask[[10]],                              
    
  # Near-infrared greenness
  mDatt = (ms_mask[[10]] - ms_mask[[8]]) / (ms_mask[[10]] - ms_mask[[6]]),
  NDVI = (ms_mask[[10]] - ms_mask[[6]]) / (ms_mask[[10]] + ms_mask[[6]]), 
    
  # Chlorophyll 
  NDRE1 = (ms_mask[[10]] - ms_mask[[7]]) / (ms_mask[[10]] + ms_mask[[7]]),     
  NDRE2 = (ms_mask[[10]] - ms_mask[[8]]) / (ms_mask[[10]] + ms_mask[[8]]), 
  NDRE3 = (ms_mask[[10]] - ms_mask[[9]]) / (ms_mask[[10]] + ms_mask[[9]]), 
  EVI = (2.5 * (ms_mask[[10]] - ms_mask[[6]])) / (ms_mask[[10]] + (6 * ms_mask[[6]]) - ( 7.5 * ms_mask[[1]] + 1)),             
  GCC = (ms_mask[[4]]+ ms_mask[[3]]) / (ms_mask[[1]] + ms_mask[[2]] + ms_mask[[3]]+ms_mask[[4]] + ms_mask[[5]] + ms_mask[[6]]),             
  
  # Carotenoids, waxes
  ARI = (1 / ms_mask[[4]]) - (1 / ms_mask[[7]]),    
  EWI9 = (ms_mask[[6]] - ms_mask[[8]]) / (ms_mask[[6]] + ms_mask[[8]]),
    
  # Carotenoids             
  PRI = (ms_mask[[3]] - ms_mask[[4]]) / (ms_mask[[3]] + ms_mask[[4]]),             
  CCI = (ms_mask[[3]] - ms_mask[[5]]) / (ms_mask[[3]] + ms_mask[[5]]),             
    
  # Red edge             
  RE_upper = (ms_mask[[9]] - ms_mask[[8]]) / 23,             
  RE_lower = (ms_mask[[8]] - ms_mask[[7]]) / 12,             
  RE_total = (ms_mask[[9]] - ms_mask[[7]]) / 35
)
rast_all = rast(rast_list)
```
</details>

Below is an example of the resolution of the calculated vegetation indices. The image shows the chlorophyll carotenoid index (CCI) of a single crown across four summer time points. Greener pixels represent a higher ratio of chlorophylls to carotenoids. As you can see, the proportion of the crown with high CCI values increases from the May to July acquisitions, when productivity is likely at its highest, and decreases during the august acquisition suggesting lower productivity, likely due to end of summer drought conditions. 

```{r echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics(here("Photos_&_gifs\\CCI_timeseries.png"))
```

Lastly, we use the *exact_extract* function to calculate crown-level statistics per vegetation index raster created above. Below we show an example of taking the mean and median crown values per index as well as count the number of pixels that were used for the index calculation. We do this as a data quality check so we can remove crowns that had very few pixels involved in their index calculation.

**Note:** the append_cols parameter should be set to a list of column names that exist in pols  (the shapefile of delineated crowns) that you would like to be appended to the dataframe containing vegetation indcies per tree crown. As you can see below we have attached many columns we found useful, however the only column necessary to append is a column in pols that represents a unique ID per tree crown.

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
# Calculating Mean Index values per crown
df_spectral_mean = exact_extract(rast_all, pols, fun = "mean", append_cols = c("treeID","Edited","tag__","rep","blk","fam","fem"))
  
# Calculating the number of non-masked pixels used for index calculation
df_count = exact_extract(ms_mask[[1]], pols, fun = "count", progress = TRUE, append_cols = c("treeID","Edited","tag__","rep","blk","fam","fem"))
  
# Joining the mean index values df and the count values df
df_spectral_mean_count <- merge(df_spectral_mean,df_count, by = c("treeID","Edited","tag__","rep","blk","fam","fem"))
  
# Saving out mean crown values + non masked pixel count to an rds file
saveRDS(df_spectral_mean_count, paste0(dir_D,"\\",updated_metrics_folder,"\\", date_list[x], "_NIRshadowMask_MeanCrownSpectralIndices.rds"))

# Calculating Median Index values per crown
df_spectral_median = exact_extract(rast_all, pols, fun = "median", append_cols = c("treeID","Edited","tag__","rep","blk","fam","fem"))
  
df_spectral_median_count <- merge(df_spectral_median,df_count, by = c("treeID","Edited","tag__","rep","blk","fam","fem"))
  
# Saving out median crown values + non masked pixel count to an rds file
saveRDS(df_spectral_median_count, paste0(dir_D,"\\",updated_metrics_folder,"\\", date_list[x], "_NIRshadowMask_MedianCrownSpectralIndices.rds"))
```
</details>



**References:**

Clevers, J. G. P. W., & Gitelson, A. A. (2013). Remote estimation of crop and grass chlorophyll and nitrogen content using red-edge bands on Sentinel-2 and -3. International Journal of Applied Earth Observation and Geoinformation, 23(1), 344–351. https://doi.org/10.1016/J.JAG.2012.10.008 

Evangelides, C., & Nobajas, A. (2020). Red-Edge Normalised Difference Vegetation Index (NDVI705) from Sentinel-2 imagery to assess post-fire regeneration. Remote Sensing Applications: Society and Environment, 17, 100283. https://doi.org/10.1016/J.RSASE.2019.100283 

Gamon, J. A., Huemmrich, K. F., Wong, C. Y. S., Ensminger, I., Garrity, S., Hollinger, D. Y., Noormets, A., & Peñuelask, J. (2016). A remotely sensed pigment index reveals photosynthetic phenology in evergreen conifers. Proceedings of the National Academy of Sciences of the United States of America, 113(46), 13087–13092. https://doi.org/10.1073/pnas.1606162113 

Reid, A. M., Chapman, W. K., Prescott, C. E., & Nijland, W. (2016). Using excess greenness and green chromatic coordinate colour indices from aerial images to assess lodgepole pine vigour, mortality and disease occurrence. Forest Ecology and Management, 374, 146–153. https://doi.org/10.1016/J.FORECO.2016.05.006 

Sims, D. A., & Gamon, J. A. (2002). Relationships between leaf pigment content and spectral reflectance across a wide range of species, leaf structures and developmental stages. Remote Sensing of Environment, 81(2–3), 337–354. https://doi.org/10.1016/S0034-4257(02)00010-X 

Wong, C. Y. S., D’Odorico, P., Arain, M. A., & Ensminger, I. (2020). Tracking the phenology of photosynthesis using carotenoid-sensitive and near-infrared reflectance vegetation indices in a temperate evergreen and mixed deciduous forest. New Phytologist, 226(6). https://doi.org/10.1111/nph.16479 

<!--chapter:end:0_Vegetation-Indicies.Rmd-->

# LiDAR Processing Workflow

## DJI Terra 


- Open DJI Terra and start a LiDAR Point Cloud Processing session
- Import the flight files, including imagery if you would like a colorized point cloud
- Below are the settings we use to process the point cloud:
  - We use the deafult settings set by DJI with the exception of setting the DEM resolution to 10cm
  
```{r echo=FALSE, fig.align = 'center',out.width="60%"}
knitr::include_graphics(here("Photos_&_gifs\\DJI_settings.PNG"))
```


## Register the LiDAR to the DAP point cloud
	
Convention is generally to register DAP to LiDAR however for this specific project it made more sense to register the LiDAR to the DAP because:

- We were flying biweekly multispectral and RGB imagery and only 2-3 LiDAR acquisitions a year. 

- We could not load the LiDAR point cloud into Agisoft Metashape to use as a reference for registration however could register all DAP clouds to a reference DAP cloud in Agisoft.

- This allowed us to more easily register the many multispectral and RGB acquisitions to a defined template in Agisoft Metashape and use the exported point cloud of the template DAP to register the LiDAR in CloudCompare.

- This process worked well for us since our priority was centimeter-level registration of orthomosaics and LiDAR between many dates and across multiple sensors. 


### Prepare LiDAR for registration

Below are snipets of a script written in LAStools that is used to ensure the LiDAR and DAP clouds are in the proper projection, filters out points in the LiDAR that are above a defined threshold, and clips the LiDAR to a boundary polygon to speed up registration by avoiding working with excess data in CloudCompare.

The unparsed script can be found on the project GitHub, see [GitHub link](index.html#github-link) 

1. Ensures both the DAP point cloud and LiDAR file are in the proper projection (NAD83 UTM 10N in our case) 
    - path_to_L1_las is the path to the LiDAR .las file from DJI terra.
    - path_to_write is where the path the projected .laz file will be written to. The files will be saved out with the same file name as the origianl LiDAR file with a "_nad83" suffix.
    - "REM" works to comment out a line in LAStools as "#" does in R.

<details>
<summary>Click to show the code</summary>
```{LAStools eval=FALSE, include=TRUE}
REM Project DAP and save out as a .laz
las2las -i path_to_DAP_las_file ^
	-odir path_to_write ^
	-odix _nad83 ^
	-olaz ^
	-nad83 ^
	-utm 10north ^
	-cpu64 ^
	-v

REM Project LiDAR and save out as a .laz
las2las -i path_to_L1_las ^
	-odir path_to_write ^
	-odix _nad83 ^
	-olaz ^
	-nad83 ^
	-utm 10north ^
	-cpu64 ^
	-v

```
</details>

2. Drops points in the LiDAR point cloud above a user defined threshold to remove noisy points that are not from the canopy (i.e. due to air moisture, birds, etc.). To determine this height threshold you can plot a histogram of Z values or visualize the point cloud in a software that allows 3D point cloud visualization to ensure that the threshold will not result in any top canopy points being removed. Though we use CloudCompare for point cloud registration, we recommend Potree to visualize point clouds as it handles large point clouds quickly and with ease. Both Potree and CloudCompare are open-source software.
    - If you do not have any point above the canopy or below the ground that need removing you can skip this step.
    - path_to_projected_L1 is the path to the projected LiDAR .laz file from above
    
<details>
<summary>Click to show the code</summary>
```{LAStools eval=FALSE, include=TRUE}
REM dropping points above 160m (160m is the height cutoff for this dataset)
las2las -i path_to_projected_L1 ^
	-odir path_to_write ^
	-odix _droppedPtsAbove160m ^
	-olaz ^
	-drop_z_above 160 ^
	-cpu64 ^
	-v
```
</details>

3. Lastly we clip the LiDAR file to a boundary polygon of the site to remove excess surrounding data that can slow processing in CloudCompare.
    - path_to_shp is the path to the site shapefile.
    - path_to_projected_below160m_L1 is the path to .laz file that has been projected and filtered for points above a set threshold in steps 1 and 2 above.
    - "-odix _clipped" will save the new laz file with the same name as the input file with "_clipped" attached to the end, change the "_clipped" to work with your naming system.

<details>
<summary>Click to show the code</summary>
```{LAStools eval=FALSE, include=TRUE}
lasclip -i path_to_projected_below160m_L1 -merged -path_to_shp -odir path_to_write -odix _clipped -olaz
```
</details>

### Register LiDAR to DAP cloud in CloudCompare
Import both the LiDAR and DAP clouds into CloudCompare (CC).  This can take up to twenty minutes.  Allow all for the first two warnings. 

```{r CC-las-input, echo=FALSE,  fig.show="hold",fig.align='center', out.width=c("32.5%","67.5%"), fig.cap = "Left: Apply all for this initial CloudCompare popup setting. Right: select 'Yes to All' for this second setting"}
knitr::include_graphics(c(here("Photos_&_gifs\\CC_las_import.png"),
                          here("Photos_&_gifs\\CC_las_import_shift.png")))

```



#### Examine the point clouds for artifacts.  
The L1 will reflect flight lines when there is moisture in the air.  Below is an example point cloud from a damp day on the coast just after a fog past through. For sake of the example, it was not filtered in LAStools with a height threshold. 

```{r CC-point-cloud, echo=FALSE, fig.align = 'center',out.width="100%"}
knitr::include_graphics(here("Photos_&_gifs\\CloudCompare_1.PNG"))
```

The residual fog patches seen above the canopy can be clipped out using the cross section or segment tool.

```{r CC-cross-section-tool, echo=FALSE, fig.align = 'center',out.width="100%"}
knitr::include_graphics(here("Photos_&_gifs\\CC_cross_section_tool.png"))
```
```{r CC-cross-section-tool-polyClip, echo=FALSE, fig.align = 'center',out.width="100%"}
knitr::include_graphics(here("Photos_&_gifs\\CC_cross_section_tool_poly.png"))
```

#### Rough Alignment 

The goal is to roughly align (move) the LiDAR to our reference “template” DAP dense cloud ahead of the ICP fine adjustment algorithm.  

Problem: Manually moving the LiDAR cloud to roughly align with the DAP cloud caused Cloud Compare to crash or hang when working with larger data sets.   

Solution: Use the Apply Transformation function in CloudCompare – Highlight the LiDAR layer in the DB tree - Hit ctrl T or Apply Transformation in the Edit menu. 


In this example the LiDAR is the lower. 
```{r echo=FALSE,fig.align = 'center',out.width="100%"}
knitr::include_graphics(here("Photos_&_gifs\\CloudCompare_2.PNG"))
```


First apply a Z transformation. It is best to do this while viewing the edge of the plot. Here we applied a Z transformation of +6. 

```{r echo=FALSE,fig.align = 'center',out.width="100%"}
knitr::include_graphics(here("Photos_&_gifs\\CloudCompare_3.PNG"))
```


Next Apply transformations in the x and y axis.   

Find a section of the clouds where you can easily see the alignment.  In this case there is a single large leave tree in the centre of the plot.  The LiDAR is the one on the left. 

Here we see the LiDAR is offset to the left (negative) on the Y(green) axis and up (positive) on the x(red) axis.  


```{r CC-stem-transform, echo=FALSE,  fig.show="hold",fig.align='center', out.width="50%", fig.cap = "Left:      . Right: ."}
knitr::include_graphics(c(here("Photos_&_gifs\\CC-stem.png"),
                          here("Photos_&_gifs\\CC-stem-aligned.png")))

```

Be careful not to rotate, only transform. 


In this case the LiDAR was shifted -5 in the X and +5 in the Y axis in total.  The shift was done in three smaller iterations to achieve the alignment seen in the above screen capture. 

#### Fine Registration: Iterative Closest Point (ICP) 

Highlight both clouds in the DB tree and apply the fine registration (ICP) algorithm using the following settings. Careful to set a max thread count that matches the resources you have available. 

```{r echo=FALSE,fig.align = 'center',out.width="60%"}
knitr::include_graphics(here("Photos_&_gifs\\CloudCompare_6.PNG"))
```

Save out the registered LiDAR point clouds at the highest resolution

### Rescale and tile the registered LiDAR: 

Below are snipets of a script written in LAStools that is used to project, rescale and tile the registered LiDAR point cloud.

The unparsed script can be found on the project GitHub, see [GitHub link](index.html#github-link)

1. Projecting the registered LiDAR to the proper projection (NAD83 UTM 10N). 
    - If you are working with already tiled data or multiple .laz/.las files on a multi-core computer than you can use the cores command shown below. Here we defined cores=4 which allows 4 cores to work on the command simultaneously on different files. If you are only working with one .laz/.las file at this point there is no need to specify the number of cores and the "^  -cores %cores%" following the "-v ^" should be removed from the below code
    - Change "path_to_registered_lidar" to the path to the registered LiDAR data exported from CloudCompare.
    - "\*.las" takes the las file in that folder, change to the file name if have more than one .las in the folder and you want to specify a specific file.
    - Change "path_to_write\01_proj_NAD83" to the path you would like the new projected  .laz file to be written to.

<details>
<summary>Click to show the code</summary>
```{LAStools eval=FALSE, include=TRUE}
set cores=4

las2las -i path_to_registered_lidar\*.las ^
    -odir path_to_write\01_proj_NAD83 ^
    -odix _nad83 ^
    -olaz ^
    -nad83 ^
    -utm 10north ^
    -cpu64 ^
    -v ^
    -cores %cores% 
```
</details>

2. Rescales the data which is necessary to later load into R for normalization and metric calculations.
    - Registered LiDAR is exported from CloudCompare at the highest resolution which changes the scale of the data, hence rescaling the x,y,z to 0.01 is necessary to avoid warnings/errors in R
    - "path_to_write" is in the input dir and the output dir because its the main folder we are now working in
    - "path_to_write\01_proj_NAD83\*.laz" selects the .laz file in the "path_to_write\01_proj_NAD83" folder, if there are more than one .laz file in your folder and you want to specify which to call, change the "*" to the name of the file.
    - The output .laz file will be written to the "path_to_write\02_rescaled" folder with the same name as the original file.
    
<details>
<summary>Click to show the code</summary>
```{LAStools eval=FALSE, include=TRUE}
las2las -i path_to_write\01_proj_NAD83\*.laz ^
        -rescale 0.01 0.01 0.01 ^
        -cpu64 ^
        -utm 10north ^
        -v ^
        -odir path_to_write\02_rescaled ^
        -olaz
```
</details>

3. Next the code indexes the LiDAR data. Indexing creates a ".lax" file for a given .las or .laz file that contains spatial indexing information. When this LAX file is present it will be used to speed up access to the relevant areas of the LAS/LAZ file for spatial queries.

<details>
<summary>Click to show the code</summary>
```{LAStools eval=FALSE, include=TRUE}
REM Indexing
lasindex -i path_to_write\02_rescaled\*.laz
```
</details>

4. Lastly tiling divides the point clouds into tiles to allow for parallel processing in the following R steps.
    - "tile_size 15" sets the size of the tiles to 15m.
    - "buffer 4 " sets the size of the buffer surrounding the tiles to 4m.
    - "flag_as_withheld " flags the buffer points so that they can be easily filtered out in the following steps in R.
    
<details>
<summary>Click to show the code</summary>
```{LAStools eval=FALSE, include=TRUE}
REM Creating 15m tiles 
lastile -i path_to_write\02_rescaled\*.laz ^
    -tile_size 15 ^
    -buffer 4 ^
    -flag_as_withheld ^
    -odir path_to_write\03_tile ^
    -olaz 
```
</details>

## Normalization and individual tree point clouds

The below workflow includes explanations and code where applicable for each of the following:

1. Creating a 10cm DTM from the registered LiDAR tiles

2. Normalizing the tiles using the DTM 

3. Creating a 4cm CHM from the max Z values in each pixel 

4. Segmenting the tiles to only retain the top 25% of each tree using the crown polygons. The threshold will be site and age dependent. We set our threshold to the top 25% given that the trees were mature with crown closure and we were not confident the lower 75% of the point cloud did not contain any invading neighboring branches. 

5. Merging segmented tiles to create one large point cloud containing the top 25% of each tree 

6. Clipping the point cloud into individual point clouds per tree

Note: The full code that can be found on the project [GitHub](index.html#github-link) runs through the above steps in a for loop. For sake of this guide we have rearranged the ordering of some steps (i.e. functions will be defined as we go in this guide, however are defined at the beginning in the full R script to allow the for loop to run). 

### Creating a DTM from the registered LiDAR tiles 

To being, when working with LiDAR data in R, an incredibly useful package is the lidR package. We highly recommend taking a look at the [lidR bookdown](https://r-lidar.github.io/lidRbook/) for useful tips and examples on using the package. 

The packages required for our workflow are:
<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
library(tidyverse)
library(sf)
library(sp)
library(spatial)
library(raster) # working with raster data
library(terra)
library(lidR) # reading and processing LiDAR data
library(sp) # defines and allows us to work with spatial objects
library(nngeo)
library(future)
library(rmapshaper)
library(concaveman)
library(parallel)
library(foreach)
library(smoothr)
library(ForestTools)
library(gdalUtilities)
library(exactextractr)
library(alphashape3d) # Creates alpha shapes used to calculate crown volume 
library(lwgeom)
library(dplyr)
```
</details>

Here we are reading in the output tiles from the [tiling and rescaling step above](index.html#Rescale-and-tile-the-registered-LiDAR). We then drop the buffer points that were flagged as withheld in the LAStools tiling stage, set the new chunk buffer to 0.5m and set the opt_output_files to empty given that we do not want to save DTMs for the individual tiles but rather one for the entire site.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
  dir <- "set_path_to_folders"
  
  TILES = readLAScatalog(folder = paste0(dir, "\\03_tile\\"), filter = "drop_withheld")
  opt_filter(TILES) <- "-drop_withheld"   # set filtering options for the LAScatalog object to drop withheld points
  opt_chunk_buffer(TILES) = .5   # set the buffer size for chunks in the LAScatalog object to 0.5m
  opt_laz_compression(TILES) = TRUE   # enable LAZ compression for the LAScatalog object
  opt_output_files(TILES) = ""   # set output file options for the LAScatalog object to empty
  opt_progress(TILES) = TRUE # enable progress tracking for processing
```
</details>

Next we create a 10cm DTM using the tin() algorithm and smooth the raster by using the mean focal statistic and a focal window of 25 by 25 cells over the DTM. As the focal window shifts over the raster, it updates the pixel that sits at its center to the mean value within the 25 x 25 cell window. We then assigned the proper CRS and exported the raster as a tif. 

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
  # Create a DTM
  DTM = grid_terrain(TILES, res = 0.1, tin(), full_raster = FALSE) %>% 
    # applying a focal operation to the DTM raster: computing the mean value within a moving window defined by a matrix.
    focal(w = matrix(1, 25, 25),  # define a 25x25 window with all values as 1
          fun = mean,             # use the mean function to compute the focal statistic
          na.rm = TRUE,           # remove NA values from computation
          pad = TRUE)             # pad the edges of the raster with NAs to maintain the original extent
    
  crs(DTM) <- CRS("+proj=utm +zone=10 +datum=NAD83") # assign a CRS to the raster using the proj4 string representation
  
  writeRaster(DTM, paste0(dir, "\\04_RASTER\\", site, "_DTM_0.1m.tif"), overwrite = TRUE) #save the DTM
```
</details>

### Normalizing the tiles using the DTM  

The R code below reads in the tiled registered LiDAR as a LAScatalog and normalizes the tiles to the DTM made in the previous step. The normalized tiles are then saved out and read back into R to filter out points below "ground" designated to be -0.25m. Here we save out the "cleaned" normalized tiles in a new folder, however for space saving reasons you can also overwrite the original normalized tiles by setting the "opt_output_files()" parameter in the filtering step to the same path as that set for the normalization step. 

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
# Read the tiles again, but with high-res parameters
  CTG = readLAScatalog(folder = paste0(dir, "\\03_tile\\"))
  opt_chunk_buffer(CTG) = .5 # small buffer; we're just normalizing
  opt_laz_compression(CTG) = TRUE
  opt_filter(CTG) = "-thin_with_voxel 0.01" # 1cm voxel thinning
  opt_output_files(CTG) = paste0(dir, "\\05_NORM\\{*}_NORM") #saving out normalized individual laz files with an extension of _NORM to the 05_NORM folder
  opt_progress(CTG) = TRUE
  
  # Normalizing the tiles in the catalog using the DTM, tiles will be saved to the location designated in the above "opt_output_files" as they are processed
  NORM = normalize_height(CTG, DTM, na.rm = TRUE)#normalizing the laz files in the CTG to the previously made DTM
  
  # Read in the normalized laz files and filter out points below -0.25 (below ground)
  NORM = readLAScatalog(paste0(dir, "\\05_NORM\\"))
  opt_chunk_buffer(NORM) = 0  # no buffer, just filtering
  opt_laz_compression(NORM) = TRUE
  opt_filter(NORM) = "-drop_z_below -.25" #drop points below -25cm
  opt_output_files(NORM) = paste0(dir, "\\06_NORM_clean\\{*}") #save the filtered normalized filed to the "06..." folder, here you can also choose to overwrite the original normalized laz files by setting the output location to the "05_norm" folder designated as the output in the normalization step
  opt_progress(NORM) = TRUE #show the progress
  
  NORM_clean = catalog_retile(NORM) # applying the filter to all files in the catalog
  
```
</details>

### Creating a 4cm CHM from the max Z values   

A 4cm CHM was made using max Z values. We chose a resolution of 4cm as it gave us a good looking (no holes, no visible noise) high resolution CHM. We recommend testing out multiple resolutions at this stage to narrow down parameters that work for your data.

<details>
<summary>Click to show the code</summary>
```{r,eval=FALSE, echo=TRUE}
# Making a 4cm resolution CHM
CHM_max = grid_metrics(NORM_clean, #NORM_clean is the catalog the CHM is being made from
                        res = 0.04, #4cm resolution
                        func = ~max(Z)) #using max Z values
  
crs(CHM_max) <- CRS("+proj=utm +zone=10 +datum=NAD83") # assign a CRS to the raster using the proj4 string representation
writeRaster(CHM_max, paste0(dir, "\\04_RASTER\\", site, "_CHM_max_0.04m.tif"), overwrite = TRUE) #saving out the CHM
  
```
</details>

### Segmenting tiles

The normalized tiles were then segmented to only retain the top 25% of each tree. This was done using a shapefile (.shp) containing a polygon for each tree crown, see the chapter on [crown delineation](#crown-delineation) for a detailed workflow showing how to create crown polygons. 

The threshold set for segmentation will be site specific. We chose to only retain the top 25% of the point cloud for each tree in our mature (~25 years old) sites with crown closure in order to:
  1. limit the chance of capturing invading branches in our LiDAR data
  2. Capture a similar scope of the trees in both the LiDAR and photogrametic data, given that the aerial imagery is limited to branches that can be seen from an aerial perspective. 

A range of thresholds, from 50% (for ~10 year old trees that were around ~8m in height) to 80% (for ~4 year old trees that were around ~1m in height), were used for the younger sites we worked with depending on the proportion of the tree crown that is visible from an aerial, oblique perspective. 

To begin, we must first define the function that will segment the trees. Here "zq" is the threshold for the height segmentation and can either be set in the polys_to_las function (if you would like it to be the same value each time the function is run) or defined as a variable prior to initiating the function (use this option if the function will be called for different sites that have different zq values). 

The polys_to_las function works by:
1. Identifying points that fall within a tree crown and labeling the points with the unique treeID for that crown
2. Labels points within the crown that fall below the defined height threshold to have a treeID of zero 
3. Filters to only keep points with a treeID value greater than zero

Important:
- If zq = 0.75, then the points in the bottom 75% of the tree will be removed, leaving a point cloud for the top 25% only.
- "treeID" is a column in our dataset that functions as a unique identifier per tree within the site, ensure you change each instance of treeID to a column containing a unique identifying for your trees. The unique identifiers can come from the experimental site setup or the [crown delineation](09-Crown_delineation#Crown-delineation) step.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
# Function that will segment the LiDAR tiles to only retain points above a defined cutoff
polys_to_las = function(chunk, zq = zq, polygons = pols) { #edit zq = zq here if you would like the function to always use the same zq value, ie zq = 0.75, otherwise leave zq = zq as you will be able to define zq in the next code chunk
  
  las = readLAS(chunk)                  
  if (lidR::is.empty(las)) {
    return(NULL) }# If the LAS file is empty, return NULL
  
  las2 = merge_spatial(las, polygons, "treeID") # Merge the LAS points with the polygons based on the treeID attribute, change the "treeID" attribute to a unique identifier for the trees you are working with
  las_df = las2@data %>%
    dplyr::group_by(treeID) %>%
    dplyr::mutate(Zq999 = quantile(Z, 0.999)) %>% # compute Zq999 (the 99.9th percentile of Z) for each tree 
    dplyr::mutate(treeID = if_else(Z > quantile(Z, 0.999) * zq, as.numeric(treeID), 0))# assign points below the top zq% of the tree height a treeID of zero to filter them out later

  las3 = las2 %>% 
    add_lasattribute(las_df$treeID, name = "treeID", desc = "treeID") %>% #add treeID las a lasattribute
    filter_poi(treeID > 0) #filter for points with a treeID greater than 0 
  
  if (lidR::is.empty(las3)) {
    return(NULL)#return NULL is the las3 is empty
  } else {
    return(las3)#return the las file that has a treeID associated with it and only contains points in the top zq%
  }
}
```
</details>

Next we read in the normalized files, set our zq threshold and run the tiles through the polys_to_las segmentation function using the catalog_apply function.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
# Read in the normalized cloud from the end of the last step
NORM = readLAScatalog(paste0(dir, "\\06_NORM_clean\\"))
crs(NORM) <- st_crs(26910) # setting CRS
opt_chunk_buffer(NORM) = 0 # no buffer
opt_laz_compression(NORM) = TRUE # compress output files to .laz objects
opt_output_files(NORM) = paste0(dir, "\\07_SEGMENTED\\{*}_SEGMENTED") # write output files to "07_SEG..." folder and name with suffix "_SEGMENTED"
opt_progress(NORM) = TRUE # show progress
  
zq = 0.75 # Height percentile to drop, here we will be filtering for the top 25% of the tree 
# New tiles have only segmented portions of tree crowns
SEGMENTED = catalog_apply(NORM, polys_to_las) # apply the poly_to_las function to the NORM catalog

```
</details>

### Merging segmented tiles to create one large point cloud containing the top defined height % of each tree

This is done to facilitate clipping out individual tree point clouds in the next step.  

**Note:**  

- The *opt_chunk_size(SEGMENTED) = 10000* sets the new chunk size to 10,000 units of the CRS, in this case the chunk size is 10km x 10km. This is done to read in all the segmented tiles at one time so they can then be merged together into one tile. This value will be site specific however a very large value of 10km x 10km should work for the vast majority of sites.

- The *catalog_retile* function merges the point clouds into one large point cloud 

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
SEGMENTED = readLAScatalog(paste0(dir, "\\07_SEGMENTED\\"))
opt_chunk_buffer(SEGMENTED) = 0 #zero buffer
opt_chunk_size(SEGMENTED) = 10000 #set the chunk size for the LAScatalog object to 10000 units of the CRS so that all laz files are merged into one large laz file
opt_laz_compression(SEGMENTED) = TRUE
opt_progress(SEGMENTED) = TRUE
opt_output_files(SEGMENTED) = paste0(dir, "\\08_MERGED\\", site, "_HULLS_merged") #write output files to "08_M..." with the suffix "_HULLS_merged"
  
# merge all the segmented trees into a single point cloud 
MERGED = catalog_retile(SEGMENTED) #apply the above opt_ commands
print("merged") #print statement to show where the code is at
```
</details>

### Clipping the point cloud into individual point clouds per tree

This step results in individual tree point clouds that represent the top #% of the tree, where # is the height percentile defined in the segmentation step above. 

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
opt_output_files(MERGED) = paste0(dir, "\\09_CROWNS\\", site, "_fam{fam}_rep{rep}_tag{tag}_treeID{treeID}") #where the individual crown point clouds will be written to and the suffix they will have. Note values associated with the laz file will replace the names in {}
CROWNS = clip_roi(MERGED, pols) #clip the MERGED laz file to individual crowns using the crown polygons 
```
</details>

Below we define the clean_crowns function that ensures that only points:
- with the correct treeID remain in each individual tree point cloud. 
- that pass the defined height percentile threshold remain in the point cloud

This function was added to double check that only the points that meet the criteria remain in the point clouds

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
clean_crowns = function(chunk) {
  las = readLAS(chunk) #reading in las file                 
  if(lidR::is.empty(las)) return(NULL) #removing empty
  
  #ensuring all points in the cloud have the same/right tree ID
  treeID_true = as.numeric(names(sort(table(las@data$treeID), decreasing = TRUE))[1])
  las2 = filter_poi(las, treeID == treeID_true)
  
  #(added to Sam's code) filter points below the 75% threshold that were not dropped previously
  las_df = las2@data %>%
    dplyr::group_by(treeID) %>%
    dplyr::mutate(Zq99 = quantile(Z, 0.99)) %>%
    dplyr::mutate(Zq999 = quantile(Z, 0.999)) %>%
    dplyr::ungroup()
  
  las3 = filter_poi(las2, Z >= quantile(Z, 0.99)*zq) #only keeping points above 99%*zq height percentile
  las4 = filter_poi(las3, Z <= quantile(Z, 0.999)) #removing points too high
}

## Cleaning crowns
CROWNS = readLAScatalog(paste0(dir, "\\09_CROWNS\\"), filter = "-drop_withheld") #read in the clipped crown point clouds
opt_chunk_size(CROWNS) = 0 # processing by files
opt_laz_compression(CROWNS) = TRUE
opt_chunk_buffer(CROWNS) = 0 # no buffer
opt_wall_to_wall(CROWNS) = TRUE # disable internal checks to ensure a valid output
opt_output_files(CROWNS) = paste0(dir, "\\10_CROWNS_clean\\{*}") #location for output files
print("cleaning crowns")
CROWNS_clean = catalog_apply(CROWNS, clean_crowns) #applying the clean_crowns function on the CROWNS
```
</details>

## Individual Tree Metrics

This script calculates the following metrics for each tree: 

- **Height percentiles**: 99, 97.5, 95, 92.5, mean height  
  - To create new height percentiles, change the “X” here:  
  - as.numeric(quantile(Z, X, na.rm = TRUE)) 

- **Volumes**: convex and two variations of concave (α = 1, α = 0.5) 
  - Volume calculations are dependent on the alpha shape created. The larger the alpha parameter used to create the alpha shape the more convex the hull shape, whereas the smaller the alpha the more concave the shape. 
  - Very small alpha values can result in capturing holes in the point cloud as it will hug tightly to the points 
  - The alpha shape for convex (lines curving outward) hull volume requires an α = inf and represents the tightest convex boundary around all points in the point cloud 
  - Concave volume often uses α = 1, however below we also calculate concave volume with α = 0.5 for a more defined shape 
  - To create a new alpha shape with a new alpha value change the “α_value” below:
    alphashape3d::ashape3d(x = a3d, alpha = “α_value, pert = TRUE,eps = 1e-09) 

- **Crown complexity**: rumple, canopy rugosity ratio (CRR), coefficient of variation of height  
  - Rumple measures the ratio of canopy surface area to ground surface area to give an idea of complexity. 
  - Canopy Rugosity Ratio (CRR) quantifies the vertical complexity of the canopy or crown. 
  - Coefficient of variation of height quantifies the variability of points in the point cloud. This metric is often used to look at structural diversity of a stand, however we added it to look at variation in point distribution within individual trees, to give an idea of variation in vegetation density between crowns. 
  
<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
Z = las@data$Z # Z values of each point in the .laz cloud
chm = grid_metrics(las, func = ~max(Z,na.rm = TRUE), res = 0.05) # 5cm CHM with max Z values #grid_metrics, replaced by pixel_metrics :https://github.com/r-lidar/lidR/releases
      # Extract X, Y, and Z coordinates from the LAS data and create a matrix 'a3d'
a3d <-  cbind(las@data$X, las@data$Y, las@data$Z)
      
# Center the points around the origin (0,0,0) by subtracting the mean of each dimension
a3d[,1] = a3d[,1] - mean(a3d[,1],na.rm = TRUE) #center x values
a3d[,2] = a3d[,2] - mean(a3d[,2],na.rm = TRUE) #center y values
a3d[,3] = a3d[,3] - mean(a3d[,3],na.rm = TRUE) #center z values # sams orginal code did not center the Z values, but that is the only way we found to not get an error thrown in the ashape3d function.
      
# Generate different shapes using the alphashape3d package
shape_convex = alphashape3d::ashape3d(x = a3d, alpha = Inf, pert = TRUE,eps = 1e-09)# Compute a convex hull using alpha = Inf (convex hull) #USED PERT= TRUE : https://cran.r-project.org/web/packages/alphashape3d/alphashape3d.pdf
shape_concave = alphashape3d::ashape3d(x = a3d, alpha = 1, pert = TRUE,eps = 1e-09)# Compute a concave hull using alpha = 1 (concave hull)
shape_a05 = alphashape3d::ashape3d(x = a3d, alpha = 0.5, pert = TRUE,eps = 1e-09)# Compute a shape for alpha = 0.5 (balanced between convex and concave)
      
structural_metrics_df <- data.frame(
  treeID = unique(las@data$treeID), 
  tag = tag_value,
  n_points = length(las@data$Z),
        
  # Crown height
  Zq99 = as.numeric(quantile(Z, 0.990,na.rm = TRUE)),# 99th percentile
  Zq975 = as.numeric(quantile(Z, 0.975,na.rm = TRUE)), # 97.5th percentile
  Zq95 = as.numeric(quantile(Z, 0.95,na.rm = TRUE)),# 95th percentile
  Zq925 = as.numeric(quantile(Z, 0.925,na.rm = TRUE)), # 92.5th percentile
  Z_mean = mean(Z,na.rm = TRUE), #mean crown height
        
  # Crown volume
  vol_convex = alphashape3d::volume_ashape3d(shape_convex), # convex volume
  vol_concave = alphashape3d::volume_ashape3d(shape_concave), # concave volume
  vol_a05= alphashape3d::volume_ashape3d(shape_a05), #volume with alpha = 0.5 (balance between concave and convex)

  # Crown complexity
  CV_Z = sd(Z,na.rm = TRUE) / mean(Z,na.rm = TRUE),# Compute the coefficient of variation (CV) of Z values, representing the variability of heights relative to the mean height
  rumple = lidR::rumple_index(chm), #rumple: ratio of canopy outer surface area to ground surface area as measured by the CHM and DTM
  CRR = (mean(Z,na.rm = TRUE) - min(Z,na.rm = TRUE)) / (max(Z,na.rm = TRUE) - min(Z,na.rm = TRUE))# Compute the Canopy Rugosity Ratio (CRR), 
  # representing the ruggedness or roughness of the canopy surface
)
      
#Create structural metrics folder
if (!dir.exists(paste0(dir,"\\10_CROWNS_clean\\Structural_metrics\\"))) {
      dir.create(paste0(dir,"\\10_CROWNS_clean\\Structural_metrics\\"), recursive = TRUE)
}

#writing out .rds files
saveRDS(structural_metrics_df, paste0(dir,"\\10_CROWNS_clean\\Structural_metrics\\",name,"_", date,"_structuralMetrics_tag",tag_value,".rds")) #USED PERT= TRUE : https://cran.r-project.org/web/packages/alphashape3d/alphashape3d.pdf
    
```
</details>

Below are a individual tree point cloud (left), clipped to the top 25% of the tree and its corresponding alphashape used for volume (right).

<!-- ```{r echo=FALSE, results='asis'} -->
<!-- # Define the image paths -->
<!-- img_paths <- c( -->
<!--   "C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\Individual_tree_point_cloud_resize_bookdown.png",  -->
<!--                           "C:\\Users\\owaite\\OneDrive - NRCan RNCan\\Documents\\GitHub\\GenomeBC_BPG\\Photos_&_gifs\\Individual_tree_volume_resized_bookdown.gif" -->
<!-- ) -->

<!-- # Create the HTML for the images -->
<!-- html <- paste0( -->
<!--   '<div style="display: flex; flex-direction: row; justify-content: center; align-items: center;">', -->
<!--   paste0('<img src="', img_paths, '" style="height: 300px; margin: 0 10px;" />', collapse = ""), -->
<!--   '</div>' -->
<!-- ) -->

<!-- # Print the HTML -->
<!-- cat(html) -->
<!-- ``` -->

  
```{r indv-tree-metrics, echo=FALSE,  fig.show="hold",fig.align='center', out.width="50%", fig.cap = "Left:      . Right: ."}
knitr::include_graphics(c(here("Photos_&_gifs\\Individual_tree_point_cloud_resize_bookdown.png"),
                          here("Photos_&_gifs\\Individual_tree_volume_resized_bookdown.gif")))

```
.

<!--chapter:end:0_LiDAR_processing.Rmd-->

# MicaSense Irradiance Correction {#MicaSense-Irradiance-Correction}

## Background

The MicaSense system measures radiance at the camera, which needs to be transformed into surface reflectance. The basic relationship is given by:

$$
R = \frac{L}{I_{\text{hor}}}
$$

where $L$ is the at-camera radiance and $I_{\text{hor}}$ is the horizontal irradiance.

### Deriving Horizontal Irradiance

Below we go through the derivation to compute $I_{\text{hor}}$.

The MicaSense system includes both the cameras and a downwelling light sensor (DLS). Since the DLS is tilted at the angle of the drone during flight and the sun is rarely directly overheard the sensor, the DLS measures non-horizontal irradiance, referred to as spectral irradiance ($I_{\text{spec}}$). Spectral irradiance is a function of direct irradiance ($I_{\text{direct}}$), scattered irradiance ($I_{\text{scattered}}$), and the angle between the sun and the sensor ( $\theta_{\text{sun-sensor}}$):

$$
I_{\text{spec}} = I_{\text{direct}} \cdot \cos(\theta_{\text{sun-sensor}}) + I_{\text{scattered}} 
$$

If we take $r$ to be the ratio of scattered to direct irradiance, $r = \frac{I_{\text{scattered}}}{I_{\text{direct}}}$, we can substitute $I_{\text{scattered}} = r \cdot I_{\text{direct}}$ into the equation:

$$
I_{\text{spec}} = I_{\text{direct}} \cdot \cos(\theta_{\text{sun-sensor}}) + r \cdot I_{\text{direct}} 
$$

Next, by factoring out $I_{\text{direct}}$ from the equation:

$$
I_{\text{spec}} = I_{\text{direct}} \cdot \left( \cos(\theta_{\text{sun-sensor}}) + r \right) 
$$

We can solve for $I_{\text{direct}}$ by dividing both sides of the equation by $\cos(\theta_{\text{sun-sensor}}) + r$:

$$
I_{\text{direct}} = \frac{I_{\text{spec}}}{\cos(\theta_{\text{sun-sensor}}) + r}
$$

If we assume that $I_{\text{hor}}$ can be expressed in terms of $I_{\text{direct}}$ with a similar function as seen above for $I_{\text{spec}}$ however using the zenith angle, we can express this as:

$$
I_{\text{hor}} = I_{\text{direct}} \cdot \left( \cos(\theta_{\text{zenith}}) + r \right)
$$

Substituting the equation for $I_{\text{direct}}$ into the equation for $I_{\text{hor}}$ gives:

$$
I_{\text{hor}} = \frac{I_{\text{spec}}}{\cos(\theta_{\text{sun-sensor}}) + r} \cdot \left( \cos(\theta_{\text{zenith}}) + r \right)
$$

### Calculating Reflectance

Finally, substituting $I_{\text{hor}}$ into the equation for reflectance, we obtain:

$$
R = L \cdot \frac{\cos(\theta_{\text{sun-sensor}}) + r}{I_{\text{spec}} \cdot \left( \cos(\theta_{\text{zenith}}) + r \right)}
$$

Thus we see that reflectance is highly dependent on the ratio of scattered to direct irradiance as well as the sun-sensor angle. This is important for understanding the issue we have found when working with the DLS2 and the MicaSense MX Dual camera system.

## The Problem

The issue we have noticed is that the sun-sensor angle can be absurdly high or low, especially in strong illumination conditions. This leads to equally absurd, sometimes impossible, values for direct and scattered irradiance, that can be multiples higher than the direct solar irradiance.

To begin, let us first look at some main differences between the legacy DLS1 and the current DLS2 system.

The DLS1:

-   calculates the sun zenith angle using the GPS location of the drone and the time of acquisition

-   calculates the sun-sensor angle from the yaw/pitch/roll of the drone

The DLS2:

-   directly measures the sun sensor angle and the direct and diffuse irradiance components using a proprietary method that takes in information from the light sensors on the surface of the DSL2

## Is Correction Needed? {#is-correction-needed}

Deciding whether data needs to be corrected can be challenging. We have found that looking at these three questions can be helpful:

*1) Do the sun sensor angles make sense?*

*2) Does the relationship between direct and horizontal irradiance make sense?*

*3) Does the scattered: direct ratio look right for the lighting conditions of that day?*

Below we have plotted the sun sensor angle (top left), the position of each photo (bottom left) and the irradiance values (right) from the exif data of MicaSense imagery captured on June 25th, 2024 on Vancouver Island in British Columbia, Canada (Figure \@ref(fig:Irradiance-flight-lines)). We used yaw values to identify flight lines (blue and green) and roll to filter for images taken when the drone is turning (red). The black horizontal line is the calculated solar zenith angle at the time of the flight. This value was calculated using the latitude and longitude of the imagery and the time of day and was around 30°.

<!-- ```{r Irradiance-flight-lines, echo=FALSE, out.width="100%", fig.align = 'center',fig.cap= " "} -->

<!-- knitr::include_graphics(here("Photos_&_gifs\\labelled_time_Irradiance_flight_lines_with_turning_all_with_spatial_plot_CROPPED.PNG")) -->

<!-- ``` -->

```{r Irradiance-flight-lines, echo=FALSE, out.width="100%", fig.align = 'center',fig.cap= "The sun sensor angle (top left), position of each photo (bottom left) and direct, scattered, horizontal, and spectral irradiance values (right) taken from the exif data of MicaSense images captured on June 25th, 2024 on Vancouver Island, BC, Canada. Flight line direction (blue and green) was identified using yaw values  and images taken while the drone was turning were identified using roll (red)."}
knitr::include_graphics(here("Photos_&_gifs\\time_Irradiance_flight_lines_with_turning_all_with_spatial_plot_CROPPED.PNG"))
```

### Do the sun sensor angles make sense?

Solar zenith angle is the angle between the normal (vertical from the Earths surface) at the location of interest and the position of the sun. Given the tilt of the Earth, the solar zenith reaches its maximum (i.e. greatest angle between the sun and the normal to the Earths surface) in the winter, as the northern hemisphere is tilted away from the sun, and its minimum in the summer when the northern hemisphere is tilted towards the sun.

The solar zenith also changes throughout the day, with its minimum occurring at solar noon, a.k.a. when the sun is at its highest. This is why it is suggested to fly within ± 2 hours of solar noon to avoid shadows in your imagery.

Below we have graphed solar zenith angles over the winter solstice (December 24, 2024), summer solstice (June 21, 2024) and the date of the flight acquisition (June 25th, 2024) to demonstrate the daily and seasonal change in values (Figure \@ref(fig:zenith-overview)).

The gray vertical bar highlights the flight time, which was just within the ± 2 hours of solar noon bounds and shows the solar zenith at around 30°, as we saw in Figure \@ref(fig:Irradiance-flight-lines).

```{r zenith-overview, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap = "Solar zenith values plotted over the winter solstice (December 24, 2024), summer solstice (June 21, 2024) and the date of the flight acquisition (June 25th, 2024). Vertical lines indicate solar noon for the winter solstice (blue), summer solstice (red), and the duration of the drone flight (grey)."}
knitr::include_graphics(here("Photos_&_gifs\\Solar_zenith_solstice_flight.PNG"))
```

Now that we've refreshed our understanding of the solar zenith, let's dive into the sun sensor angle. The sun sensor angle is, as it sounds, the angle between the normal (perpendicular) of the sensor and the direction of the sun (Figure \@ref(fig:SSA-diagram)). Given that drone flights occur relatively close to the ground compared to the distance from the Earth to the sun, this difference is often negligible. Therefore, the solar zenith angle can serve as a rough estimate of the sun sensor angle for a sensor that is parallel to the Earth's surface - making it a helpful check to see if the sun sensor angle recorded by the DLS2 is within a reasonable range.

```{r SSA-diagram, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap="Visual representation of the solar zenith angle (left) and the sun sensor angle (right)." }
knitr::include_graphics(here("Photos_&_gifs\\SSA_verse_solarZenith_diagram.PNG"))
```

However, drones often fly at an angle, which can cause a cyclical pattern in sun sensor angles depending on flight direction. For example, let's take a look at Figure \@ref(fig:SSA-flight-direction). When the drone is flying in the general direction of the sun (left), the forward tilt of the drone mid-flight results in a smaller sun sensor angle compared to when the drone is flying away from the general direction of the sun, where the tilt of the drone away from the sun increases the sun sensor angle (right).

```{r SSA-flight-direction, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap = "Schematic illustrating the impact of drone tilt during flight on the sun sensor angle." }
knitr::include_graphics(here("Photos_&_gifs\\SSA_flight_direction_diagram.PNG"))
```

This pattern of sun sensor angle dependence on drone orientation relative to the sun can be seen for the June 25th flight. Figure \@ref(fig:SSA-DSL2-data) shows the uncorrected sun sensor angles, derived from the DLS2 sensor located on the top of the drone, colored by flight line direction. Here red indicates turning and blue and green are opposite flight directions.

```{r SSA-DSL2-data, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap="Sun sensor angles aquired from the DLS2 sensor colored by flight line direction."}
knitr::include_graphics(here("Photos_&_gifs\\time_SSA_flightlines.PNG"))
```

Now that we understand the reason behind the cyclical pattern of the sun sensor angles, we can use the solar zenith value, plotted as the horizontal black line in Figure \@ref(fig:SSA-DSL2-data), to make an informed decision on whether or not the values need correcting.

To help this decision we recommend plotting the DLS2 sun sensor values and the sun sensor values calculated with the DLS1 method which will be referred to as the "corrected" values, see the section on [sun sensor anlge correction](#SSA-correction) for the correction method.

```{r SSA-DSL2-DLS1-data, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap=" Sun sensor angles over the flight duration from the DLS2 (purple) and corrected to mimic the DLS1 method (orange). Horizontal lines indicate the moving averages for the DLS2 (purple) and DLS1 (orange), where the balack line is the solar zenith calculated from GPS longitude, latitude, and time."}
knitr::include_graphics(here("Photos_&_gifs\\SSA_DLS1_verse_DLS2.PNG"))
```

In Figure \@ref(fig:SSA-DSL2-DLS1-data) the dashed horizontal lines are the moving averages of the sun sensor angles for the DLS1 and DLS2. Here the moving average of the sun sensor angles from the DLS2 is at times almost 10°lower than the solar zenith, whereas the moving average of the DLS1 (corrected method) is more or less consistently \~2-3° lower than the solar zenith.

Given this data, we would conclude that the DLS1 method provides more accurate sun sensor angles compared to the DLS2. We will now continue to look at irradiance to confirm if correction is needed.

### Does the relationship between direct and horizontal irradiance make sense?

First, lets define direct and horizontal irradiance with respect to the DLS:

**Direct Irradiance**: The direct component of the sunlight reaching the sensor's surface that is not being scattered. On a clear sunny day, this component is high.

**Scattered Irradiance**: The sunlight that is scattered by particles in the atmosphere. This component is generally weaker on sunny days and higher on overcast days.

**Horiztonal Irradiance**: The total irradiance on a horizontal surface, including both direct sunlight (projected onto the horizontal plane) and diffuse light. Hence, if the sun is directly above the sensor and it is a clear sunny day, the direct irradiance component will likely be quite close to the horizontal irradiance, however still lower given that horizontal irradiance includes diffuse light as well.

Below we have plotted the direct and horizontal irradiance from the DLS2 and values calculated with the DLS1 method (Figure \@ref(fig:Irr-DSL2-DLS1-data) ). We can see that the direct irradiance from the DLS2 is greater than the horizontal irradiance, which is physically impossible. In contrast, the direct irradiance from the DLS1 method is approximately 50 W/m² less than the horizontal irradiance.

Explanations for the lower direct irradiance from the DLS1 compared to the horizontal irradiance include:

-   The \~30° sun sensor angle, which reduces the effective direct component captured by the sensor.

-   The portion of scattered irradiance in the horizontal irradiance signal that is not included in the direct irradiance value.

```{r Irr-DSL2-DLS1-data, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap= "Direct and horizontal irradiance values throughout the flight from the DLS2 (purple) and corrected DLS1 method (orange)."}
knitr::include_graphics(here("Photos_&_gifs\\Hor_dir_irradiance_DLS1_v_DLS2.PNG"))
```

From Figure \@ref(fig:Irr-DSL2-DLS1-data), we would recommend correcting the data using the DLS1 method outlined below

### Does the scattered: direct ratio make sense for the lighting conditions of that day?

For a clear sunny day around solar noon, direct radiation generally accounts for \~85% of the total insolation with scattered radiation accounting for the remaining \~15%. For a completely overcast day, scattered radiation contributes to 100% of solar radiation.<!-- https://www.ftexploring.com/solar-energy/direct-and-diffuse-radiation.htm -->

Hence, for a clear sunny day in the summer a ratio of 1:6 scattered to direct irradiance is often used as an estimate. This means that direct irradiance is around 6 times stronger than the scattered irradiance.

Irradiance data plotted in Figure \@ref(fig:Dir-Irr-DSL2-DLS1-data) from June 25th was captured on a sunny day with occasional light clouds.

```{r Dir-Irr-DSL2-DLS1-data, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap = "Direct and scattered irradiance values throughout the duration of the flight from the DLS2 (purple) and corrected DLS1 method (orange)."}
knitr::include_graphics(here("Photos_&_gifs\\Dir_Scat_irradiance_DLS1_v_DLS2.PNG"))
```

The scattered: direct ratio for the data plotted in Figure \@ref(fig:Dir-Irr-DSL2-DLS1-data) is \~0.09 for the DLS2 and \~0.46 for the DLS1.

**DLS2:** The ratio of 0.09 scattered:direct components from the DSL2 states that the direct irradiance during the flight was \~11 times greater than the scattered irradiance, or in other words that 91% of the total radiation reaching the earth was direct radiation and 9% was scattered radiation.

**DLS1:** The ratio of 0.46 scattered:direct components from the DLS1 states that the direct component was 2.17 times greater than the scattered component, hence 46% of the solar radiation was from the scattered component and the remaining 54% from the direct component.

Given that the flight on June 25th was flown just at the end of the 2 hour solar noon window under a sunny sky with the occasional cloud, we would assume that the scattered component would be greater than the minimum \~15% for fully clear skies at solar noon. Hence a value of 46% scattered radiation could be the result of extra scattering by the occasional clouds and a slight increase in scattering from a greater solar zenith angle. Overall the value of 0.46 is more logical than the value of 0.09, since 0.09 implies less scattered radiation than the generally accepted minimum scattered component percentage on a clear sunny day.

Overall, we recommend correcting this flight using the DLS1 method. The DLS1 corrections are done on individual photos, if the orthomosaic has already been built, you can re-upload the imagery, re-calibrate, and re-build the orthomosaic in metashape without needing to reprocess the dense cloud.

## Correcting Irradiance Values for MicaSense Cameras {#DLS1-correction}

This chapter explains the process of correcting irradiance values for MicaSense cameras using R. The correction involves several steps, including filtering and mutating data, calculating solar angles, and estimating scattered to direct light ratios. Each step is crucial for ensuring accurate irradiance readings, which are essential for downstream analyses. The code was taken largely from [MicaSense's GitHub](https://github.com/micasense/imageprocessing/blob/master/MicaSense%20Image%20Processing%20Tutorial%203.ipynb) and converted into R to use in our workflow.

To fix the sun-sensor angle and irradiance values we have taken the methodology used by the DLS1. The process involves:

-   Using the GPS location and time of acquisition to determine the solar zenith angle, and the yaw/pitch/roll of the drone to determine the sun sensor angle

-   Generating a rolling regression of the relationship: $I_{\text{spec}} = I_{\text{direct}} \cdot \cos(\theta_{\text{sun-sensor}}) + I_{\text{scattered}}$ to determine the direct/ scattered irradiance ratio, $r$, and only keeping realistic models with good fits

-   Computing the horizontal irradiance using $I_{\text{hor}} = I_{\text{direct}} \cdot (cos(\theta_{\text{zenith}}) + r)$

-   Editing the exif data of the images so that the imagery can be processed by Metashape with the corrected irradiance values

### Reading Exif Data

First, we read in the exif data of the imagery we want to correct using the exifr package.

<details>
<summary>Click to show the code</summary>

```r
# This code was written to be within a for loop that looped through many acquisition dates. Hence the path names used will be in the form " paste0(dir, date,"\\1_Data\\",MS_folder_name,"\\"...etc)". Change these path names to match your data
library(exifr)
dir <- "directory to folder holding folders representing each flight aquisition date" #change to match your data
MS_folder_name <- "MicaSense" #change to match the name of your folder holding the folders of imagery from the MicaSense camera
band_length <-  10 # 10 bands for MicaSense RedEdge-MX Dual
#Loops through bands
 for (j in 1:band_length){ # bands 1 through 10 
   pics = list.files(paste0(dir, date,"\\1_Data\\",MS_folder_name,"\\"), #path the original data that will NOT be written over
                     pattern = c(paste0("IMG_...._", j, ".tif"), paste0("IMG_...._", j, "_", ".tif")), recursive = TRUE, 
                     full.names = TRUE) # list of directories to all images
        
  if(!dir.exists(paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\"))){ #creating CSV folders
      dir.create(paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\"))}
        
  mask_images <- list.files(paste0(dir,date,"\\2_Inputs\\metashape\\MASKS\\",MS_folder_name,"\\")) #path to folder with masks that are exported from metashape 
  mask_img_names <- gsub("_mask\\.png$", "", mask_images) # removes the suffix "_mask.png" from each element in the 'mask_images' list, ie : IMG_0027_1_mask.png becomes : IMG_0027_1 
  print(paste0("example of mask name: ",mask_img_names[[1]])) #print to ensure your naming is correct, the list of mask_img_names will be used to identify panel images in the micasense image 
  # Loops through images in each band
  for (i in 1:length(pics)){ #for each image in MicaSense_Cleaned_save, one at a time
    pic = pics[i]
    pic_root <- gsub(".*/(IMG_.*)(\\.tif)$", "\\1", pic)# The pattern captures the filename starting with "IMG_" and ending with ".tif".Ie: IMG_0027_1.tiff becomes IMG_0027_1
    print(pic_root)
    if (substr(pic_root,11,11) == "0"){ # Distinguishes between _1 (band 1) and _10 (band 10), ie IMG_0027_1 verse IMG_0027_10
        band = substr(pic_root,10,11) # for _10
        } else {
          band = substr(pic_root,10,10) # for _1, _2, _3, ... _9 (bands 1-9)
          } 
    img_exif = exifr::read_exif(pic) # read the XMP data 
    print(paste0(date, " band ", j, " ", i, "/", length(pics))) # keep track of progress
    
    # Creating df with same column names as exif data
    if (i == 1){ # If it's the first image, make new df from XMP data 
      exif_df = as.data.frame(img_exif)%>%
        mutate(panel_flag = ifelse( gsub(".*/(IMG_.*)(\\.tif)$", "\\1", SourceFile) %in% mask_img_names, 1, 0)) # Give a value of 1 if the pic root name is in the list of mask root names and is therefore a calibration panel, give 0 otherwise (ie not a panel) 
      } else {   # Add each new image exif to dataframe
        exif_df = merge(exif_df, img_exif, by = intersect(names(exif_df), names(img_exif)), all = TRUE)%>%
          mutate(panel_flag = ifelse( gsub(".*/(IMG_.*)(\\.tif)$", "\\1", SourceFile) %in% mask_img_names, 1, 0)) # Give a value of 1 if the pic root name is in the list of mask root names and is therefore a calibration panel, give 0 otherwise (ie not a panel) 
        }
    }
  saveRDS(exif_df, paste0(dir,date, "\\1_Data\\",MS_folder_name,"\\CSV\\XMP_data_", date, "_", j, ".rds")) # save output for each band, set path
  #binding all bands into one dataframe
  if (j == 1){
    df_full = exif_df
    }else{
      # Add missing columns to xmp_all and fill with NA values
      df_full= rbind(df_full, exif_df)
    }
  saveRDS(df_full, paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\XMP_data_", date,"_AllBands.rds")) #.rds file containing the original exif data if all images for the flight
 } 
```

</details>

<!-- ```{r eval=FALSE, include=TRUE} -->
<!-- # This code was written to be within a for loop that looped through many acquisition dates. Hence the path names used will be in the form " paste0(dir, date,"\\1_Data\\",MS_folder_name,"\\"...etc)". Change these path names to match your data -->
<!-- library(exifr) -->
<!-- dir <- "directory to folder holding folders representing each flight aquisition date" #change to match your data -->
<!-- MS_folder_name <- "MicaSense" #change to match the name of your folder holding the folders of imagery from the MicaSense camera -->
<!-- band_length <-  10 # 10 bands for MicaSense RedEdge-MX Dual -->
<!-- #Loops through bands -->
<!--  for (j in 1:band_length){ # bands 1 through 10  -->
<!--    pics = list.files(paste0(dir, date,"\\1_Data\\",MS_folder_name,"\\"), #path the original data that will NOT be written over -->
<!--                      pattern = c(paste0("IMG_...._", j, ".tif"), paste0("IMG_...._", j, "_", ".tif")), recursive = TRUE,  -->
<!--                      full.names = TRUE) # list of directories to all images -->

<!--   if(!dir.exists(paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\"))){ #creating CSV folders -->
<!--       dir.create(paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\"))} -->

<!--   mask_images <- list.files(paste0(dir,date,"\\2_Inputs\\metashape\\MASKS\\",MS_folder_name,"\\")) #path to folder with masks that are exported from metashape  -->
<!--   mask_img_names <- gsub("_mask\\.png$", "", mask_images) # removes the suffix "_mask.png" from each element in the 'mask_images' list, ie : IMG_0027_1_mask.png becomes : IMG_0027_1  -->
<!--   print(paste0("example of mask name: ",mask_img_names[[1]])) #print to ensure your naming is correct, the list of mask_img_names will be used to identify panel images in the micasense image  -->
<!--   # Loops through images in each band -->
<!--   for (i in 1:length(pics)){ #for each image in MicaSense_Cleaned_save, one at a time -->
<!--     pic = pics[i] -->
<!--     pic_root <- gsub(".*/(IMG_.*)(\\.tif)$", "\\1", pic)# The pattern captures the filename starting with "IMG_" and ending with ".tif".Ie: IMG_0027_1.tiff becomes IMG_0027_1 -->
<!--     print(pic_root) -->
<!--     if (substr(pic_root,11,11) == "0"){ # Distinguishes between _1 (band 1) and _10 (band 10), ie IMG_0027_1 verse IMG_0027_10 -->
<!--         band = substr(pic_root,10,11) # for _10 -->
<!--         } else { -->
<!--           band = substr(pic_root,10,10) # for _1, _2, _3, ... _9 (bands 1-9) -->
<!--           }  -->
<!--     img_exif = exifr::read_exif(pic) # read the XMP data  -->
<!--     print(paste0(date, " band ", j, " ", i, "/", length(pics))) # keep track of progress -->

<!--     # Creating df with same column names as exif data -->
<!--     if (i == 1){ # If it's the first image, make new df from XMP data  -->
<!--       exif_df = as.data.frame(img_exif)%>% -->
<!--         mutate(panel_flag = ifelse( gsub(".*/(IMG_.*)(\\.tif)$", "\\1", SourceFile) %in% mask_img_names, 1, 0)) # Give a value of 1 if the pic root name is in the list of mask root names and is therefore a calibration panel, give 0 otherwise (ie not a panel)  -->
<!--       } else {   # Add each new image exif to dataframe -->
<!--         exif_df = merge(exif_df, img_exif, by = intersect(names(exif_df), names(img_exif)), all = TRUE)%>% -->
<!--           mutate(panel_flag = ifelse( gsub(".*/(IMG_.*)(\\.tif)$", "\\1", SourceFile) %in% mask_img_names, 1, 0)) # Give a value of 1 if the pic root name is in the list of mask root names and is therefore a calibration panel, give 0 otherwise (ie not a panel)  -->
<!--         } -->
<!--     } -->
<!--   saveRDS(exif_df, paste0(dir,date, "\\1_Data\\",MS_folder_name,"\\CSV\\XMP_data_", date, "_", j, ".rds")) # save output for each band, set path -->
<!--   #binding all bands into one dataframe -->
<!--   if (j == 1){ -->
<!--     df_full = exif_df -->
<!--     }else{ -->
<!--       # Add missing columns to xmp_all and fill with NA values -->
<!--       df_full= rbind(df_full, exif_df) -->
<!--     } -->
<!--   saveRDS(df_full, paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\XMP_data_", date,"_AllBands.rds")) #.rds file containing the original exif data if all images for the flight -->
<!--  }  -->
<!-- ``` -->

Next, we read in the exif data extracted above for all bands.

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
xmp_all <- readRDS(paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\XMP_data_", date,"_AllBands.rds"))
```
</details>

### Calculating Solar Zenith Anlge

Below we convert the DateTimeOrginal column from the exif data to a date object and use the date object and average lat and lon to calculate solar angle.

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}

xmp_all_filtered <- xmp_all %>%
  mutate(Date_time = ymd_hms(DateTimeOriginal),
         BandName_Wavelength = paste0(BandName, "_", CentralWavelength),
         Date = as.Date(Date_time),
         Time = format(Date_time, format = "%H:%M:%S"),
         img_name = str_split(FileName, "\\.tif", simplify = TRUE)[, 1],
         img_root = sub("_(\\d+)$", "", img_name)) %>%
  drop_na(DateTimeOriginal) %>% 
  dplyr::mutate(
    site_avg_lat = median(GPSLatitude, na.rm = TRUE),
    site_avg_long = median(GPSLongitude, na.rm = TRUE),
    solar_angle = photobiology::sun_zenith_angle(time = ymd_hms(DateTimeOriginal),
                                                 geocode = tibble::tibble(lon = unique(site_avg_long),
                                                                          lat = unique(site_avg_lat),
                                                                          address = "Greenwich")))
```
</details>

Next, we check for any missing timestamps in image metadata as this usually indicates that the image was not saved properly and can't be opened which will throw errors later in the workflow. In our experience, these images are few and far between, so we remove them from the analysis. We recommend manually checking that the image is in fact corrupted before removing it from analysis.

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
missing_imgs <- xmp_all[is.na(as.Date(xmp_all$CreateDate, format = "%Y:%m:%d"))]$FileName
missing_imgs_roots <- sub("_[^_]*$", "", missing_imgs)

xmp_all_filtered_mis <- xmp_all_filtered %>% #removing images with missing date information
  filter(!FileName %in% c(missing_imgs))

```
</details>

<!-- ````{=html} -->
<!-- <!-- -->
<!-- ### Caluclating DNI -->

<!-- The solar position and irradiance values are necessary to understand whether or not DLS acquired values are reasonable. We calculate the direct normal irradaiance using the average latitude and longitude from the GPS coordinates of the imagery and the time of the acquisition.  -->

<!-- ```{r eval=FALSE, include=TRUE} -->
<!-- (lat <- unique(xmp_all_filtered$site_avg_lat)) -->
<!-- (lon <- unique(xmp_all_filtered$site_avg_long)) -->
<!-- (time <- mean(xmp_all_filtered$Date_time, na.rm = TRUE)) -->

<!-- sun_pos <- getSunlightPosition(date = time, lat = lat, lon = lon) -->
<!-- elevation_angle <- sun_pos$altitude -->
<!-- turbidity <- 2.5 -->
<!-- solar_constant <- 1367 -->

<!-- air_mass <- 1 / cos(pi/2 - elevation_angle) -->
<!-- dni <- solar_constant * exp(-0.2 * air_mass * turbidity) -->

<!-- print(dni) -->
<!-- ``` -->

### Caluclating Sun-Sensor Angles {#SSA-correction}

Here we show the difference between the archived DLS1 method and the current DLS2 method.

*DSL1:* The following code has been taken from [MicaSense's GitHub](https://github.com/micasense/imageprocessing/blob/master/MicaSense%20Image%20Processing%20Tutorial%203.ipynb) and coded in R to match our workflow

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
compute_sun_angle <- function(SolarElevation, SolarAzimuth, Roll, Pitch, Yaw) {
    ori <- c(0, 0, -1)
    SolarElevation <- as.numeric(SolarElevation)
    SolarAzimuth <- as.numeric(SolarAzimuth)
    Roll <- as.numeric(Roll)
    Pitch <- as.numeric(Pitch)
    Yaw <- as.numeric(Yaw)
  
    elements <- c(cos(SolarAzimuth) * cos(SolarElevation),
                  sin(SolarAzimuth) * cos(SolarElevation),
                  -sin(SolarElevation))
  
    nSun <- t(matrix(elements, ncol = 3))
  
    c1 <- cos(-Yaw)
    s1 <- sin(-Yaw)
    c2 <- cos(-Pitch)
    s2 <- sin(-Pitch)
    c3 <- cos(-Roll)
    s3 <- sin(-Roll)
  
    Ryaw <- matrix(c(c1, s1, 0, -s1, c1, 0, 0, 0, 1), ncol = 3, byrow = TRUE)
    Rpitch <- matrix(c(c2, 0, -s2, 0, 1, 0, s2, 0, c2), ncol = 3, byrow = TRUE)
    Rroll <- matrix(c(1, 0, 0, 0, c3, s3, 0, -s3, c3), ncol = 3, byrow = TRUE)
  
    R_sensor <- Ryaw %*% Rpitch %*% Rroll
    nSensor <- R_sensor %*% ori
  
    angle <- acos(sum(nSun * nSensor))
    return(angle)
}

SSA_xmp_all_filtered <- xmp_all_filtered %>%
  rowwise() %>% 
  mutate(SunSensorAngle_DLS1_rad = compute_sun_angle(SolarElevation, SolarAzimuth, Roll, Pitch, Yaw),
         SunSensorAngle_DLS1_deg = SunSensorAngle_DLS1_rad * 180 / pi)

saveRDS(SSA_xmp_all_filtered,paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\XMP_", date,"_with_SSA.rds")) #Save XMP rds file with sun sensor angle added

```
</details>


*DLS2:*

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
SSA_xmp_all_filtered$SunSensorAngle_DLS2_rad <- sapply(xmp_all_filtered$EstimatedDirectLightVector, function(vec) acos(-1 * as.numeric(vec[[3]])))
SSA_xmp_all_filtered$SunSensorAngle_DLS2_deg <- as.numeric(SSA_xmp_all_filtered$SunSensorAngle_DLS2_rad) / pi * 180
```
</details>

Check that the sun-sensor angles are within a reasonable range. They should range from \~30 degrees mid summer to \~80 degrees mid winter. This code outputs Figure \@ref(fig:Dir-Irr-DSL2-DLS1-data) facet wrapped for each of the 10 bands (Figure \@ref(fig:SSA-bands)). This is a good visual to compare DLS2 to DLS1 sun sensor angles as well as to check that data is consistent between bands and there is no missing information.


<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
xmp_all_ssa = SSA_xmp_all_filtered %>%
  # Converting from radians to degrees
  mutate(Yaw_deg = as.numeric(Yaw)*180/pi,
         Roll_deg = as.numeric(Roll)*180/pi,
         Pitch_deg = as.numeric(Pitch)*180/pi) %>% 
  # Grouping images by Date and band
  group_by(Date, BandName) %>% 
  arrange(ymd_hms(DateTimeOriginal)) %>% # Converting DateTimeOriginal to a Date and Time object and arranging in order 
  mutate(GPSLatitude_plot = scale(as.numeric(GPSLatitude)), # These are for clean ggplotting, no other reason to scale
         GPSLongitude_plot = scale(as.numeric(GPSLongitude)),
         cos_SSA = cos(SunSensorAngle_DLS1_rad),
         Irradiance = as.numeric(Irradiance),
         Date2 = ymd_hms(DateTimeOriginal)) # this is the date/time we will use moving forward 

(SSA_plot <- xmp_all_ssa %>%
    ggplot(aes(Date_time)) +
    geom_line(aes(y = SunSensorAngle_DLS2_deg, color = "DLS2"), linewidth = 1) +
    geom_line(aes(y = SunSensorAngle_DLS1_deg, color = "DLS1"), linewidth = 1) +
    geom_vline(data = subset(xmp_all_ssa, panel_flag == 1), aes(xintercept = as.numeric(Date_time)), color = "grey", alpha = 0.3) +
    geom_line(aes(y = solar_angle), color = "black", linewidth = 1, alpha = 1) +
    geom_smooth(
      aes(y = SunSensorAngle_DLS2_deg),
      method = "loess",
      se = FALSE,
      color = "blue",
      linetype = "dashed",
      size = 1
    ) +
    # Moving average for DLS1
    geom_smooth(
      aes(y = SunSensorAngle_DLS1_deg),
      method = "loess",
      se = FALSE,
      color = "red",
      linetype = "dashed",
      size = 1
    ) +
    scale_x_datetime(date_breaks = "10 min", date_labels = "%H:%M") +
    labs(x = "Time", y = "Sun Sensor Angle", title = paste0("SSA, Site: ",site_to_plot,", Date: ", date_to_plot, ", Camera: ", camera_to_plot))+
    labs(subtitle = "Grey vertical lines are calibration panel images, black hoirzontal line is the solar angle (calculated from lat, long, and flight time)", size = 10) + 
     scale_color_manual(values = c("DLS2" = "blue",
                                  "DLS1" = "red"),
                       labels = c("DLS2" = "DLS2 (from metadata)", 
                                  "DLS1" = "DLS1 (calculated)"),
                       name = "Sun Sensor Angle Comparison")+
    facet_wrap(. ~BandName_Wavelength, 
               scales = "free_x", ncol = 2)+ # this will plot each band as its own plot as a check for all data
    theme_bw()
)

```
</details>

<!-- ````{=html} -->
<!-- <!-- -->
<!-- ```{r eval=FALSE, include=TRUE} -->
<!-- ggsave(plot = SSA_plot, #save out the plot for reference -->
<!--        filename = paste0(plot_dir, "0_SSA_plot.jpeg"), #change plot_dir to the location you would like to save to -->
<!--        device = jpeg, -->
<!--        width = 6, -->
<!--        height = 6, -->
<!--        units = 'in', -->
<!--        dpi = 300, -->
<!--        bg = 'white') -->
<!-- ``` -->
<!-- --> 
<!-- ```` -->

```{r SSA-bands, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap= "Sun sensor angles across the duration of the flight for each of the 10 MicaSense bands from image metadata written by the DLS2 compared to sun sensor angles corrected using the DLS1 method. The black horiztonal lines plotted are the calculated solar zenith throughout the flight, where as the dotted lines are the moving average of sun sensor angles from the DLS1 (red) and DLS2 (blue). Grey vertical lines highlight calibration imagery taken when the drone was on the ground."}
knitr::include_graphics(here("Photos_&_gifs\\0_SSA_plot_updated.jpeg"))
```

### Scattered:Direct Ratio Estimate

Here we estimate scattered:direct ratio for each flight by relating cosine of the sun-sensor angle to spectral irradiance measured by the DLS2. This relationship, in a perfect world, should give you the scattered irradiance as the intercept, which is independent of angle, and the direct irradiance, which is the slope, and therefore perfectly proportional to sun angle. In reality, these relationships are extremely messy and most data needs to be discarded.

Below are three main steps to find the estimated scattered:direct ratio:

1)  First, generate a rolling regression of this linear relationship over a certain time window

2)  Second, eliminate all models with poor fits using the R$^2$ value

3)  Third, drop any models with negative slopes or intercepts (physically impossible)

First, we generate a rolling regression of the linear relationship $I_{\text{spec}} = I_{\text{direct}} \cdot \cos(\theta_{\text{sun-sensor}}) + I_{\text{scattered}}$ via a regression of irradiance on $\cos(\theta_{\text{sun-sensor}})$ over a specified time window (we used 30 seconds here).

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
# via a regression of Irradiance on cos_SSA over a specified time window (30s here)
mod_frame = xmp_all_ssa_site_date %>%
  drop_na(Date2) %>% 
  drop_na(cos_SSA) %>% 
  drop_na(Irradiance) %>% 
  # Fit a rolling regression
  # for each image, fit a linear model of all images (of the same band) within 30 seconds of the image
  tidyfit::regress(Irradiance ~ cos_SSA, m("lm"),
                   .cv = "sliding_index", .cv_args = list(lookback = lubridate::seconds(30), index = "Date2"),
                   .force_cv = TRUE, .return_slices = TRUE)
# df : summary of models, adding R sqaured and Dates
df = mod_frame %>% 
  # Get a summary of each model and extract the r squared value
  mutate(R2 = map(model_object, function(obj) summary(obj)$adj.r.squared)) %>% 
  # Extract the slope and intercept
  coef() %>% 
  unnest(model_info) %>% 
  mutate(Date2 = ymd_hms(slice_id)) 
# df_params : adding slope (direct irradiance) and y-intercept (scatterd irradiance) values
df_params = df %>%
  dplyr::select(Date:estimate, Date2) %>% 
  # we will have to go from long, with 2 observations per model, to wide
  pivot_wider(names_from = term, values_from = estimate, values_fn = {first}) %>% 
  dplyr::rename("Intercept" = `(Intercept)`,
                "Slope" = "cos_SSA")
# Cleaning up the df
df_p = df %>%
  filter(term == "cos_SSA") %>% 
  dplyr::select(Date:model, R2, p.value, Date2)
# Joining model info, parameter (slope, y intercept) info and XMP data with sun sensor angle and using the linear relationship 
# (spectral_irr = direct_irr * cos(SSA) + scattered_irr) to create % scattered and scattered/direct ratios
df_filtered = df_params %>% 
  left_join(df_p) %>% 
  left_join(xmp_all_ssa_site_date) %>% 
  mutate(percent_scattered = Intercept / (Slope + Intercept),
         dir_diff = Intercept/Slope)
```
</details>

Next, we eliminate all models with poor fits. In this case we eliminate models with R$^2$ \< 0.4 and drop any models with negative slopes or intercepts (physically impossible)

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
df_to_use = df_filtered %>% 
  mutate(R2 = as.numeric(R2)) %>% 
  filter(R2 > .4 
         & Slope > 0 & Intercept > 0) %>% 
  group_by(Date) %>% 
  mutate(mean_scattered = mean(percent_scattered),
         dir_diff_ratio = mean(dir_diff))
#save out dataframe as an rds
saveRDS(df_to_use,paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\",date,"_rolling_regression_filteredModels_used_plot1.rds"))  #SET PATH to save the rds to 
```
</details>

Below we plot the percent of scattered irradiance calculated from the models kept (blues) and those removed (grey). Percent scattered was calculated by diving the intercept (scattered component) by the sum of the intercept and the slope (direct component), see equation:$I_{\text{spec}} = I_{\text{direct}} \cdot \cos(\theta_{\text{sun-sensor}}) + I_{\text{scattered}}$. The maroon horizontal line shows the mean percent scattered irradiance, in this case just above 30% (Figure \@ref(fig:check-percent-scat-plot)). If you are loosing too many models, adjust the filters.

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
(RR_params <- df_to_use %>%
    group_by(Date, BandName) %>% 
    ggplot(aes(x = Date2, y = percent_scattered, color = R2)) +
    geom_point(data = df_filtered, color = "grey") +
    geom_hline(yintercept = 1, linetype = 2) +
    geom_hline(yintercept = 0, linetype = 2) +
    geom_hline(aes(yintercept = mean_scattered), color = "red4", linewidth = 1) +
    scale_y_continuous(breaks = seq(0, 1, .2), limits = c(0,1)) +
    ggnewscale::new_scale_color() +
    labs(title = paste0("Site: ",site_to_plot,", Date: ", date_to_plot, ", Camera: ", camera_to_plot),
         x = "Time (UTC)")+
    theme_bw(base_size = 16) +
    facet_wrap(. ~ Date, 
               scales = "free"))
```
</details>

<!-- ````{=html} -->
<!-- <!-- -->
<!-- ```{r eval=FALSE, include=TRUE} -->

<!-- ggsave(plot = RR_params, -->
<!--        filename = paste0(plot_dir,"1_CheckRollingRegressionParams_plot.jpeg"), -->
<!--        device = jpeg, -->
<!--        width = 15, -->
<!--        height = 10, -->
<!--        units = 'in', -->
<!--        dpi = 300, -->
<!--        bg = 'white') -->
<!-- ``` -->
<!-- ```` -->

```{r check-percent-scat-plot, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap= "Percent scattered irradiance calculated from the slope and y-intercept of the regression models. Blue points are those from regression models that remain after the models were filtered, where the shade of blue represents the R-squared of the regression model the value was calculated from. Grey point are values from mdoels that have been filtered out.  "}
knitr::include_graphics(here("Photos_&_gifs\\1_CheckRollingRegressionParams_plot_panel_paths_removed.jpeg"))
```

Next, check that the linear relationships you're keeping look realistic. The slope should be steeper for sunny days (i.e. more direct irradiance) and shallower for overcast days (Figure \@ref(fig:linear-plot-cosSSA)).

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
(linear_plots <- df_to_use %>%
    filter(BandName == "Blue") %>% 
    ggplot(aes(x = cos_SSA, y = Irradiance, color = R2)) +
    geom_point(data = filter(df_filtered, BandName == "Blue"), color = "grey30", alpha = .4) +
    geom_point(data = filter(df_filtered, BandName == "Blue" & R2 > .4), aes(color = as.numeric(R2))) +
    geom_smooth(method = "lm", se = FALSE, aes(group = BandName)) +
    lims(x = c(0, 1),
         y = c(0, max(df_to_use$Irradiance))) +
    geom_abline(aes(slope = Slope, intercept = Intercept, color = R2), alpha = .3) +
    labs(title = paste0("Site: ",site_to_plot,", Date: ", date_to_plot, ", Camera: ", camera_to_plot))+
    theme_bw(base_size = 16) +
    scale_color_viridis_c() +
    facet_wrap(. ~ Date, 
               scales = "free_y"))
```
</details>

<!-- ````{=html} -->
<!-- <!-- -->
<!-- ```{r eval=FALSE, include=TRUE} -->
<!-- ggsave(plot = linear_plots, -->
<!--        filename = paste0(plot_dir,"2_LinearRegression_plot.jpeg"), -->
<!--        device = jpeg, -->
<!--        width = 15, -->
<!--        height = 10, -->
<!--        units = 'in', -->
<!--        dpi = 300, -->
<!--        bg = 'white') -->
<!-- ``` -->

```{r linear-plot-cosSSA, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap= "Output of the linear regression models run on spectral irradiance and cos(sun sensor angle) over 30 second time windows. These models were filtered to only retain those that had an R2 greater than 0.4 and both a positive slope and intercept."}
knitr::include_graphics(here("Photos_&_gifs\\2_LinearRegression_plot_reDone.PNG"))
```

We recommend checking that there is no excessive spatial pattern in the data by plotting the locations of imagery used in the models. Here you want to ensure that imagery throughout the plot is being used rather than only images from a certain location (Figure \@ref(fig:spatial-regression-kept)). The code to do this is below:

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
(photos_kept <- df_to_use %>%
    filter(GPSLongitude != 0 & GPSLatitude != 0) %>% # Filter out imgs with GPSLongitude and GPSLatitude of zero, 
    # this is rare and in my experience were corrupted imgs where the XMP could not be properly read
    ggplot(aes(x = GPSLongitude, y = GPSLatitude, color = SunSensorAngle_DLS1_deg)) +
    geom_point(data = df_filtered, color = "grey60") +
    geom_point(size = 3) +
    labs(title = paste0("Site: ",site_to_plot,", Date: ", date_to_plot, ", Camera: ", camera_to_plot))+
    theme_bw() +
    scale_color_viridis_c() +
    facet_wrap(. ~ Date, scales = "free"))

```
</details>

```{r spatial-regression-kept, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap= "Location of MicaSense imagery aquired over the June 25th, 2024 flight on Vancouver Island. Grey points are the locations of images that were filtered out in the above steps and not used in the final regression models. Coloured dots are those used in the regression models kept and are coloured by sun sensor angle calculated with the DLS1 method."}
knitr::include_graphics(here("Photos_&_gifs\\3_GPSphotosKept_plot_lat_cutoff.jpeg"))
```

Lastly, we compute the scattered/ direct irradiance ratios dataframe from the dataframe of filtered models

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
(ratios = df_to_use %>% 
    dplyr::select(Date, mean_scattered, dir_diff_ratio,
                  GPSLatitude, GPSLongitude, Date2) %>% 
    group_by(Date) %>% 
    mutate(Lat_mean = mean(GPSLatitude),
           Long_mean = mean(GPSLongitude),
           Date_mean = mean(Date2)) %>% 
    distinct(Date, mean_scattered, dir_diff_ratio, Lat_mean, Long_mean, Date_mean))

saveRDS(ratios,paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\",date,"_ratios.rds"))  #SET PATH to save the rds to 

# Print the values to use in the calibration as a check (does this ratio looks reasonable?)
round(ratios$dir_diff_ratio, 2)
```
</details>

The ratio for the DLS1 was 0.46 and the mean_scattered component was \~32 W/m$^2$.

### Computing Horizontal (Corrected) Irradiance

This is the Fresnel correction, which adjusts for the DLS reflecting, rather than measuring, some of the irradiance that hits it. We aquired this code from [MicaSense's GitHub](https://github.com/micasense/imageprocessing/blob/master/MicaSense%20Image%20Processing%20Tutorial%203.ipynb) and converted to R to use in this workflow.

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
fresnel_transmission = function(phi, n1, n2, polarization) {
  f1 = cos(phi)
  f2 = sqrt(1 - (n1 / n2 * sin(phi))^2)
  Rs = ((n1 * f1 - n2 * f2) / (n1 * f1 + n2 * f2))^2
  Rp = ((n1 * f2 - n2 * f1) / (n1 * f2 + n2 * f1))^2
  T = 1 - polarization[1] * Rs - polarization[2] * Rp
  T = pmin(pmax(T, 0), 1)  # Clamp the value between 0 and 1
  return(T)
}

multilayer_transmission = function(phi, n, polarization) {
  T = 1.0
  phi_eff = phi
  for (i in 1:(length(n) - 1)) {
    n1 = n[i]
    n2 = n[i + 1]
    phi_eff = asin(sin(phi_eff) / n1)
    T = T * fresnel_transmission(phi_eff, n1, n2, polarization)
  }
  return(T)
}

# Defining the fresnel_correction function 
fresnel_correction = function(x) {
  
  Irradiance = x$Irradiance
  SunSensorAngle_DLS1_rad = x$SunSensorAngle_DLS1_rad
  n1=1.000277
  n2=1.38
  polarization=c(0.5, 0.5)
  
  # Convert sun-sensor angle from radians to degrees
  SunSensorAngle_DLS1_deg <- SunSensorAngle_DLS1_rad * (180 / pi)
  
  # Perform the multilayer Fresnel correction
  Fresnel <- multilayer_transmission(SunSensorAngle_DLS1_rad, c(n1, n2), polarization)
  return(Fresnel)
}

```
</details>

Now we put it all together to compute the horizontal irradiance. Here the horizontal irradiance can be thought of as a corrected value for the total irradiance that is reaching a point on the flat ground directly underneath the drone.

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
xmp_corrected = xmp_all_ssa  %>% 
  group_by(Date, BandName) %>% 
  #filter(BandName == "Red") %>% 
  #slice_head(n = 900) %>% 
  nest(data = c(Irradiance, SunSensorAngle_DLS1_rad)) %>% # Creates a nested df where each group is stored as a list-column named data, containing the variables Irradiance and SunSensorAngle_DLS1_rad
  mutate(Fresnel = as.numeric(map(.x = data, .f = fresnel_correction))) %>% # Applies the fresnel_correction function to each group of nested data
  unnest(data) %>% #unnesting
  # Joining the ratios
  left_join(ratios, by = "Date") %>% 
  mutate(SensorIrradiance = as.numeric(SpectralIrradiance) / Fresnel, # irradiance adjusted for some reflected light from the DLS diffuser
         DirectIrradiance_new = SensorIrradiance / (dir_diff_ratio + cos(as.numeric(SunSensorAngle_DLS1_rad))), # adjusted for sun angle, 
         HorizontalIrradiance_new = DirectIrradiance_new * (dir_diff_ratio + sin(as.numeric(SolarElevation))), 
         ScatteredIrradiance_new = HorizontalIrradiance_new - DirectIrradiance_new)
saveRDS(xmp_corrected,paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\CSV\\",date,"_xmp_corrected.rds"))  #SET PATH to save the rds to

```
</details>

Now we plot the sensor irradiance along with the corrected horizontal, direct, and scattered irradiance values

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
(Sensor_irr <- xmp_corrected %>% 
    filter(BandName == "Blue") %>% 
    ggplot(aes(x = Date2)) +
    geom_point(aes(y = SensorIrradiance, color = "Sensor Irradiance"),size = 1,show.legend = TRUE) +
    geom_point(aes(y = HorizontalIrradiance_new, color = "Horizontal_DLS1"), size = 1, show.legend = TRUE) +
    geom_point(aes(y = DirectIrradiance_new, color = "Direct_DLS1"), size = 1, show.legend = TRUE) +
    geom_point(aes(y = ScatteredIrradiance_new, color = "Scattered_DLS1"), size = 1, show.legend = TRUE) +
    geom_hline(yintercept = 0) +
    scale_color_manual(values = c("Sensor Irradiance"= "black", "Horizontal_DLS1" = "red", "Direct_DLS1" = "orange", "Scattered_DLS1" = "purple")) +
    #lims(y = c(50, 150)) +
    labs(y = "Irradiance", title = paste0("Site: ",site_to_plot,", Date: ", date_to_plot, ", Camera: ", camera_to_plot))+
    theme_bw() +
    facet_wrap(. ~ Date, scales = "free",
               ncol = 3) +
    labs(color = "Irradiance Type"))
```
</details>

<!-- ````{=html} -->
<!-- <!-- -->
<!-- ```{r eval=FALSE, include=TRUE} -->

<!-- ggsave(plot = Sensor_irr, -->
<!--        filename = paste0(plot_dir,"4_SensorIrradiance_w_CorrectedIrradiance_plot.jpeg"), -->
<!--        device = jpeg, -->
<!--        width = 15, -->
<!--        height = 10, -->
<!--        units = 'in', -->
<!--        dpi = 300, -->
<!--        bg = 'white') -->
<!-- ``` -->

```{r DLS1-corrected-irradiance, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap= "Direct, horizontal, sensor, and scattered irradiance across the duration of the flight corrected using the DLS1 method."}
knitr::include_graphics(here("Photos_&_gifs\\4_SensorIrradiance_w_CorrectedIrradiance_reDone.PNG"))
```

Now that we have corrected the data using the DLS1 method (Figure \@ref(fig:DLS1-corrected-irradiance)), we can compare the DLS1 values to the DLS2 values to see if correction is needed. The code below creates a dataframe that has the median scatted/direct ratios and median percent scattered irradiance values for the DLS1 method and from the DLS2 sensor. These values will be used to annotate the comparison plot in Figure \@ref(fig:DLS1-vs-DLS2-irr).

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
avg_ratio_data <- xmp_corrected %>%
  filter(!is.na(BandName))%>%
  group_by(BandName) %>%
  summarize(median_ScatteredDirectRatio_DLS2 = median(as.numeric(ScatteredIrradiance) /as.numeric(DirectIrradiance), na.rm = TRUE),
            median_ScatteredDirectRatio_DLS1_calc = median(as.numeric(ScatteredIrradiance_new) /as.numeric(DirectIrradiance_new), na.rm = TRUE),
            median_percent_scat_DLS2 = median(100*as.numeric(ScatteredIrradiance)/(as.numeric(ScatteredIrradiance)+as.numeric(DirectIrradiance))),
            median_percent_scat_DLS1 = median(100*as.numeric(ScatteredIrradiance_new)/(as.numeric(ScatteredIrradiance_new)+as.numeric(DirectIrradiance_new))),
            max_Date_time = max(Date_time),
            max_dir = max(DirectIrradiance, na.rm = TRUE)) %>%
  ungroup()%>%
  mutate(
    Scattered_To_Direct_Ratio_DLS2 = as.character(round(median_ScatteredDirectRatio_DLS2,2)),
    Scattered_To_Direct_Ratio_DLS1 = as.character(round(median_ScatteredDirectRatio_DLS1_calc,2)),
    Percent_Scat_DLS2 = as.character(round(median_percent_scat_DLS2,2)),
    Percent_Scat_DLS1 = as.character(round(median_percent_scat_DLS1,2)),
    x = max_Date_time,
    y = max_dir,
  )%>%
  dplyr::select(c(Scattered_To_Direct_Ratio_DLS2,Scattered_To_Direct_Ratio_DLS1,Percent_Scat_DLS2,Percent_Scat_DLS1, x, y, BandName, camera))

unique(xmp_corrected$CenterWavelength) #make sure all wavebands are present

saveRDS(avg_ratio_data,paste0(dir,date,"\\1_Data\\",MS_folder_name ,"\\CSV\\",date,"_avg_ratio_data.rds"))  #SET PATH to save the rds to

```
</details>


Plotting original VS correctedirradiance exif data to compare values from the DLS2 to corrected values using the DLS1 method (Figure \@ref(fig:DLS1-vs-DLS2-irr)).

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
data = avg_ratio_data %>% filter(BandName %in% c("Blue")) # only need to look at one band

(Dir_scat_irr_plot <- xmp_corrected %>%
    filter(BandName %in% c("Blue")) %>%
    mutate(DirectIrradiance = as.numeric(DirectIrradiance),
           ScatteredIrradiance = as.numeric(ScatteredIrradiance),
           Irradiance = as.numeric(Irradiance),
           SpectralIrradiance = as.numeric(SpectralIrradiance),
           HorizontalIrradiance = as.numeric(HorizontalIrradiance),
           # Date_time = ymd_hms(CreateDate),
           Time = format(Date_time, format = "%H:%M:%S"),
           #scaled 
           DirectIrradiance_scaled = as.numeric(DirectIrradiance)*0.01,
           ScatteredIrradiance_scaled = as.numeric(ScatteredIrradiance)*0.01,
           SpectralIrradiance_scaled = as.numeric(SpectralIrradiance)*0.01) %>%
    
    ggplot(aes(Date_time)) +
    geom_line(aes(y = DirectIrradiance, color = "Direct"), linewidth = 1) +
    geom_line(aes(y = DirectIrradiance_new, color = "Direct_DLS1"), linewidth = 1, linetype = "dashed") +
    
    geom_line(aes(y = ScatteredIrradiance, color = "Scattered"), linewidth = 1) +
    geom_line(aes(y = ScatteredIrradiance_new, color = "Scattered_DLS1"), linewidth = 1,linetype = "dashed") +
    
    geom_line(aes(y = HorizontalIrradiance, color = "Horizontal"),linewidth = 1) +
    geom_line(aes(y = HorizontalIrradiance_new, color = "Horizontal_DLS1"),linewidth = 1, linetype = "dashed") +
    
    geom_line(aes(y = Irradiance, color = "Irradiance"), linewidth = 1) +
    geom_line(aes(y = HorizontalIrradiance, color = "Horizontal"),linewidth = 1) +
    geom_line(aes(y = SpectralIrradiance, color = "Spectral"), linewidth = 1.5, linetype = "dotted") +
    labs(x = "Time (UTC)", y = "Irradiance", title = paste0("DLS1 (corrected) vs. DLS2 (metadata) Irradiance, Site: ",site_to_plot,", 
                                                      \nDate: ", date_to_plot,", Camera: ", camera_to_plot,
                                                      "\nScattered_To_Direct_Ratio_DLS1:", data$Scattered_To_Direct_Ratio_DLS1,
                                                      "\nScattered_To_Direct_Ratio_DLS2:", data$Scattered_To_Direct_Ratio_DLS2,
                                                      "\nPercent_Scat_DLS1:", data$Percent_Scat_DLS1,
                                                      "\nPercent_Scat_DLS2:", data$Percent_Scat_DLS2
                                                      )) +
    scale_x_datetime(date_breaks = "10 min", date_labels = "%H:%M") +
    scale_color_manual(values = c("Direct" = "blue", 
                                  "Direct_DLS1" = "lightblue",
                                  "Scattered" = "black",
                                  "Scattered_DLS1" = "grey",
                                  "Horizontal" = "red",
                                  "Horizontal_DLS1" = "orange",
                                  "Spectral" = "forestgreen",
                                  "Irradiance" ="purple"
    ),
    name = "Irradiance (W/m2/nm)")+
    facet_grid(BandName ~ camera, scales = "free")+
    theme_bw()
)
```
</details>

<!-- ````{=html} -->
<!-- <!-- -->
<!-- ```{r eval=FALSE, include=TRUE} -->

<!-- ggsave(plot = Dir_scat_irr_plot, -->
<!--        filename = paste0(plot_dir, "5_Irradiance_DLS1_DLS2_plot.jpeg"), -->
<!--        device = jpeg, -->
<!--        width = 15, -->
<!--        height = 10, -->
<!--        units = 'in', -->
<!--        dpi = 300, -->
<!--        bg = 'white') -->
<!-- ``` -->

```{r DLS1-vs-DLS2-irr, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap="Direct, horizontal, scattered, and spectral irradiance from the DLS2 and DLS1 method from the MicaSense exif data, flown on June 25th, 2024. Average ratio of scattered: direct irradiance componenets for the DLS1 was 0.46 and 0.09 for the DSL2."}
knitr::include_graphics(here("Photos_&_gifs\\5_Irradiance_DLS1_DLS2_plot_reDone.PNG"))
```

Refer to the section [Is Correction Needed?](#is-correction-needed) to help decide whether or not your data needs correcting.

Up to this point, we have calculated "corrected" irradiance and sun sensor angles however have not overwritten the exif data within the MicaSense imagery. In this next step, we will overwrite image exif data with the corrected DLS1 values. Before moving on to this step ensure that :

1)  You have decided that your data needs correcting

2)  You have created a backup of your original data, since you will be irreversibly overwriting the exif data of the imagery. Prior to running the code below we copied all images in our MicaSense folder to a folder in the same directory as the MicaSense folder however named named MicaSense_cor. Imagery in the MicaSense_cor folder will be the imagery we will be editing, and the MicaSense folder will contain the unedited imagery

3)  The MicaSense.config has been downloaded from [GitHub](https://github.com/owaite/GenomeBC_BPG), the file can be found in the "Scripts" folder under MicaSense

<details>
<summary>Click to show the code</summary>
```{r eval=FALSE, include=TRUE}
# Writing over exif data with corrected SSA, horizontal irradiance, direct irradiance, and scattered irradiance

corrected_directory <- paste0(dir,date,"\\1_Data\\",MS_folder_name,"_cor\\") #i.e. this folder is called MicaSense_cor and is a copied version of the MicaSense folder, we will be correcting the exif data of images in the MicaSense_cor folder and leaving the MicaSense folder untouched with the original exif data
original_directory <- paste0(dir,date,"\\1_Data\\",MS_folder_name,"\\") # path the folder of micasense images

xmp_corrected = readRDS(paste0(dir, date, "\\", MS_folder_name,"\\CSV\\",date,"_xmp_corrected.rds")) %>% #path to corrected xmp data .rds file
  mutate(SourceFile = str_replace(SourceFile,original_directory, corrected_directory), # Changing source file name from the original directory to the corrected_directory. This will result in imagery in the corrected direcotry only to be edited
         TargetFile = SourceFile) #the TargetFiles are the path names to the files to be edited

# As vectors
(img_list = xmp_corrected$FileName)
(targets = xmp_corrected$TargetFile)
(SSA = xmp_corrected$SunSensorAngle_DLS1_rad)

(horirrorig = xmp_corrected$HorizontalIrradiance)
(horirr = xmp_corrected$HorizontalIrradiance_new)
(dirirr = xmp_corrected$DirectIrradiance_new)
(scairr = xmp_corrected$ScatteredIrradiance_new)

targets[1] #checking its the right imgs

for (i in seq_along(targets)) {
  # given a micasense config file, overwrite tags with computed values
  # using exiftool_call from the exifr package
  call = paste0("-config G:/GitHub/GenomeBC_BPG/MicaSense/MicaSense.config", # SET PATH to the config file, this file is on the PARSER GitHub in the same folder as this R script
                " -overwrite_original_in_place",
                " -SunSensorAngle=", SSA[i],
                " -HorizontalIrradiance=", horirr[i],
                " -HorizontalIrradianceDLS2=", horirrorig[i],
                " -DirectIrradiance=", dirirr[i],
                " -ScatteredIrradiance=", scairr[i], " ",
                targets[i])
  
  exiftool_call(call, quiet = TRUE)
  print(paste0(i, "/", length(targets), " updated img:",img_list[i] ))
}
```
</details>

Lastly, check the exif data of the first and last image to ensure that the data has been properly overwritten.

<!--chapter:end:0_MicaSense_Irradiance_Correction.Rmd-->

# Thermal Conversion {#Thermal-Conversion}

This method converts raw imagery from the Zenmuse H20T to single band temperature tiffs. We use this method to bypass the DJI thermal analysis tool app since we had thousands of images to process and the app would often crash when a few images were uploaded. This method uses the DJI thermal analysis tool's software developer kit (SDK) instead to loop through a folder of images and return temperature rasters.

This method consists of:

1)  Gathering all "\*\_T.JPG" imagery into a single folder to loop through

2)  Calculating weighted averaged values for humidity and ambident temperature, which will be needed for the thermal analysis SDK to run

3)  Looping through imagery and converting to temperature rasters, then attaching original exif data to the new single band temperature rasters

## Folder Set-Up

This step goes over how we organized our imagery for the thermal conversion.

Here we:

-   create a folder named:

    -   "Merged_T" within the original "H20T" folder, this is the location the raw thermal images will be copied. This allow us to (a) keep a backup of the original data and (b) easily iterate over the images to convert them in later steps

    -   "Temperature_rasters" within the Merged_T folder that will hold converted imagery

    -   "Temperature_rasters_EXIF" within the Merged_T folder that will hold converted imagery that has original imagery exif data written to it

-   locate all folders within the defined directory that contain raw thermal images, in our case these were folders that had DJI and H20T in the name

-   Iterate over the folders found in the above step and:

    -   find files that end in \_T.JPG (the raw thermal images)
    -   copy over the raw thermal images to the merged folder

**Note:** Our folder structure was:

-   paste0("I:\\PARSER_Ext\\",site,"\\Flights\\", data_date, "\\1_Data\\H20T"), where "site" is a string of the site name that we defined prior to the for loop and data_date is a string representing the flight date that is defined in the for loop. This was done so we could easily iterate over sites and dates. Feel free to change this to match your folder structure.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
library(exiftoolr)
library(dplyr)
library(lubridate)
library(OpenImageR)
library(raster)
library(stringr)

# Setting the site and flight date to convert thermal imagery
flight_dates <-  c("2023_05_10") # Here only one date is shown however this can be a list of however many dates you would like to iterate over
site <- "site name here" 
merged <- "Merged_T" #name of the 'merged' folder that will contain all the h20T images from the multiple folders ouput by the H20T

for(i in 1:length(flight_dates)){
  data_date <- flight_dates[i]
  print(data_date)
  dir <- paste0("I:\\PARSER_Ext\\",site,"\\Flights\\", data_date, "\\1_Data\\H20T")
  #create folders
  if (!dir.exists(paste0(dir, "\\",merged))) {
    dir.create(paste0(dir, "\\",merged),recursive = TRUE)
  }
  if (!dir.exists(paste0(dir, "\\",merged,"\\Temperature_rasters"))) {
    dir.create(paste0(dir, "\\",merged,"\\Temperature_rasters"),recursive = TRUE)
  }
  if (!dir.exists(paste0(dir, "\\",merged,"\\Temperature_rasters_EXIF"))) {
    dir.create(paste0(dir, "\\",merged,"\\Temperature_rasters_EXIF"),recursive = TRUE)
  }
  
  list_dir <- list.files(dir, full.names = TRUE,pattern = c("DJI.+-H20T")) #selecting folders in the H20T folder that have DJI and end in -H20T, this is how we named our folders, change this pattern to match your folders. You should be selecting all folders with H20T imagery ending in _T for that flight
  print(list_dir) #to check only correct directories are being read
  
  for (d in 1:length(list_dir)){
    folder_dir <- list_dir[d] #calling each directly separately
    T_files <- list.files(folder_dir, pattern = "_T\\.JPG$") #selecting all JPEGs ending in _T 
    file.copy(from = paste0(folder_dir,"\\", T_files),
              to = paste0(dir,"\\",merged,"\\", T_files), overwrite = FALSE) #copying h20T images ending in _T (aka unprocessed thermal images) into the thermal only folder for ease in DJI SDK step
  }
}
```
</details>

## Weather Data

DJI thermal's SDK requires inputs of humidity, reflection (or ambient temperature), distance, and emissivity (see section on [parameters](#parameters) for more detail on each of the inputs). In this section we will take hourly weather data from an on-site HOBO climate logger and calculate weighted averages for humidity and ambient temperature.

First, we use the first and last image in the Merged_T folder to define the start and end time of the flight, along with the date of the flight as a date object.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
#getting image capture date from exif data of the first image in the H20T folder

data_date <- flight_dates[1]

time_file_list <- list.files(paste0(dir,"\\",merged), pattern = "_T\\.JPG$")

#Retrieving the time the FIRST H20T thermal image was taken
(img_1 <- time_file_list[1]) #find the first thermal image and print the name
start_exif_flight_date <- data.frame(exif_read(paste0(dir,"\\",merged,"\\",img_1),
                                               tags = "CreateDate",
                                               quiet = TRUE)) #find the time the image was taken/written from the CreateDate tag in the exif data
(start_flight_date_time <- start_exif_flight_date$CreateDate) #Start of the flight

#Retrieving the time the LAST H20T thermal image was taken
(img_last <- tail(time_file_list, n=1))#End flight time
end_flight_date_time <- data.frame(exif_read(paste0(dir,"\\",merged,"\\",img_last),
                                             tags = "CreateDate",
                                             quiet = TRUE))
(end_flight_date_time <- end_flight_date_time$CreateDate)#End of the flight


(flight_date <- as.Date(start_exif_flight_date$CreateDate, '%Y:%m:%d %H:%M:%S')) #the flight date as a date object
(flight_date_filt <- as.Date(flight_date, format = '%Y-%m-%d')) #flight date in the format of 2024-03-10, YYYY-mm-dd, this ill be used in the next step to filter weather data
```
</details>

Next, since our weather station provided hourly measurements, we calculate a weighted average of temperature and humidity values based on time intervals. This was done to account for the fact that variables will change throughout the flight. The hourly variables are weighted in proportion to how much of the hour fell within the flight window which gives us a better estimate of the overall weather variables than if a plain average was taken.

To begin, we read in the weather data and set a start time and end time of the flight. Start time is rounded down to the nearest hour (i.e. 10:15am becomes 10:00am) and end time is rounded up to the nearest hour (i.e. 11:30am becomes 12:00pm). These values will be used to filter the weather data.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
cfs_weather <- "path to weather data" #loading in HOBO weather station data
cfs_weather$Date <- as.Date(cfs_weather$Date, format = "%m/%d/%y") #converting date in the format m/d/Y to a date object written as YYY-mm-dd

start_time_avg <- "10:00:00" #Based off of start_flight_date_time of 10:15am
stop_time_avg <- "12:00:00" #Based off of end_flight_date_time of 11:30am

```
</details>

For simplicity we kept weights as 0.25 (15min), 0.5 (30min), 0.75 (45min) or 1 (1 hr) and rounded the start time to the earliest 15 min marker and the end time to the latest 15 min marker. For example, for a flight that started at 10:03am and ended at 11:09am, we rounded 10:03am to 10:00am and 11:09am to 11:15am.

Weights for start, middle, and end hours:

-   The start weight corresponds to how much of the first hour is covered by the flight time. For example, if the flight starts at 10:15, 45 minutes (or 0.75 of the hour) are included in that hour, so the start weight is 0.75.

-   The middle weight (if the flight spans more than one hour) represents the full hour covered by the flight. In the example of a flight from 10:15 to 11:30, the middle weight would be 1 which takes into account that the drone was flying at 11am when the climate logger logged data. 

-   The end weight applies similarly for the last hour of the flight. If the flight ends at 11:30, 30 minutes (or 0.5 of the hour) are included in that hour, so the end weight is 0.5.

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
#Ie: Flight starts at 10:15am and ends at 11:30am
start_weight <- 0.75 # 75% of 10am
mid_weight <- 1 # 100% of  11am 
end_weight <- 0.5 # 50% of 12pm

sum_weight <- sum(start_weight,
                  mid_weight,#comment out the mid_weight if no mid weight
                  end_weight) 

#creating df with weighted average air temperature and humidity values
(cfs_weather_time_avg <- cfs_weather%>%
  filter(Date == flight_date_filt ) %>%
  filter(Time <= stop_time_avg & Time >= start_time_avg)%>% #filtering for values taken between the defined start (10am) and end (12pm) times
  mutate(Average_hum = (start_weight*RH[1]
                         + mid_weight*RH[2] #comment out this line if no mid_weight and just below index of [3] to [2]
                         + end_weight*RH[3])/sum_weight,
         Average_air_temp = (start_weight*Temperature[1] 
                             + mid_weight*Temperature[2]#comment out this line if no mid_weight and just below index of [3] to [2]
                             + end_weight*Temperature[3])/sum_weight))

#check values 
(avg_hum <- cfs_weather_time_avg$Average_hum)
(avg_temp <- cfs_weather_time_avg$Average_air_temp)
(date <- as.Date(cfs_weather_time_avg$Date, '%d-%m-%Y'))

#creating a data frame  with weighted averaged values
Canoe_flight_table <- data.frame(date, avg_hum, avg_temp)
Canoe_flight_table <- Canoe_flight_table %>% distinct()#removing duplicate rows

#resetting index in table to go from 1-N
rownames(Canoe_flight_table) <- NULL

#setting variables for weighted average of humidity and ambient temperature, these variables will be used as inputs into DJI's thermal anlysis tool SDK to convert rasters to temperature
flight_humidity <- Canoe_flight_table[Canoe_flight_table$date == flight_date, "avg_hum"]
flight_humidity

flight_amb_temp <- Canoe_flight_table[Canoe_flight_table$date == flight_date, "avg_temp"]
flight_amb_temp

```
</details>

## DJI Thermal SDK: temperature conversion

### Parameters {#parameters}

The parameters required as inputs for DJI thermal's SDK are:

-   **Reflected Temperature** : The reflected temperature of the target takes into account the impact of radiation reflected from nearby objects that can significantly impact the temperature reading of the object. This parameter is important if there are any objects nearby with either an extremely high or low temperature. Otherwise the reflected temperature can be set to the ambient air temperature. Since none of our sites had an object with extreme high or low temperature nearby we used ambient air temperature (flight_amb_temp) from on-site HOBO climate loggers.

-   **Relative Humidity**: The relative humidity during the flight. The default value is set to 70% and the allowed range is 20-100%. We used a weighted average humidity value (flight_humidity) calculated using data from the on-site HOBO climate loggers.

-   **Distance**: The distance from the sensor to the target (i.e. tree crowns). The H20T is an infrared thermal sensor that measures the infrared radiation received from objects. Hence, the further away the object, the more the radiation attenuates and the less accurate the temperature measurement. The maximum distance that the thermal analysis tools allows is 25m. In our case, we are flying higher (\~40m away from the crowns, depending on the site) and thus use the 25m maximum and interpret the temperature values as relative to each other while understanding that the absolute temperature likely contains error given the larger distance between the sensor and the object.

-   **Emissivity**: The emissivity of a material is a measure of its ability to emit energy as thermal radiation. The default value is 1. We used a value of 0.98 as an estimate for all conifers (Rubio et al., 1997).

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
#setting parameters:
(flight_amb_temp) # already set above, () to print out the value in the console to ensure it is the correct one
(flight_humidity) # already set above
distance <- 25 # max distance that can be set
emissivity <- 0.98 # approximate value for trees
```
</details>

### Running the SDK

The SDK is run using a batch script that calls the SDK in a for loop to iterate over the imagery. To set up, first we copy the original .bat file into the Merged_T folder. We do this so that the original script does not get edited and instead each flight will have their own edited .bat file. This allows you to go back and see what parameters were used if needed.

The original [.bat file](https://github.com/owaite/GenomeBC_BPG/blob/main/Scripts/DJI_SDK_loop.bat) can be found on GitHub and looks like this:

<details>
<summary>Click to show the code</summary>
```{cmd, eval=FALSE, echo=TRUE}
REM to change dir to the location of this file
REM start time used to average humidity and temperature values: start_time_avg
REM end time used to average humidity and temperature values: stop_time_avg
REM weights (in 15min intervals ie 0.25, 0.5, 0.75 or 1, round down for start and up for end time): start w: start_weight, mid w: mid_weight (might not have one), end w: end_weight

cd /D "%~dp0"

mkdir DJI_SDK_raw

for %%i in (*.JPG) do (
echo Working on %%i...
C:\dji_thermal_sdk_v1.3_20220517\utility\bin\windows\release_x64\dji_irp.exe -s %%i -a measure -o DJI_SDK_raw/%%~ni.raw --humidity hum_change --distance dist_change --emissivity emis_change --reflection reflec_change
)
pause
```
</details>

This script makes a folder called DJI_SDK_raw in the directory the .bat is saved in (which will become the Merged_T folder once we copy it over in the below code) and converts the imagery in the Merged_T folder ending in .JPG to temperature using DJI's thermal anlysis tool's SDK.

Before continuing to the next steps:

1)  save the [.bat file](https://github.com/owaite/GenomeBC_BPG/blob/main/Scripts/DJI_SDK_loop.bat) from GitHub and update the omp_dir path in the below R code to the folder where this file was saved to

2)  download the dji thermal sdk [here](https://www.dji.com/ca/downloads/softwares/dji-thermal-sdk)

3)  edit the location of the dji_irp.exe in your saved [.bat file](https://github.com/owaite/GenomeBC_BPG/blob/main/Scripts/DJI_SDK_loop.bat) to match where the program is saved on your computer

    -   i.e. replace the "C:\\dji_thermal_sdk_v1.3_20220517\\utility\\bin\\windows\\release_x64\\dji_irp.exe" in the above script to the directory you saved the dji_irp.exe file from step 2

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
omp_dir <- "C:/Users/owaite/Documents/Scripts/DJI_SDK_TemperatureConversion_Script" #change to the directory where you saved the original .bat script from the above GitHub link to

file.copy(from = paste0(omp_dir,"\\", "DJI_SDK_loop.bat"), #copy the original omp.bat file into the Merged_T folder
          to = paste0(dir,"\\",merged,"\\","DJI_SDK_loop.bat"), overwrite = FALSE)

bat_old <- file(paste0(dir,"\\",merged,"\\","DJI_SDK_loop.bat")) #opening .bat file to edit
bat_new <- readLines(bat_old) #reading in lines of .bat file to edit and saving them out
close(bat_old)

#replacing values
comment out mid_weight line if necessary
bat_new <- gsub("hum_change",flight_humidity, bat_new)
bat_new <- gsub("dist_change",distance, bat_new)
bat_new <- gsub("emis_change",emissivity,bat_new)
bat_new <- gsub("reflec_change",flight_amb_temp,bat_new)
bat_new <- gsub("start_time_avg",start_time_avg, bat_new)
bat_new <- gsub("stop_time_avg",stop_time_avg,bat_new)
bat_new <- gsub("start_weight", start_weight, bat_new)
bat_new <- gsub("mid_weight", mid_weight, bat_new) #if no mid_weight, comment this line out
bat_new <- gsub("end_weight", end_weight, bat_new)

#Write the new file
fileConn <- file(paste0(dir,"\\",merged,"\\DJI_SDK_loop_updated.bat"))
writeLines(bat_new, fileConn)
close(fileConn)
```
</details>

Next, we run the updated .bat file in windows terminal using the R code below. This will automatically output a folder of new raw images with temperature in binary saved to the Merged_T/DJI_SDK_raw folder

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
shell.exec(paste0(dir,"\\",merged,"\\DJI_SDK_loop_updated.bat"))
```
</details>

## Convert to Raster

The following R script:

-   reads in the binary data outputted by the thermal SDK

-   converts the data to rasters

-   updates the new temperature raster with exif data from original \_T.JPG H20T imagery

<details>
<summary>Click to show the code</summary>
```{r, eval=FALSE, echo=TRUE}
dir_raw <- paste0(dir,"\\",merged,"\\DJI_SDK_raw\\")
raw_list <- list.files(dir_raw) #check this is a list of the images saved out from the SDK step above
start.time <- Sys.time()#to log how long it takes for code to run 

for (i in 1:length(raw_list)){#loop that only writes out a folder with the Temperature rasters
  wh <- c(640, 512)
  print("new")
  str_m <- raw_list[i]
  #getting name of .raw file but without the .raw extension so it can be saved as a tiff with the same name as the .raw (which has the same name as the original H20T images) so they can be swapped after alignment in metashape
  name <- substr(str_m,1,nchar(str_m)-4)
  print(name)
  #If else below allows you to rerun this code from where it left off if R aborts the session
  test_exists <- paste0(dir,"\\",merged,"\\Temperature_rasters_EXIF\\", name,".tiff")
  if (file.exists(test_exists)){
    print("exists")
  } else {
    #divide by 2 because 16 means 2 bit integers (?)
    #print(paste0(dir_raw, str_m))
    
    file.info(paste0(dir_raw, str_m))$size/2 
    prod(wh) #getting product of width and height and checking it is correct
    v <- readBin(paste0(dir_raw,str_m), what = "integer",  #getting numerical values
                 n = prod(wh), size = 2,  
                 signed = TRUE, endian = "little")
    
    v_temp <- v/10 #temp in deg Celsius, DJI requires we divide by 10 here for Celsius
    range(v_temp) #check the values
    matrix <- matrix(v_temp, wh[1], wh[2])[wh[1]:1,] # creating a matrix of the proper size
    mat_rotated = rotateFixed(matrix, 90)#rotate the matrix by 90 to get correct orientation of image, requires library OpenImageR
    #to view image
    #image(matrix(v_temp, wh[1], wh[2])[wh[1]:1,], useRaster = TRUE, col = grey.colors(256))
    rast <- raster(mat_rotated) #creating a raster 
   
    #matching original image to measure.raw file
    dir_h20t <- paste0(dir,"\\",merged,"\\") #original h20T images:
    str_h20t <- paste0(substr(str_m, 1, nchar(str_m)-3),"JPG") #since names are the same, here we call the name of the raw file -.raw and add .JPG
    r_h20t <- raster(paste0(dir_h20t,str_h20t)) #reading in jpg h20T raster
    r_h20t_path <- paste0(dir_h20t,str_h20t) #for later exiftool
    print(r_h20t_path) #checking the path
    
    #need to add these before exif because exif command used won't overwrite already written ones
    extent(rast) <- extent(r_h20t)# the extent is bound by the resolution that you have assigned to it - so must change extent before resolution
    res(rast) <- res(r_h20t)
    #crs(rast) <- crs(r_h20t)
    print(paste0(dir,"\\",merged,"\\Temperature_rasters\\", name, ".tiff"))
    writeRaster(rast, paste0(dir,"\\",merged,"\\Temperature_rasters\\", name,".tiff"), overwrite = TRUE) #writing out temperature raster, that does not have exif data attached
    r_temperature_path <- paste0(dir,"\\",merged,"\\Temperature_rasters\\", name,".tiff")
    
    #Use EXIFTOOL in terminal to add all remaining/missing exif data from the original H20T images to the new single band temperature TIFFS
    out_dir <- paste0(dir,"\\",merged,"\\Temperature_rasters_EXIF\\", name,".tiff")
    shell(paste0("exiftool -wm cg -tagsfromfile ",r_h20t_path," -all:all ",r_temperature_path, " -o ",out_dir)) #shell lets you run windows terminal cmds from R  
    }
  percent <- i/length(raw_list)*100
  print(paste0("percent complete: ", round(percent, 3))) #gives you the % of photos done to give an idea of code speed/progress
}
end.time <- Sys.time()
(time.taken <- round(end.time - start.time,2)) #time taken for the script to run
```
</details>

The output are single band thermal tiffs that can be loaded into metashape to create a thermal orthomosaic.

## Examples

Below are two examples of data aquired using the H20T to show both the radiometric and spatial resolution of the H20T within the crowns as well as some important factors to keep in mind when interpreting crown temperatures. 

Figure \@ref(fig:2-crown-temp-NIR) shows the temperature and NIR values of two mature (\~25 year old) Douglas-fir tree crowns, where the crowns delineate the upper 25th percentile of the tree, from a flight that occurred on July 27th, 2022 under heatdome conditions. These flights occurred in full sun conditions. Looking at the NIR plots, we can see shadowed areas (see chapter \@ref(shadow-mask) for shadow masking with NIR) towards the north east sides of both crowns. These shadowed areas roughly align with the areas of cooler temperature seen in the crown temperature plots. As you can see in the daily temperature plot on the left, the MicaSense flight occurred from 1:30-2:30pm and was followed by the thermal flight, flown with the H20T, from roughly 2:40-3:40pm. Hence, we see a greater area of cooler crown temperatures in the thermal imageru than the NIR proxy for shadow would suggest likely due to the increase in shadow from the MicaSense flight to the thermal flight as the solar zenith angle decreases into the afternoon.

```{r 2-crown-temp-NIR, echo=FALSE, fig.align='center', out.width="100%", fig.cap = "Left: plot of hourly ambient tempatures from an on-site HOBO climate logger at Canoe for July 27th, 2022. Middle column: near-infrared (NIR) values for two Douglas-fir crowns aquired from a MicaSense flight flown directly before the thermal flight. Right column: temperature values within two mature Douglas-fir crowns flown under heatdome conditions. Pixels plotted for both the NIR and temperature rasters are those within a delineated crown representing the top 25th percentile of the tree."}
knitr::include_graphics(here("Photos_&_gifs/two_crown_nir_rgb_temp_graph.PNG")
)
```

<!-- # ```{r crown-temp, echo=FALSE, fig.show="hold",fig.align='center', out.width="50%", fig.cap = "Left: temperature values within a Douglas-fir crown flown under heat dome conditions (July 27th, 2022). Right: plot of hourly ambient tempatures from an on-site HOBO climate logger for July 27th, 2022, the grey vertical line is the time of the flight"} -->

<!-- # # Include the pre-brushing and post-brushing images -->

<!-- # knitr::include_graphics(c( -->

<!-- #   here("Photos_&_gifs/crown_temp_graph.png"), -->

<!-- #   here("Photos_&_gifs/Temperature_graph_for_crown_pic_2.png") -->

<!-- # )) -->

<!-- # ``` -->

Figure \@ref(fig:crown-temp-RGB) shows the temperature values for two young (\~5 year old) Douglas-fir trees from an overcast day. Whereas we saw higher temperatures in areas with more direct light in Figure \@ref(fig:2-crown-temp-NIR), we see the opposite pattern below. This is likely due to the heat of the ground radiating (i.e. warming) the lower branches of the young trees which are in view given the relatively small crown sizes and lack of crown closure. This is an important factor to keep in mind when analyzing crown temperatures.

```{r crown-temp-RGB, echo=FALSE, fig.align='center', out.width="100%", fig.cap = "Left: section of a P1 orthomosaic of W45 showing two neighbouring tree crowns. Right: temperature values for the two neighbouring young Douglas-fir."}
# Include the pre-brushing and post-brushing images
knitr::include_graphics(here("Photos_&_gifs/Thermal_RGB_crowns.PNG")
)
```

References:

Rubio, E., Caselles, V., & Badenas, C. (1997). Emissivity measurements of several soils and vegetation types in the 8–14, μm Wave band: Analysis of two field methods. Remote Sensing of Environment, 59(3), 490–521. [https://doi.org/10.1016/S0034-4257(96)00123-X](https://doi.org/10.1016/S0034-4257(96)00123-X){.uri}

<!-- **Note**: We have run across an error in some imagery that we have copied below, we are in contact with DJI to look for a solution. -->

<!-- The error is as follows: -->

<!-- ERROR: call dirp_set_measurement_params failed -->

<!-- ERROR: call prv_isp_config failed -->

<!-- Test done with return code -6 -->

<!--chapter:end:0_Thermal_Conversion.Rmd-->

