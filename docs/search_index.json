[["index.html", "Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials GitHub link Acknowledgements How to cite this report:", " Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials Jake King*, Olivia Waite, Miriam Isaac-Renton, Nicholas C. Coops, Samuel Grubinger, Liam Irwin, Lise Van Der Merwe, Jon Degner, Alex Liu Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials Date last updated: 2024-09-10 Jake King1, Olivia Waite1, Samuel Grubinger2, Alex Liu1, Miriam Isaac-Renton1, Nicholas C. Coops2, Liam Irwin2, Lise Van Der Merwe3, Jon Degner3, Alvin Yanchuk3 1 Natural Resources Canada, Canadian Forest Services, Canadian Wood Fibre Center, 506 Burnside Road West, Victoria, British Columbia, V8Z 1MZ. 2 Integrated Remote Sensing Studio, Faculty of Forestry, University of British Columbia,2424 Main Mall, Vancouver, BC V6T 1Z4, Canada. 3 BC Ministry of Forests, Cowichan Lake Research Station, 7060 Forestry Rd, Mesachie Lake, BC V0R 2N0. GitHub link Below is the link to the GitHub where you can access full scripts referenced in this document GitHub Link: PARSER-GBC-GitHub Acknowledgements These guidelines were made possible thanks for project funding from Genome British Columbia’s GeneSolve program, the Canadian Forest Service’s Fibre Solutions, 2 Billion Tree programs, and Assistant Deputy Minister’s Innovation Fund. For administrative assistance, we thank Adam Dick, Olivier van Lier, Marlene Francis, Annick Pelletier, Lise Carron, Guy Smith and Amélie Roberge. For practical input, we thank Bill Lakeland, Alec Wilson, Eric Saczuk, Keenan Rudichuk and David Huntley. How to cite this report: "],["range-of-sites-assessed.html", "Chapter 1 Range of Sites Assessed 1.1 Jordan River 1.2 Powell River 1.3 2024 Sites", " Chapter 1 Range of Sites Assessed 1.1 Jordan River Western Red cedar (Cw) Douglas fir (Fdc) Cw_East – planted in 2000, 3980 live trees on 4480 positions Fdc_East – planted in 2003, 1612 live trees on 3019 positions Cw_Sandcut – planted in 2012, 3500 live trees on 3841 positions Fdc_W45 – planted in 2019, 2076 live trees on 2494 positions 1.2 Powell River Western Red cedar (Cw) Douglas fir (Fdc) Cw_Rainbow – planted in 2000, 3674 live trees on 4745 positions Fdc_Canoe – Planted in 1999, 1739 live trees on 3164 positions 1.3 2024 Sites Douglas fir (Fdc) Douglas fir (Fdc) Big Tree GCA – Planted in 2003, 2229 live trees on 2900 positions Hillcrest GCA – Planted in 2003, 1514 live trees "],["preflight-planning.html", "Chapter 2 Preflight Planning 2.1 Achieving positional accuracy on multi-temporal and multi-sensor data collections 2.2 Site Selection Considerations 2.3 Drone and Sensors: What will I need to purchase and how much will it cost? 2.4 ", " Chapter 2 Preflight Planning 2.1 Achieving positional accuracy on multi-temporal and multi-sensor data collections 2.1.1 Kinematic processing: RTK/PPK 2.1.2 Ground Control Points (GCP) Here in Figure 2.1 Figure 2.1: Aerial view of a GCP at the 12-year old Western redcedar Sandcut site taken with the P1 sensor. 2.1.3 Absolute and Relative Reference with Precise Point Positioning (PPP) 2.1.4 Terrain Following on Sites with Elevation Change Figure 2.2: Terrain following on variable topography. 2.2 Site Selection Considerations Ideal sites: Figure 2.3: Aerial view of a 5 year old Douglas fir trial. Effects of topography: Warning to take extra care Below Figure 2.4 Figure 2.4: Effect of brushing on crown visibility. Left: GCP alignment on flat ground (Error ~4cm). Right: GCP alignment on ground with elevation change (Error ~28cm) Below Figure 2.5 shows an example of the effect of brushing on crown visibility at a moist, high-productivity common-garden trial. Figure 2.5: Effect of brushing on crown visibility. Left: early August prior to brushing. Right: late August post brushing 2.3 Drone and Sensors: What will I need to purchase and how much will it cost? 2.3.1 Hardware Costs Table 2.1 Table 2.1: A breakdown of drone and sensor costs, where newer versions are available, both are listed. The * would be a “wishlist” full set up. This is just the major purchases and doesn’t include general field equipment. Hardware Purpose Cost Detail *Zenmuse P1, RGB Sensor $9,000 Positional accuracy - Horizontal: 3 cm, Vertical: 5 cm. Ground Sampling Distance (GSD) - 1cm at 80 m elevation Micasense RedEdge-MX Dual Sensor $16,000 Ten band multispectral camera, no longer in production *Micasense RedEdge-P Dual (Panchromatic) Sensor $22,500 Ten band multispectral camera with Panchromatic sharpening Zenmuse H20T, Thermal and RGB Sensor $13,350 Thermal and RGB sensor Zenmuse L1, LiDAR and RGB Sensor $11,600 LiDAR sensor released in 2020 *Zenmuse L2 LiDAR and RGB Sensor $16,660 LiDAR sensor released in 2023, now with 5 returns and better accuracy DJI Matrice (M)300 RTK Drone $12,000 2020 released DJI enterprise drone with RC, intelligent battery case and one set of batteries *DJI Matrice (M)350 RTK Drone $13,500 2023 released DJI enterprise drone with RC plus, intelligent battery case and one set of batteries *DJI TB65 batteries Drone batteries $1,910 Pair of the newer model M300/350 batteries. Good for one approximately 30-minute mapping flight including safety margins. At least 4 pairs are recommended for continuous flight when paired with a small generator DJI DRTK2 GNSS receiver $4,200 GNSS receiver for use as base to stream RTK to drone *Emlid RS3 GNSS receiver $3,600 GNSS receiver for use as base to stream RTK to drone and rover *Emlid RS3 GNSS receiver $3,600 GNSS receiver for use as rover for centimeter precise GCP or stem mapping Table 2.2 are some untested options: Table 2.2: Options that we would like to test in the future but have not yet had the chance to. Hardware Purpose Cost Detail DJI Mavic 3 Multispectral0 Sensor, Drone $5,945 RTK capable drone, RGB and 4 multispectral wavelengths Senterra 6X Sensor $17,120 RTK capable, 5 multispectral plus RGB, potential to customize wavelengths, but extra price unknown Micasense Altum PT Sensor $25,950 Thermal, and multispectral panchromatic, RTK capable sensor 2.3.2 Drone Training in Canada 2.4 "],["data-collection-and-flights.html", "Chapter 3 Data Collection and Flights 3.1 Hardware - DJI Matrice 3000 RTK (M300) 3.2 Workflow: Site Reconnaissance 3.3 Data Collection: Flight Planning 3.4 Data Collection: Sensors, Parameters, and Ideal Conditions", " Chapter 3 Data Collection and Flights 3.1 Hardware - DJI Matrice 3000 RTK (M300) Figure 3.1: DJI Matrice 300 RTK in flight over one of our Vancouver Island feild sites, taken by Alec Liu with a Mavic3. A complete SOP for mapping flights with the M300 was developed in-house and can be found in the SOPs folder on GitHub. Transport Canada (TC) regulations require logging all flights and to have the logs on-hand when flying. We have developed a simple spreadsheet that logs the details required by TC and other useful details for each flight. When undertaking a project with repeated data collection campaigns, we would recommend developing a similar approach to organizing these data. A copy of the one we use is available within the Example Documents folder on GitHub. 3.2 Workflow: Site Reconnaissance Ahead of the first data collection flights we follow the reconnaissance workflow outlined in Figure 3.2 Figure 3.2: Reconnaissance workflow. 3.3 Data Collection: Flight Planning 3.3.1 Flight planning theory Figure 3.3 outlines the difference in ground overlap and canopy overlap. We have developed a calculator in which you enter the flight altitude, overlaps, and canopy height and it will output the canopy overlap. You can then adjust the inputs until you get the required overlap. It is available in the Example Documents folder on GitHub. Figure 3.3 below is an example calculation detailed in the next paragraph. Figure 3.3: Effective overlap calculation. 3.3.2 Flight planning with DJI Pilot Figure 3.4 Figure 3.4: Flight planning a P1 flight on the remote controller. 3.4 Data Collection: Sensors, Parameters, and Ideal Conditions 3.4.1 MicaSense: 10-Band Spectral Sensor for Vegetative Indices Figure 3.5 Figure 3.5: Micasense RedEdge-MX Dual and the DLS2 mounted on the M300. Here we are connecting the DLS2 to the Micasense camera. Sensor Specifications: Camera Type: MicaSense Dual cameras Image Format: .tif with a suffix indicating the band (e.g., _1, _2) Bands/Wavelengths: Table 3.1: Band names, center wavelengths, and bandwidths collected by the Red and Blue cameras within the Micasense Dual camera systems. Panchro* is only available in the pan chromatic models. Band Name Center Wavelength (nm) Bandwidth (nm) Camera Blue 475 32 Red Green 560 27 Red Red 668 14 Red Red Edge 717 12 Red NIR 842 57 Red Panchromatic 634 463 Red Coastal Blue 444 28 Blue Green 531 14 Blue Red 650 16 Blue Red Edge 705 10 Blue Red Edge 740 18 Blue Image Capture Rate: The default setting for timed intervals is one image capture every two seconds, with simultaneous capture for all bands. The capture rate can be adjusted to a maximum of one image per second Output: 5-6 folders per camera (10-12 thousand images) for a 25–30-minute flight covering a 1.5-2 Ha site GSD: 2.5-4cm at 40m above canopy. 1.5-2cm when processed pansharpened with the panchromatic band. Flight Parameters: Flight Elevation: 40 m above canopy Flight Speed: ~2 m/s (slower speeds reduce motion blur, especially in windy conditions) Image Overlap: Extra high to ensure sufficient coverage and allow for exclusion of poor-quality images during processing. Front: ~86% (timed interval is the limiting factor) Side: ~86% Margins: 10 m around the site perimeter is sufficient for flight planning due to the wide-angle shot of the MS camera Flight Time: Approximately 30 minutes for a 1.5-2 Ha area site. This allows for a single battery flight, which helps to maintain even light conditions. Best Practices: Weather Conditions: Avoid flying in any precipitation as MicaSense cameras are unprotected from moisture, with exposed data and power connections. Lighting: Conduct flights within two hours of solar noon to minimize shadowing in the imagery. The DLS calibration helps to reduce the effect changing light conditions, but extreme changes in light conditions will leave residual effects that are difficult to process out. Figure 3.6 below shows a flight in variable conditions (left) in which the calibration has failed to correct the changing light conditions. The flight on the right was flown on an ideal day with even diffuse light. Figure 3.6 Figure 3.6: Left: MS orthomosaic in variable light. Right: MS orthomosaic in even diffuse light. The ideal flight as in Figure 3.6 would be at solar noon in evenly overcast (diffuse) light conditions. Wind Conditions: Lower wind speeds yield better results; however, the impact of wind will vary depending on species and site-specific factors. Consider flying at a higher altitude and overlap in windier conditions. Flight Elevation Considerations: While lower flight elevations produce higher resolution images, they increase flight time, making calibration more challenging. Click here to access the Micasense field SOP housed on GitHub. Best practices: Collecting Data with MicaSense Sensors – MicaSense Knowledge Base is an important resource. 3.4.2 Zenmuse P1: High-Quality Natural-Colour (RGB) Imagery Figure 3.7 Figure 3.7: These images are the same area, the left was taken with the above parameters with the P1 before greenup. The right was taken with the wide angle RGB on the Zenmuse H20T, a lower resolution camera, mid-August. Figure 3.8: L1 LiDAR screenshot from the remote controller during a flight at Big Tree Creek site on July 7, 2024. On the left is the RGB image instantaneously acquired for the frame and the right shows the LIDAR data acquisition from the scanner as it acquired the data up the frame. "],["photogrametric-processing.html", "Chapter 4 Photogrametric Processing 4.1 Processing Orthomosaics 4.2 Shadow Masks 4.3 Crown-level Vegetation Indicies", " Chapter 4 Photogrametric Processing Figure @ref(fig: aligned-agisoft) Figure 4.1: A dense point cloud processed in Agisoft Metashape with the camera locations turned on and disabled flight trajectories shown. Figure 4.2: Recommended Metashape preferences setup under the Advanced tab. Figure 4.3: Calibrate reflectance tool found under Tools in Metashape. Figure 4.4: Calibrate reflectance tool found under Tools in Metashape with sun sensor and reflectance panels enabled for calibration. Figure 4.5: Photogrametric camera alignment settings using the alignment tool in Metashape Figure 4.6: Sparse cloud created in image alignment. Pink points are those selected by the Gradual Selection tool that have a reconstruction uncertainty greater than or equal to 30.4608. These points will be filtered out. Figure 4.7: Settings used for the Optimize Camera Alignment tool Figure 4.8: Settings used for point based chunk alignment when aligning the multispectral data to the P1. Figure 4.9: Visual of the [T] that will appear next to chunks that have successfully aligned via chunk alignment. The [T] stands for transposed, visual inspection of alignment at all four corners and the center of the plot should be done to ensure the error in the alignment is within an acceptable range. We have had sites complete alignment ‘successfully’ however with large, unacceptable error that required alignment using GCPs. Figure 4.10: Metashape settings for building the depth maps and the dense cloud. Figure 4.11: Metashape settings for DEM generation. Figure 4.12: Metashape settings for building the orthomosaic. Figure 4.13: Raster calculator tool in Metashape to look at reflectance values of trees for band 1 (Blue - 444nm) prior to exporting the orthomosaic. Figure 4.14: Reflectance values of trees for band 4 (Green - 560nm). Figure 4.15: Reflectance values of trees for band 10 (NIR - 842nm). Figure 4.16: The spreadsheet map of the layout of the site, each tree is located on a grid with a unique row and column value assigned to it. Figure 4.17: P1 orthomosaic of the site with GCPs and georeferenced trees marked with red dots Figure 4.18: Visual of the Grid Georeferenced plugin in QGIS. Figure 4.19: Left: treeID 1, reference for the bottom left corner of the plot. Right: treeID 2757, reference for the top right of the plot Figure 4.20: Left: Transformation settings. Right: zoom in on vector layer and place a point on the Map Canvas Figure 4.21: Zoomed in visual of the georeferenced tree grid for 6 trees overlain on the P1 orthomosaic. Figure 4.22: Georeferenced grid overlain on P1 orthomosaic for a site that was well maintained. The georeferenced grid has been snapped to the highest point on the CHM within a given radius to pull the grid to the treetops. 4.1 Processing Orthomosaics 4.2 Shadow Masks Pixels containing shadows and openings in the canopy can introduce error in vegetation indices as reflectance values are often lower in shaded pixels (Malenovský et al., 2013; Zhang et al., 2024). Masking out shaded pixels and openings within the crown is necessary to calculate accurate vegetation indices. We found masking with NIR reflectance to be a consistent and effective approach for masking out shadows and gaps. We use the NIR reflectance band (842nm) from the Micasense RedEgde-MX Dual. NIR thresholds are determined using the shape of the NIR distribution. If the NIR distribution is bimodal, the threshold is set to the local minimum, whereas if the distribution is unimodal, the threshold is set to the local maximum (Chen et al., 2007; D’Odorico et al., 2021; Otsu et al., 2019). Below we define a function find_local_min that takes a list of first derivatives and returns a dataframe containing: neg_value: The value of the first derivative directly before the minimum (i.e. the negative derivative value to the left of the minimum) pos_value: The value of the first derivative directly after the minimum (i.e. the positive derivative value to the right of the minimum) pos_def: The summation of the positive gradient values (a.k.a. slope values) in the first 15 gradient values following the local minimum. This value provides an idea of how great the increase in slope is after the local minimum. neg_def: The summation of the negative gradient values in the first 15 gradient values preceding the local minimum. This value provides an idea of how great the decrease in slope is before the local minimum. Index: Index position of the negative gradient value bordering the local minimum. This is the index of our threshold value since we do not get a true zero slope value at the local minimum but rather a change from a very small negative gradient value to a very small positive gradient value. definition: The summation of the absolute value of pos_def and neg_def values. The greater this number, the more defined the minimum. The above parameters were added to describe each minimum found by the function so that the true local minimum could be filtered for. This function will be used in the next few steps to isolate minimums in the NIR distribution. # This function finds local minimums and ranks how defined they are by the &#39;definition&#39; attribute find_local_min &lt;- function(values) { #input &quot;values&quot; is a list of 1st derivatives of NIR values #initializing variables neg_slope &lt;- numeric() pos_slope &lt;- numeric() index_of_closest &lt;- numeric() definition = numeric() neg_sum = numeric() pos_sum = numeric() k &lt;- 1 #initializing iterator for (i in 2:length(values)) { #skip index 1 so following &quot;values[i-1]...&quot; can properly index #print(i) if (values[i - 1] &lt; 0 &amp; values[i] &gt;= 0 &amp; i &gt; 15) { #finds a change from negative to positive 1st derivative (local min) #i&gt;15 because local min will not be in first 15 values and this stops an error occurring where a local min is found in the first 15 values and the def_positive indexing does not work def_positive &lt;- c(values[i:(i+15)]) # vector of next 15 gradient values def_pos &lt;- def_positive[def_positive &gt; 0] #only taking positive 1st derivative values (aka positive slopes) pos_sum[k] &lt;- sum(def_pos) #adding up the positive 1st derivative values def_negative &lt;- c(values[(i - 15):i]) # vector of the 15 values to the left of the local min (negative 1st derivatives) def_neg &lt;- def_negative[def_negative &lt; 0] #only taking the negative slopes in the list neg_sum[k] &lt;- sum(def_neg) #adding up negative values neg_slope[k] &lt;- values[i - 1] #1st derivative value at i-1 (right before sign change from neg to pos) pos_slope[k] &lt;- values[i] #1st derivative at i (at the switch from neg to positive) index_of_closest[k] &lt;- i - 1 #index value of the last neg 1st derivative before the local min definition[k] &lt;- sum(abs(def_neg), abs(def_pos)) # higher the value, more pronounced the local min k &lt;- k + 1 } } return(data.frame(neg_value = neg_slope, pos_value = pos_slope, Index = index_of_closest, definition = definition, neg_def = neg_sum, pos_def = pos_sum)) } To begin, we load the necessary packages, the multispectral orthomosaic that contains the NIR (842nm) band, and the delineated crowns shapefile and create the folder where the shadow mask will be saved to: #Required Packages library(ggplot2) # to plot library(terra) # to work with the orthomosaics (rasters) library(dplyr) #for data manipulation library(tidyverse) # for data manipulation library(sf) # to work with the delineated crown polygons library(pracma) # for gradient/ derivative function library(LaplacesDemon) # is.multimodal function # Setting directory dir = &quot;Change to match your folder strucutre&quot; #directory where shadow mask folder will be made. This dir is also used in path names for the orhtomosaics. Change up paths throughout the code to call your data. # Reading in the multispectral orthomosaic ms_temp = rast(list.files(paste0(dir, &quot;metashape\\\\3_MS_ORTHO\\\\&quot;), pattern = &quot;.*MS_Calibrated.*_bestPanel.tif$&quot;, full.names = TRUE)) # here we read in the ortho that is in the set directory and contains &quot;MS_Calibrated&quot; in the name and end in &quot;_bestPanel.tif&quot;. We had multiple orthos in this folder, so did this to ensure the proper one was called. Change to match your ortho name, or remove the pattern if you only have one ortho in the defined path. # Reading in the shapefile containing delineated crowns and buffering inward by 5cm to limit any mixed pixels from neighboring vegetation dir_crowns &lt;- &quot;D:\\\\Sync\\\\Fdc_PR_Canoe\\\\Crowns.shp&quot; #path to crowns shapefile pols_spat = st_read(paste0(dir_crowns)) %&gt;% # reading in crown shp filter(!st_is_empty(.)) %&gt;% #removing empty st_buffer(dist = -.05) %&gt;% #buffering inward by 5cm vect() # Creating a folder for shadow masks Nir_shadow_folder_baseName &lt;- &quot;NIR_shadow_mask_localMinOrMax&quot; #name of the folder shadow masks will be written to. We have the folder name defined outside the folder creation step below so that we could change the folder name once without having to change it throughout the code. if (!dir.exists(paste0(dir, Nir_shadow_folder_baseName,&quot;\\\\&quot;))) { dir.create(paste0(dir,Nir_shadow_folder_baseName,&quot;\\\\&quot;)) } Next, the NIR band is selected from the multispectral orthomosaic. Here you can choose to: Crop the mulispectral othomosaic to the extent of the crowns shapefile. We have found this to be a good option for mature sites with minimal exposed ground. Mask the mulispectral othomosaic to the individual crowns. We have found this to be a good option for younger sites with lots of ground exposure. Below we crop the raster to the extent of the polygons and reformat from a raster to a vector of values: #Create a vector of near-infrared (NIR) values from the 10th multispectral band (842nm) NIR = ms_temp[[10]] %&gt;% #isolating the NIR (842nm) band crop(pols_spat) %&gt;% #cropping to the extent of the crown polygon shp clamp(upper = 50000, values = FALSE) %&gt;% as.vector() Density of the NIR values is calculated and plotted, after removing NA values. NIR_na &lt;- na.omit(NIR) #remove NA values from the NIR vector density_values &lt;- density(NIR_na) # calculates density of NIR values # Plot to see the distribution of NIR values plot(density_values) # check to see the distribution of NIR values (ie a visual check for whether or not a local min exists) At this point the threshold can be estimated manually from the graph however, this is not a feasible method when you have many data acquisitions, nor is it the most accurate method. Below we describe the method we used to create shadow masks for many acquisitions withouth having to visually inspect the NIR distributions. To computationally find the threshold we first need to decipher whether the NIR distribution is unimodal (one peak) or multimodal (more than one peak). To do so we use the is.multimodal function from the LaplacesDemon package. If the distribution was multimodal, we found the local minimum by: calculating the first derivatives fo the NIR density values and storing the derivatives in dy_dt applying the find_local_min function to the list of first derivatives to find minimums filtering out small minimums that are not true minimums but rather dips on a larger slope Identifying false multimodals and assigning the threshold value to be the local maximum Identifying the largest minimum (a.k.a. the local minimum) and setting that as the threshold value If the distribution was not multimodal (and therefore unimodal), we found the local maximum by: locating the NIR value that corresponds to the greatest density of values in the NIR distribution Note: for each method there are mode and thresh_name variables defined. These variables are used to annotate the histrogram made in the next step if(is.multimodal(NIR_na)){ # if the NIR distribution is multimodal, continue to the below steps dy_dt &lt;- pracma::gradient(density_values$y) #list of first derivatives of NIR vector zeros &lt;- find_local_min(dy_dt) # Finds index locations where slope switches from neg to post (local min) and ranks the intensity of each local min zeros_filtered &lt;- zeros[(zeros$pos_def &gt; 0),] # Filters rows with pos_def &gt; 0, filtering out small minimums on negative slopes (not true local mins) if(nrow(zeros_filtered)==0){#if empty, then detected a false multimodal distribution and defaulting to max as threshold value print(&quot;It is a false multimodal&quot;) max_density = max(density_values$y, na.rm = TRUE) threshold = density_values$x[which(density_values$y == max_density)] mode &lt;- &quot;Unimodal (False Multi)&quot; thresh_name &lt;- &quot;LocalMax&quot; }else{ zeros_local_min &lt;- zeros_filtered[which.max(zeros_filtered$definition), ] #isolating the largest local min # x_zeros &lt;- density_values$x[zeros_local_min$Index] #selecting NIR (aka x_mid) value that corresponds to the index value of the most defined local min threshold &lt;- density_values$x[zeros_local_min$Index] #selecting NIR (aka x_mid) value that corresponds to the index value of the most defined local min # Catching cases of &quot;inf&quot; returns: if (threshold &lt;= 0.7 &amp; threshold &gt; 0){#setting limits on the threshold to remove any thresholds from the tails of the distribution print(threshold) mode &lt;- &quot;Multimodal&quot; thresh_name &lt;- &quot;LocalMin&quot; }else{ print(&quot;Multimodal with org thresh &gt; 0.7&quot;) #This output usually indicates an error in the function, make sure to look at the NIR distribution to confirm this is in fact correct or if the code needs modification for a special case #This often indicates a false multimodal as well, and therefore the NIR value with the max frequency in the NIR distribution with be used as the threshold max_density = max(density_values$y, na.rm = TRUE) threshold = density_values$x[which(density_values$y == max_density)] mode &lt;- &quot;Unimodal (False Multi with org thresh &gt; 0.7)&quot; thresh_name &lt;- &quot;LocalMax&quot; } } }else{ #if the NIR vector is NOT multimodal, continue to the below steps #Finding NIR value with the greatest frequency in the distribution - this max value will be the threshold for unimodal distributions max_density = max(density_values$y, na.rm = TRUE) threshold = density_values$x[which(density_values$y == max_density)] print(threshold) thresh_name &lt;- &quot;LocalMax&quot; mode &lt;- &quot;Unimodal&quot; } Next, we plot a histogram of the NIR values and annotate it to contain: a vertical line at the threshold the threshold value the mode (i.e. unimodal verse mulitmodal) (hist = NIR %&gt;% #plotting a histogram of NIR values with a vertical red line for the defined threshold value as_tibble() %&gt;% ggplot() + geom_histogram(aes(x = NIR), bins = 150) + geom_vline(xintercept = threshold, color = &quot;red3&quot;) + labs(title = paste0(mode, &quot; , threshold: &quot;,round(threshold, digits = 2)))+ theme_bw()+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.title = element_text(size = 13,hjust = 0.75, vjust = -28))) ggsave(hist, #saving out the plot filename = paste0(dir, Nir_shadow_folder_baseName,&quot;\\\\&quot;, date_list[x], &quot;_NIR_shadow_hist_localMin_orMax.jpeg&quot;), device = jpeg, width = 8, height = 8) We then isolate the NIR band from the multispectral orthomosaic and filter the raster to set pixels with NIR values greater than the threshold to NA and those that are less than or equal to the threshold to 1. shadow_mask = ms_temp[[10]] #isolating the NIR band of the multispectral ortho shadow_mask[shadow_mask &gt; threshold] = NA #for NIR values &gt; threshold, make them NA shadow_mask[shadow_mask &lt;= threshold] = 1 #for NIR values &lt; or = to the threshold value, make them 1 # Writing out the shadow mask that has values of 1 for all pixels that will be masked out terra::writeRaster(shadow_mask, paste0(dir, Nir_shadow_folder_baseName,&quot;\\\\&quot;, date_list[x], &quot;_NIR_shadow_thresh&quot;,threshold ,&quot;_&quot;,thresh_name, &quot;.tif&quot;), overwrite = TRUE) Lastly, we apply the clump function to group adjacent pixels that represent shadows creating “clumps” a.k.a. shadow patches. These shadow patches are then converted into a dataframe containing a unique ID per shadow patch and the number of pixels within each patch. # Grouping adjacent pixels with the same value (i.e. representing shadowed areas) into distinct patches or clumps shadow_patches = raster::clump(raster::raster(shadow_mask), directions = 4) %&gt;% rast() # Summarizing the clumps obtained from the shadow patches raster. It contains two columns: #1. value: representing the unique ID of each clump: #2. count: representing the number of pixels within each clump clumps = data.frame(freq(shadow_patches)) Here we filter the shadow patches by a threshold to remove insignificant patches and write out the cleaned shadow mask for future use. The threshold num_pix is calculated by: setting an initial area threshold of 0.02m^2 (200cm^2) dividing the 0.02m^2 threshold by the area of a pixel to determine the threshold number of pixels a shadow patch must have to not be filtered out In our case, the resolution of the raster was just over ~3cm, giving us a threshold of ~23 pixels. # Calculating the threshold for the number of pixels that a clump must contain to be considered significant # It is calculated based on the desired area threshold (200 cm²) divided by the area of a single pixel num_pix = 0.02 / (res(shadow_patches)[1]^2) #0.02 represents 200cm² in meters flecks = clumps[clumps$count &gt; num_pix,] # remove clump observations with frequency smaller than the threshold flecks = as.vector(flecks$value) # record IDs from clumps which met the criteria in previous step new_mask = shadow_patches %in% flecks #keep clumps that have IDS in flecks new_mask[new_mask == 0] = NA # make clumps that are zero, NA #writing out a &#39;cleaned&#39; shadow mask terra::writeRaster(new_mask, paste0(dir, Nir_shadow_folder_baseName,&quot;\\\\&quot;, date_list[x], &quot;_NIR_shadow_thresh&quot;,threshold ,&quot;_&quot;,thresh_name, &quot;_mask2.tif&quot;), overwrite = TRUE) Below you can see that in this case the filter did not result in a large change in the shadow mask as there were minimal small patches to begin with. Here the white areas in the filtered mask image on the right are areas that will not be masked given that the mask did not contain enough pixels. This will result in a less patchy shadow mask. 4.3 Crown-level Vegetation Indicies Vegetation indices (VIs) are important tools for near-real time monitoring and function as proxies for health, productivity, structural changes, and stress. Below we describe a few commonly used VIs that will be calculated in this workflow: Normalized Difference Vegetation Index (NDVI) is of the oldest vegetation indices established in vegetation monitoring and is used as a metric of greenness. However, due to saturation of the chlorophyll absorption peak around 660-680nm at moderately low chlorophyll levels, NDVI is often not able to tease out small differences between healthy plants (Sims &amp; Gamon, 2002). To solve this issue, the red reflectance band can be replaced with a red edge reflectance band creating the normalized difference red edge index mentioned below. Normalized Difference Red Edge index (NDRE) normalizes a reflectance band in the red edge to a reflectance band in the near-infrared region to estimate chlorophyll content. This index is a derivative of NDVI that is sensitive to shifts in chlorophyll content at high concentrations (Clevers &amp; Gitelson, 2013; Evangelides &amp; Nobajas, 2020). Chlorophyll Carotenoid Index (CCI) is a proxy for the ratio of chlorophylls to carotenoids in pigment pools. It is often used as a metric for tracking the onset of the growing season and photosynthetic activity (Gamon et al., 2016). Photochemical Reflectance Index (PRI) can be used as a proxy for xanthophyll pigment epoxidation or changes in bulk seasonal pigment pool ratios depending on the timescale of analysis. Over a scale of milliseconds to minutes PRI can isolate the photoprotective conversion of violaxanthin into antheraxanthin and zeaxanthin within the xanthophyll cycle by normalizing the 531nm reflectance band to the 560nm reference reflectance band. However, on the seasonal scale the 560nm reflectance band no longer acts as a reference for isolating changes in xanthophyll pigments since it changes with bulk pigment shifts. Hence, seasonal PRI meaurments are instead used as another proxy for the ratio of carotenoid to chlorophyll pigments and are used to show changes in photosynthetic activity (Wong et al., 2020). Red Edge (RE) slope captures changes in chlorophyl content and structural health. Steep RE slopes generally indicate healthy vegetation while more gradual slopes often indicate stressed vegetation. This is due to the base of the RE slope sitting around a main chlorophyll absorption peak and the end of the RE slope sitting near the near-infrared (NIR) range. Hence, the more chlorophyll the lower the reflectance around the base of the RE slope and the greater the reflectance in the NIR, often attributed to healthy vegetative material, the steeper the overall slope of the RE (Sims and Gamon, 2002, Clevers and Gitelson) Green Chromatic Cordinate (GCC) is a measure of green reflectance relative to the total reflectance in the visible light portion on the electromagnetic spectrum. It is a metric of greeness that is often used as a proxy for vegetation health (Reid et al., 2016). 4.3.1 VI Workflow Below are code chunks that take the delineated crowns and multispectral orthomosaics and output a .rds file containing several popular vegetation indices at the crown-level. In this script we demonstrate how to calculate both mean and median values of vegetation indices per crown. We begin by loading the necessary packages: library(terra) # to work with the orthomosaics (rasters) library(dplyr) #for data manipulation library(tidyverse) # for data manipulation library(sf) # to work with the delineated crown polygons library(exactextractr) # for the exact_extract function Next, we read in the mulispectral orthomosaic and shadow mask. The shadow mask will work to remove shadowed pixels so they do not skew the values for the vegetation indices. See the section on shadow masks for a detailed workflow. # Load in multispectral orthomosaic ms_ortho = rast(list.files(paste0(dir, &quot;metashape\\\\3_MS_ORTHO\\\\&quot;), pattern = &quot;.*MS_Calibrated.*_bestPanel.tif$&quot;, full.names = TRUE)) #path to multispectral ortho # Reading in the NIR shadow mask shadow_mask = rast(list.files(paste0(dir, Nir_shadow_folder,&quot;\\\\&quot;), pattern = &quot;.*_NIR_shadow.*_thresh.*_mask2.tif$&quot;, full.names = TRUE))# NIR shadow mask # Below selects the root name of the multispectral ortho ie: &quot;Name_of_multispectral_ortho&quot; that will be used to name the multispectral shadow masked orthos created in the following steps ms_ortho_name_root &lt;- substr(names(ms_ortho)[1], 1, nchar(names(ms_ortho)[1]) - 2) #names(ms_ortho)[1] grabs the name for band 1 of the ms_ortho: ie: &quot;Name_of_multispectral_ortho_1&quot;, so that the substr function can selection the rootname as done above To speed up processing, we mask the shadow mask to only contain areas inside delineated crowns. The shadow mask is a raster containing values 1 (shadowed pixel) and NA (not shadowed pixel). The mask function below identifies pixels in the multispectral orthomosaic that align with shadowed pixels in the shadow mask and changes their value to NA. # Mask shadow mask to delineated crowns: shadow_mask &lt;- terra::mask(shadow_mask, pols) # Mask the multispectral ortho using the shadow mask ms_mask &lt;- terra::mask(ms_ortho, shadow_mask, maskvalues = 1, updatevalue = NA) #writing out the shadow masked multispectral ortho terra::writeRaster(ms_mask, paste0(dir, Nir_shadow_folder,&quot;\\\\&quot;,ms_ortho_name_root,&quot;_NirShadow_masked.tif&quot;), overwrite = TRUE) The shadow masked multispectral orthomosaic is then masked to the delineated crowns polygon to speed raster calculations in the next step. # Mask shadow masked raster to delineated crowns ms_mask &lt;- terra::mask(ms_mask, pols) # Write out the masked raster terra::writeRaster(ms_mask, paste0(dir, Nir_shadow_folder,&quot;\\\\&quot;,ms_ortho_name_root,&quot;_NirShadow_ms_mask_to_pols.tif&quot;), overwrite = TRUE) Here we calculate several vegetation indices using the multispectral orthomosaic from above with shadowed pixels and non-crown pixels removed. # List of band reflectances (R444 - R842) and index values that will be calculated per tree crown rast_list = list( # reflectance values R444 = ms_mask[[1]], R475 = ms_mask[[2]], R531 = ms_mask[[3]], R560 = ms_mask[[4]], R650 = ms_mask[[5]], R668 = ms_mask[[6]], R705 = ms_mask[[7]], R717 = ms_mask[[8]], R740 = ms_mask[[9]], R842 = ms_mask[[10]], # Near-infrared greenness mDatt = (ms_mask[[10]] - ms_mask[[8]]) / (ms_mask[[10]] - ms_mask[[6]]), NDVI = (ms_mask[[10]] - ms_mask[[6]]) / (ms_mask[[10]] + ms_mask[[6]]), # Chlorophyll NDRE1 = (ms_mask[[10]] - ms_mask[[7]]) / (ms_mask[[10]] + ms_mask[[7]]), NDRE2 = (ms_mask[[10]] - ms_mask[[8]]) / (ms_mask[[10]] + ms_mask[[8]]), NDRE3 = (ms_mask[[10]] - ms_mask[[9]]) / (ms_mask[[10]] + ms_mask[[9]]), EVI = (2.5 * (ms_mask[[10]] - ms_mask[[6]])) / (ms_mask[[10]] + (6 * ms_mask[[6]]) - ( 7.5 * ms_mask[[1]] + 1)), GCC = (ms_mask[[4]]+ ms_mask[[3]]) / (ms_mask[[1]] + ms_mask[[2]] + ms_mask[[3]]+ms_mask[[4]] + ms_mask[[5]] + ms_mask[[6]]), # Carotenoids, waxes ARI = (1 / ms_mask[[4]]) - (1 / ms_mask[[7]]), EWI9 = (ms_mask[[6]] - ms_mask[[8]]) / (ms_mask[[6]] + ms_mask[[8]]), # Carotenoids PRI = (ms_mask[[3]] - ms_mask[[4]]) / (ms_mask[[3]] + ms_mask[[4]]), CCI = (ms_mask[[3]] - ms_mask[[5]]) / (ms_mask[[3]] + ms_mask[[5]]), # Red edge RE_upper = (ms_mask[[9]] - ms_mask[[8]]) / 23, RE_lower = (ms_mask[[8]] - ms_mask[[7]]) / 12, RE_total = (ms_mask[[9]] - ms_mask[[7]]) / 35 ) rast_all = rast(rast_list) Below is an example of the resolution of the calculated vegetation indices. The image shows the chlorophyll carotenoid index (CCI) of a single crown across four summer time points. Greener pixels represent a higher ratio of chlorophylls to carotenoids. As you can see, the proportion of the crown with high CCI values increases from the May to July acquisitions, when productivity is likely at its highest, and decreases during the august acquisition suggesting lower productivity, likely due to end of summer drought conditions. Lastly, we use the exact_extract function to calculate crown-level statistics per vegetation index raster created above. Below we show an example of taking the mean and median crown values per index as well as count the number of pixels that were used for the index calculation. We do this as a data quality check so we can remove crowns that had very few pixels involved in their index calculation. Note: the append_cols parameter should be set to a list of column names that exist in pols (the shapefile of delineated crowns) that you would like to be appended to the dataframe containing vegetation indcies per tree crown. As you can see below we have attached many columns we found useful, however the only column necessary to append is a column in pols that represents a unique ID per tree crown. # Calculating Mean Index values per crown df_spectral_mean = exact_extract(rast_all, pols, fun = &quot;mean&quot;, append_cols = c(&quot;treeID&quot;,&quot;Edited&quot;,&quot;tag__&quot;,&quot;rep&quot;,&quot;blk&quot;,&quot;fam&quot;,&quot;fem&quot;)) # Calculating the number of non-masked pixels used for index calculation df_count = exact_extract(ms_mask[[1]], pols, fun = &quot;count&quot;, progress = TRUE, append_cols = c(&quot;treeID&quot;,&quot;Edited&quot;,&quot;tag__&quot;,&quot;rep&quot;,&quot;blk&quot;,&quot;fam&quot;,&quot;fem&quot;)) # Joining the mean index values df and the count values df df_spectral_mean_count &lt;- merge(df_spectral_mean,df_count, by = c(&quot;treeID&quot;,&quot;Edited&quot;,&quot;tag__&quot;,&quot;rep&quot;,&quot;blk&quot;,&quot;fam&quot;,&quot;fem&quot;)) # Saving out mean crown values + non masked pixel count to an rds file saveRDS(df_spectral_mean_count, paste0(dir_D,&quot;\\\\&quot;,updated_metrics_folder,&quot;\\\\&quot;, date_list[x], &quot;_NIRshadowMask_MeanCrownSpectralIndices.rds&quot;)) # Calculating Median Index values per crown df_spectral_median = exact_extract(rast_all, pols, fun = &quot;median&quot;, append_cols = c(&quot;treeID&quot;,&quot;Edited&quot;,&quot;tag__&quot;,&quot;rep&quot;,&quot;blk&quot;,&quot;fam&quot;,&quot;fem&quot;)) df_spectral_median_count &lt;- merge(df_spectral_median,df_count, by = c(&quot;treeID&quot;,&quot;Edited&quot;,&quot;tag__&quot;,&quot;rep&quot;,&quot;blk&quot;,&quot;fam&quot;,&quot;fem&quot;)) # Saving out median crown values + non masked pixel count to an rds file saveRDS(df_spectral_median_count, paste0(dir_D,&quot;\\\\&quot;,updated_metrics_folder,&quot;\\\\&quot;, date_list[x], &quot;_NIRshadowMask_MedianCrownSpectralIndices.rds&quot;)) References: Clevers, J. G. P. W., &amp; Gitelson, A. A. (2013). Remote estimation of crop and grass chlorophyll and nitrogen content using red-edge bands on Sentinel-2 and -3. International Journal of Applied Earth Observation and Geoinformation, 23(1), 344–351. https://doi.org/10.1016/J.JAG.2012.10.008 Evangelides, C., &amp; Nobajas, A. (2020). Red-Edge Normalised Difference Vegetation Index (NDVI705) from Sentinel-2 imagery to assess post-fire regeneration. Remote Sensing Applications: Society and Environment, 17, 100283. https://doi.org/10.1016/J.RSASE.2019.100283 Gamon, J. A., Huemmrich, K. F., Wong, C. Y. S., Ensminger, I., Garrity, S., Hollinger, D. Y., Noormets, A., &amp; Peñuelask, J. (2016). A remotely sensed pigment index reveals photosynthetic phenology in evergreen conifers. Proceedings of the National Academy of Sciences of the United States of America, 113(46), 13087–13092. https://doi.org/10.1073/pnas.1606162113 Reid, A. M., Chapman, W. K., Prescott, C. E., &amp; Nijland, W. (2016). Using excess greenness and green chromatic coordinate colour indices from aerial images to assess lodgepole pine vigour, mortality and disease occurrence. Forest Ecology and Management, 374, 146–153. https://doi.org/10.1016/J.FORECO.2016.05.006 Sims, D. A., &amp; Gamon, J. A. (2002). Relationships between leaf pigment content and spectral reflectance across a wide range of species, leaf structures and developmental stages. Remote Sensing of Environment, 81(2–3), 337–354. https://doi.org/10.1016/S0034-4257(02)00010-X Wong, C. Y. S., D’Odorico, P., Arain, M. A., &amp; Ensminger, I. (2020). Tracking the phenology of photosynthesis using carotenoid-sensitive and near-infrared reflectance vegetation indices in a temperate evergreen and mixed deciduous forest. New Phytologist, 226(6). https://doi.org/10.1111/nph.16479 "],["lidar-processing-workflow.html", "Chapter 5 LiDAR Processing Workflow 5.1 DJI Terra 5.2 Register the LiDAR to the DAP point cloud 5.3 Normalization and individual tree point clouds 5.4 Individual Tree Metrics", " Chapter 5 LiDAR Processing Workflow 5.1 DJI Terra Open DJI Terra and start a LiDAR Point Cloud Processing session Import the flight files, including imagery if you would like a colorized point cloud Below are the settings we use to process the point cloud: We use the deafult settings set by DJI with the exception of setting the DEM resolution to 10cm 5.2 Register the LiDAR to the DAP point cloud Convention is generally to register DAP to LiDAR however for this specific project it made more sense to register the LiDAR to the DAP because: We were flying biweekly multispectral and RGB imagery and only 2-3 LiDAR acquisitions a year. We could not load the LiDAR point cloud into Agisoft Metashape to use as a reference for registration however could register all DAP clouds to a reference DAP cloud in Agisoft. This allowed us to more easily register the many multispectral and RGB acquisitions to a defined template in Agisoft Metashape and use the exported point cloud of the template DAP to register the LiDAR in CloudCompare. This process worked well for us since our priority was centimeter-level registration of orthomosaics and LiDAR between many dates and across multiple sensors. 5.2.1 Prepare LiDAR for registration Below are snipets of a script written in LAStools that is used to ensure the LiDAR and DAP clouds are in the proper projection, filters out points in the LiDAR that are above a defined threshold, and clips the LiDAR to a boundary polygon to speed up registration by avoiding working with excess data in CloudCompare. The unparsed script can be found on the project GitHub, see GitHub link Ensures both the DAP point cloud and LiDAR file are in the proper projection (NAD83 UTM 10N in our case) path_to_L1_las is the path to the LiDAR .las file from DJI terra. path_to_write is where the path the projected .laz file will be written to. The files will be saved out with the same file name as the origianl LiDAR file with a “_nad83” suffix. “REM” works to comment out a line in LAStools as “#” does in R. REM Project DAP and save out as a .laz las2las -i path_to_DAP_las_file ^ -odir path_to_write ^ -odix _nad83 ^ -olaz ^ -nad83 ^ -utm 10north ^ -cpu64 ^ -v REM Project LiDAR and save out as a .laz las2las -i path_to_L1_las ^ -odir path_to_write ^ -odix _nad83 ^ -olaz ^ -nad83 ^ -utm 10north ^ -cpu64 ^ -v Drops points in the LiDAR point cloud above a user defined threshold to remove noisy points that are not from the canopy (i.e. due to air moisture, birds, etc.). To determine this height threshold you can plot a histogram of Z values or visualize the point cloud in a software that allows 3D point cloud visualization to ensure that the threshold will not result in any top canopy points being removed. Though we use CloudCompare for point cloud registration, we recommend Potree to visualize point clouds as it handles large point clouds quickly and with ease. Both Potree and CloudCompare are open-source software. If you do not have any point above the canopy or below the ground that need removing you can skip this step. path_to_projected_L1 is the path to the projected LiDAR .laz file from above REM dropping points above 160m (160m is the height cutoff for this dataset) las2las -i path_to_projected_L1 ^ -odir path_to_write ^ -odix _droppedPtsAbove160m ^ -olaz ^ -drop_z_above 160 ^ -cpu64 ^ -v Lastly we clip the LiDAR file to a boundary polygon of the site to remove excess surrounding data that can slow processing in CloudCompare. path_to_shp is the path to the site shapefile. path_to_projected_below160m_L1 is the path to .laz file that has been projected and filtered for points above a set threshold in steps 1 and 2 above. “-odix _clipped” will save the new laz file with the same name as the input file with “_clipped” attached to the end, change the “_clipped” to work with your naming system. lasclip -i path_to_projected_below160m_L1 -merged -path_to_shp -odir path_to_write -odix _clipped -olaz 5.2.2 Register LiDAR to DAP cloud in CloudCompare Import both the LiDAR and DAP clouds into CloudCompare (CC). This can take up to twenty minutes. Allow all for the first two warnings. Figure 5.1: Left: Apply all for this initial CloudCompare popup setting. Right: select ‘Yes to All’ for this second setting 5.2.2.1 Examine the point clouds for artifacts. The L1 will reflect flight lines when there is moisture in the air. Below is an example point cloud from a damp day on the coast just after a fog past through. For sake of the example, it was not filtered in LAStools with a height threshold. The residual fog patches seen above the canopy can be clipped out using the cross section or segment tool. 5.2.2.2 Rough Alignment The goal is to roughly align (move) the LiDAR to our reference “template” DAP dense cloud ahead of the ICP fine adjustment algorithm. Problem: Manually moving the LiDAR cloud to roughly align with the DAP cloud caused Cloud Compare to crash or hang when working with larger data sets. Solution: Use the Apply Transformation function in CloudCompare – Highlight the LiDAR layer in the DB tree - Hit ctrl T or Apply Transformation in the Edit menu. In this example the LiDAR is the lower. First apply a Z transformation. It is best to do this while viewing the edge of the plot. Here we applied a Z transformation of +6. Next Apply transformations in the x and y axis. Find a section of the clouds where you can easily see the alignment. In this case there is a single large leave tree in the centre of the plot. The LiDAR is the one on the left. Here we see the LiDAR is offset to the left (negative) on the Y(green) axis and up (positive) on the x(red) axis. Figure 5.2: Left: . Right: . Be careful not to rotate, only transform. In this case the LiDAR was shifted -5 in the X and +5 in the Y axis in total. The shift was done in three smaller iterations to achieve the alignment seen in the above screen capture. 5.2.2.3 Fine Registration: Iterative Closest Point (ICP) Highlight both clouds in the DB tree and apply the fine registration (ICP) algorithm using the following settings. Careful to set a max thread count that matches the resources you have available. Save out the registered LiDAR point clouds at the highest resolution 5.2.3 Rescale and tile the registered LiDAR: Below are snipets of a script written in LAStools that is used to project, rescale and tile the registered LiDAR point cloud. The unparsed script can be found on the project GitHub, see GitHub link Projecting the registered LiDAR to the proper projection (NAD83 UTM 10N). If you are working with already tiled data or multiple .laz/.las files on a multi-core computer than you can use the cores command shown below. Here we defined cores=4 which allows 4 cores to work on the command simultaneously on different files. If you are only working with one .laz/.las file at this point there is no need to specify the number of cores and the “^ -cores %cores%” following the “-v ^” should be removed from the below code Change “path_to_registered_lidar” to the path to the registered LiDAR data exported from CloudCompare. “*.las” takes the las file in that folder, change to the file name if have more than one .las in the folder and you want to specify a specific file. Change “path_to_write\\01_proj_NAD83” to the path you would like the new projected .laz file to be written to. set cores=4 las2las -i path_to_registered_lidar\\*.las ^ -odir path_to_write\\01_proj_NAD83 ^ -odix _nad83 ^ -olaz ^ -nad83 ^ -utm 10north ^ -cpu64 ^ -v ^ -cores %cores% Rescales the data which is necessary to later load into R for normalization and metric calculations. Registered LiDAR is exported from CloudCompare at the highest resolution which changes the scale of the data, hence rescaling the x,y,z to 0.01 is necessary to avoid warnings/errors in R “path_to_write” is in the input dir and the output dir because its the main folder we are now working in “path_to_write\\01_proj_NAD83*.laz” selects the .laz file in the “path_to_write\\01_proj_NAD83” folder, if there are more than one .laz file in your folder and you want to specify which to call, change the “*” to the name of the file. The output .laz file will be written to the “path_to_write\\02_rescaled” folder with the same name as the original file. las2las -i path_to_write\\01_proj_NAD83\\*.laz ^ -rescale 0.01 0.01 0.01 ^ -cpu64 ^ -utm 10north ^ -v ^ -odir path_to_write\\02_rescaled ^ -olaz Next the code indexes the LiDAR data. Indexing creates a “.lax” file for a given .las or .laz file that contains spatial indexing information. When this LAX file is present it will be used to speed up access to the relevant areas of the LAS/LAZ file for spatial queries. REM Indexing lasindex -i path_to_write\\02_rescaled\\*.laz Lastly tiling divides the point clouds into tiles to allow for parallel processing in the following R steps. “tile_size 15” sets the size of the tiles to 15m. “buffer 4” sets the size of the buffer surrounding the tiles to 4m. “flag_as_withheld” flags the buffer points so that they can be easily filtered out in the following steps in R. REM Creating 15m tiles lastile -i path_to_write\\02_rescaled\\*.laz ^ -tile_size 15 ^ -buffer 4 ^ -flag_as_withheld ^ -odir path_to_write\\03_tile ^ -olaz 5.3 Normalization and individual tree point clouds The below workflow includes explanations and code where applicable for each of the following: Creating a 10cm DTM from the registered LiDAR tiles Normalizing the tiles using the DTM Creating a 4cm CHM from the max Z values in each pixel Segmenting the tiles to only retain the top 25% of each tree using the crown polygons. The threshold will be site and age dependent. We set our threshold to the top 25% given that the trees were mature with crown closure and we were not confident the lower 75% of the point cloud did not contain any invading neighboring branches. Merging segmented tiles to create one large point cloud containing the top 25% of each tree Clipping the point cloud into individual point clouds per tree Note: The full code that can be found on the project GitHub runs through the above steps in a for loop. For sake of this guide we have rearranged the ordering of some steps (i.e. functions will be defined as we go in this guide, however are defined at the beginning in the full R script to allow the for loop to run). 5.3.1 Creating a DTM from the registered LiDAR tiles To being, when working with LiDAR data in R, an incredibly useful package is the lidR package. We highly recommend taking a look at the lidR bookdown for useful tips and examples on using the package. The packages required for our workflow are: library(tidyverse) library(sf) library(sp) library(spatial) library(raster) # working with raster data library(terra) library(lidR) # reading and processing LiDAR data library(sp) # defines and allows us to work with spatial objects library(nngeo) library(future) library(rmapshaper) library(concaveman) library(parallel) library(foreach) library(smoothr) library(ForestTools) library(gdalUtilities) library(exactextractr) library(alphashape3d) # Creates alpha shapes used to calculate crown volume library(lwgeom) library(dplyr) Here we are reading in the output tiles from the tiling and rescaling step above. We then drop the buffer points that were flagged as withheld in the LAStools tiling stage, set the new chunk buffer to 0.5m and set the opt_output_files to empty given that we do not want to save DTMs for the individual tiles but rather one for the entire site. dir &lt;- &quot;set_path_to_folders&quot; TILES = readLAScatalog(folder = paste0(dir, &quot;\\\\03_tile\\\\&quot;), filter = &quot;drop_withheld&quot;) opt_filter(TILES) &lt;- &quot;-drop_withheld&quot; # set filtering options for the LAScatalog object to drop withheld points opt_chunk_buffer(TILES) = .5 # set the buffer size for chunks in the LAScatalog object to 0.5m opt_laz_compression(TILES) = TRUE # enable LAZ compression for the LAScatalog object opt_output_files(TILES) = &quot;&quot; # set output file options for the LAScatalog object to empty opt_progress(TILES) = TRUE # enable progress tracking for processing Next we create a 10cm DTM using the tin() algorithm and smooth the raster by using the mean focal statistic and a focal window of 25 by 25 cells over the DTM. As the focal window shifts over the raster, it updates the pixel that sits at its center to the mean value within the 25 x 25 cell window. We then assigned the proper CRS and exported the raster as a tif. # Create a DTM DTM = grid_terrain(TILES, res = 0.1, tin(), full_raster = FALSE) %&gt;% # applying a focal operation to the DTM raster: computing the mean value within a moving window defined by a matrix. focal(w = matrix(1, 25, 25), # define a 25x25 window with all values as 1 fun = mean, # use the mean function to compute the focal statistic na.rm = TRUE, # remove NA values from computation pad = TRUE) # pad the edges of the raster with NAs to maintain the original extent crs(DTM) &lt;- CRS(&quot;+proj=utm +zone=10 +datum=NAD83&quot;) # assign a CRS to the raster using the proj4 string representation writeRaster(DTM, paste0(dir, &quot;\\\\04_RASTER\\\\&quot;, site, &quot;_DTM_0.1m.tif&quot;), overwrite = TRUE) #save the DTM 5.3.2 Normalizing the tiles using the DTM The R code below reads in the tiled registered LiDAR as a LAScatalog and normalizes the tiles to the DTM made in the previous step. The normalized tiles are then saved out and read back into R to filter out points below “ground” designated to be -0.25m. Here we save out the “cleaned” normalized tiles in a new folder, however for space saving reasons you can also overwrite the original normalized tiles by setting the “opt_output_files()” parameter in the filtering step to the same path as that set for the normalization step. # Read the tiles again, but with high-res parameters CTG = readLAScatalog(folder = paste0(dir, &quot;\\\\03_tile\\\\&quot;)) opt_chunk_buffer(CTG) = .5 # small buffer; we&#39;re just normalizing opt_laz_compression(CTG) = TRUE opt_filter(CTG) = &quot;-thin_with_voxel 0.01&quot; # 1cm voxel thinning opt_output_files(CTG) = paste0(dir, &quot;\\\\05_NORM\\\\{*}_NORM&quot;) #saving out normalized individual laz files with an extension of _NORM to the 05_NORM folder opt_progress(CTG) = TRUE # Normalizing the tiles in the catalog using the DTM, tiles will be saved to the location designated in the above &quot;opt_output_files&quot; as they are processed NORM = normalize_height(CTG, DTM, na.rm = TRUE)#normalizing the laz files in the CTG to the previously made DTM # Read in the normalized laz files and filter out points below -0.25 (below ground) NORM = readLAScatalog(paste0(dir, &quot;\\\\05_NORM\\\\&quot;)) opt_chunk_buffer(NORM) = 0 # no buffer, just filtering opt_laz_compression(NORM) = TRUE opt_filter(NORM) = &quot;-drop_z_below -.25&quot; #drop points below -25cm opt_output_files(NORM) = paste0(dir, &quot;\\\\06_NORM_clean\\\\{*}&quot;) #save the filtered normalized filed to the &quot;06...&quot; folder, here you can also choose to overwrite the original normalized laz files by setting the output location to the &quot;05_norm&quot; folder designated as the output in the normalization step opt_progress(NORM) = TRUE #show the progress NORM_clean = catalog_retile(NORM) # applying the filter to all files in the catalog 5.3.3 Creating a 4cm CHM from the max Z values A 4cm CHM was made using max Z values. We chose a resolution of 4cm as it gave us a good looking (no holes, no visible noise) high resolution CHM. We recommend testing out multiple resolutions at this stage to narrow down parameters that work for your data. # Making a 4cm resolution CHM CHM_max = grid_metrics(NORM_clean, #NORM_clean is the catalog the CHM is being made from res = 0.04, #4cm resolution func = ~max(Z)) #using max Z values crs(CHM_max) &lt;- CRS(&quot;+proj=utm +zone=10 +datum=NAD83&quot;) # assign a CRS to the raster using the proj4 string representation writeRaster(CHM_max, paste0(dir, &quot;\\\\04_RASTER\\\\&quot;, site, &quot;_CHM_max_0.04m.tif&quot;), overwrite = TRUE) #saving out the CHM 5.3.4 Segmenting tiles The normalized tiles were then segmented to only retain the top 25% of each tree. This was done using a shapefile (.shp) containing a polygon for each tree crown, see the chapter on crown delineation for a detailed workflow showing how to create crown polygons. The threshold set for segmentation will be site specific. We chose to only retain the top 25% of the point cloud for each tree in our mature (~25 years old) sites with crown closure in order to: 1. limit the chance of capturing invading branches in our LiDAR data 2. Capture a similar scope of the trees in both the LiDAR and photogrametic data, given that the aerial imagery is limited to branches that can be seen from an aerial perspective. A range of thresholds, from 50% (for ~10 year old trees that were around ~8m in height) to 80% (for ~4 year old trees that were around ~1m in height), were used for the younger sites we worked with depending on the proportion of the tree crown that is visible from an aerial, oblique perspective. To begin, we must first define the function that will segment the trees. Here “zq” is the threshold for the height segmentation and can either be set in the polys_to_las function (if you would like it to be the same value each time the function is run) or defined as a variable prior to initiating the function (use this option if the function will be called for different sites that have different zq values). The polys_to_las function works by: 1. Identifying points that fall within a tree crown and labeling the points with the unique treeID for that crown 2. Labels points within the crown that fall below the defined height threshold to have a treeID of zero 3. Filters to only keep points with a treeID value greater than zero Important: - If zq = 0.75, then the points in the bottom 75% of the tree will be removed, leaving a point cloud for the top 25% only. - “treeID” is a column in our dataset that functions as a unique identifier per tree within the site, ensure you change each instance of treeID to a column containing a unique identifying for your trees. The unique identifiers can come from the experimental site setup or the crown delineation step. # Function that will segment the LiDAR tiles to only retain points above a defined cutoff polys_to_las = function(chunk, zq = zq, polygons = pols) { #edit zq = zq here if you would like the function to always use the same zq value, ie zq = 0.75, otherwise leave zq = zq as you will be able to define zq in the next code chunk las = readLAS(chunk) if (lidR::is.empty(las)) { return(NULL) }# If the LAS file is empty, return NULL las2 = merge_spatial(las, polygons, &quot;treeID&quot;) # Merge the LAS points with the polygons based on the treeID attribute, change the &quot;treeID&quot; attribute to a unique identifier for the trees you are working with las_df = las2@data %&gt;% dplyr::group_by(treeID) %&gt;% dplyr::mutate(Zq999 = quantile(Z, 0.999)) %&gt;% # compute Zq999 (the 99.9th percentile of Z) for each tree dplyr::mutate(treeID = if_else(Z &gt; quantile(Z, 0.999) * zq, as.numeric(treeID), 0))# assign points below the top zq% of the tree height a treeID of zero to filter them out later las3 = las2 %&gt;% add_lasattribute(las_df$treeID, name = &quot;treeID&quot;, desc = &quot;treeID&quot;) %&gt;% #add treeID las a lasattribute filter_poi(treeID &gt; 0) #filter for points with a treeID greater than 0 if (lidR::is.empty(las3)) { return(NULL)#return NULL is the las3 is empty } else { return(las3)#return the las file that has a treeID associated with it and only contains points in the top zq% } } Next we read in the normalized files, set our zq threshold and run the tiles through the polys_to_las segmentation function using the catalog_apply function. # Read in the normalized cloud from the end of the last step NORM = readLAScatalog(paste0(dir, &quot;\\\\06_NORM_clean\\\\&quot;)) crs(NORM) &lt;- st_crs(26910) # setting CRS opt_chunk_buffer(NORM) = 0 # no buffer opt_laz_compression(NORM) = TRUE # compress output files to .laz objects opt_output_files(NORM) = paste0(dir, &quot;\\\\07_SEGMENTED\\\\{*}_SEGMENTED&quot;) # write output files to &quot;07_SEG...&quot; folder and name with suffix &quot;_SEGMENTED&quot; opt_progress(NORM) = TRUE # show progress zq = 0.75 # Height percentile to drop, here we will be filtering for the top 25% of the tree # New tiles have only segmented portions of tree crowns SEGMENTED = catalog_apply(NORM, polys_to_las) # apply the poly_to_las function to the NORM catalog 5.3.5 Merging segmented tiles to create one large point cloud containing the top defined height % of each tree This is done to facilitate clipping out individual tree point clouds in the next step. Note: The opt_chunk_size(SEGMENTED) = 10000 sets the new chunk size to 10,000 units of the CRS, in this case the chunk size is 10km x 10km. This is done to read in all the segmented tiles at one time so they can then be merged together into one tile. This value will be site specific however a very large value of 10km x 10km should work for the vast majority of sites. The catalog_retile function merges the point clouds into one large point cloud SEGMENTED = readLAScatalog(paste0(dir, &quot;\\\\07_SEGMENTED\\\\&quot;)) opt_chunk_buffer(SEGMENTED) = 0 #zero buffer opt_chunk_size(SEGMENTED) = 10000 #set the chunk size for the LAScatalog object to 10000 units of the CRS so that all laz files are merged into one large laz file opt_laz_compression(SEGMENTED) = TRUE opt_progress(SEGMENTED) = TRUE opt_output_files(SEGMENTED) = paste0(dir, &quot;\\\\08_MERGED\\\\&quot;, site, &quot;_HULLS_merged&quot;) #write output files to &quot;08_M...&quot; with the suffix &quot;_HULLS_merged&quot; # merge all the segmented trees into a single point cloud MERGED = catalog_retile(SEGMENTED) #apply the above opt_ commands print(&quot;merged&quot;) #print statement to show where the code is at 5.3.6 Clipping the point cloud into individual point clouds per tree This step results in individual tree point clouds that represent the top #% of the tree, where # is the height percentile defined in the segmentation step above. opt_output_files(MERGED) = paste0(dir, &quot;\\\\09_CROWNS\\\\&quot;, site, &quot;_fam{fam}_rep{rep}_tag{tag}_treeID{treeID}&quot;) #where the individual crown point clouds will be written to and the suffix they will have. Note values associated with the laz file will replace the names in {} CROWNS = clip_roi(MERGED, pols) #clip the MERGED laz file to individual crowns using the crown polygons Below we define the clean_crowns function that ensures that only points: - with the correct treeID remain in each individual tree point cloud. - that pass the defined height percentile threshold remain in the point cloud This function was added to double check that only the points that meet the criteria remain in the point clouds clean_crowns = function(chunk) { las = readLAS(chunk) #reading in las file if(lidR::is.empty(las)) return(NULL) #removing empty #ensuring all points in the cloud have the same/right tree ID treeID_true = as.numeric(names(sort(table(las@data$treeID), decreasing = TRUE))[1]) las2 = filter_poi(las, treeID == treeID_true) #(added to Sam&#39;s code) filter points below the 75% threshold that were not dropped previously las_df = las2@data %&gt;% dplyr::group_by(treeID) %&gt;% dplyr::mutate(Zq99 = quantile(Z, 0.99)) %&gt;% dplyr::mutate(Zq999 = quantile(Z, 0.999)) %&gt;% dplyr::ungroup() las3 = filter_poi(las2, Z &gt;= quantile(Z, 0.99)*zq) #only keeping points above 99%*zq height percentile las4 = filter_poi(las3, Z &lt;= quantile(Z, 0.999)) #removing points too high } ## CLEANING crowns CROWNS = readLAScatalog(paste0(dir, &quot;\\\\09_CROWNS\\\\&quot;), filter = &quot;-drop_withheld&quot;) #read in the clipped crown point clouds opt_chunk_size(CROWNS) = 0 # processing by files opt_laz_compression(CROWNS) = TRUE opt_chunk_buffer(CROWNS) = 0 # no buffer opt_wall_to_wall(CROWNS) = TRUE # disable internal checks to ensure a valid output opt_output_files(CROWNS) = paste0(dir, &quot;\\\\10_CROWNS_clean\\\\{*}&quot;) #location for output files print(&quot;cleaning crowns&quot;) CROWNS_clean = catalog_apply(CROWNS, clean_crowns) #applying the clean_crowns function on the CROWNS 5.4 Individual Tree Metrics This script calculates the following metrics for each tree: Height percentiles: 99, 97.5, 95, 92.5, mean height To create new height percentiles, change the “X” here: as.numeric(quantile(Z, X, na.rm = TRUE)) Volumes: convex and two variations of concave (α = 1, α = 0.5) Volume calculations are dependent on the alpha shape created. The larger the alpha parameter used to create the alpha shape the more convex the hull shape, whereas the smaller the alpha the more concave the shape. Very small alpha values can result in capturing holes in the point cloud as it will hug tightly to the points The alpha shape for convex (lines curving outward) hull volume requires an α = inf and represents the tightest convex boundary around all points in the point cloud Concave volume often uses α = 1, however below we also calculate concave volume with α = 0.5 for a more defined shape To create a new alpha shape with a new alpha value change the “α_value” below: alphashape3d::ashape3d(x = a3d, alpha = “α_value, pert = TRUE,eps = 1e-09) Crown complexity: rumple, canopy rugosity ratio (CRR), coefficient of variation of height Rumple measures the ratio of canopy surface area to ground surface area to give an idea of complexity. Canopy Rugosity Ratio (CRR) quantifies the vertical complexity of the canopy or crown. Coefficient of variation of height quantifies the variability of points in the point cloud. This metric is often used to look at structural diversity of a stand, however we added it to look at variation in point distribution within individual trees, to give an idea of variation in vegetation density between crowns. Z = las@data$Z # Z values of each point in the .laz cloud chm = grid_metrics(las, func = ~max(Z,na.rm = TRUE), res = 0.05) # 5cm CHM with max Z values #grid_metrics, replaced by pixel_metrics :https://github.com/r-lidar/lidR/releases # Extract X, Y, and Z coordinates from the LAS data and create a matrix &#39;a3d&#39; a3d &lt;- cbind(las@data$X, las@data$Y, las@data$Z) # Center the points around the origin (0,0,0) by subtracting the mean of each dimension a3d[,1] = a3d[,1] - mean(a3d[,1],na.rm = TRUE) #center x values a3d[,2] = a3d[,2] - mean(a3d[,2],na.rm = TRUE) #center y values a3d[,3] = a3d[,3] - mean(a3d[,3],na.rm = TRUE) #center z values # sams orginal code did not center the Z values, but that is the only way we found to not get an error thrown in the ashape3d function. # Generate different shapes using the alphashape3d package shape_convex = alphashape3d::ashape3d(x = a3d, alpha = Inf, pert = TRUE,eps = 1e-09)# Compute a convex hull using alpha = Inf (convex hull) #USED PERT= TRUE : https://cran.r-project.org/web/packages/alphashape3d/alphashape3d.pdf shape_concave = alphashape3d::ashape3d(x = a3d, alpha = 1, pert = TRUE,eps = 1e-09)# Compute a concave hull using alpha = 1 (concave hull) shape_a05 = alphashape3d::ashape3d(x = a3d, alpha = 0.5, pert = TRUE,eps = 1e-09)# Compute a shape for alpha = 0.5 (balanced between convex and concave) structural_metrics_df &lt;- data.frame( treeID = unique(las@data$treeID), tag = tag_value, n_points = length(las@data$Z), # Crown height Zq99 = as.numeric(quantile(Z, 0.990,na.rm = TRUE)),# 99th percentile Zq975 = as.numeric(quantile(Z, 0.975,na.rm = TRUE)), # 97.5th percentile Zq95 = as.numeric(quantile(Z, 0.95,na.rm = TRUE)),# 95th percentile Zq925 = as.numeric(quantile(Z, 0.925,na.rm = TRUE)), # 92.5th percentile Z_mean = mean(Z,na.rm = TRUE), #mean crown height # Crown volume vol_convex = alphashape3d::volume_ashape3d(shape_convex), # convex volume vol_concave = alphashape3d::volume_ashape3d(shape_concave), # concave volume vol_a05= alphashape3d::volume_ashape3d(shape_a05), #volume with alpha = 0.5 (balance between concave and convex) # Crown complexity CV_Z = sd(Z,na.rm = TRUE) / mean(Z,na.rm = TRUE),# Compute the coefficient of variation (CV) of Z values, representing the variability of heights relative to the mean height rumple = lidR::rumple_index(chm), #rumple: ratio of canopy outer surface area to ground surface area as measured by the CHM and DTM CRR = (mean(Z,na.rm = TRUE) - min(Z,na.rm = TRUE)) / (max(Z,na.rm = TRUE) - min(Z,na.rm = TRUE))# Compute the Canopy Rugosity Ratio (CRR), # representing the ruggedness or roughness of the canopy surface ) #Create structural metrics folder if (!dir.exists(paste0(dir,&quot;\\\\10_CROWNS_clean\\\\Structural_metrics\\\\&quot;))) { dir.create(paste0(dir,&quot;\\\\10_CROWNS_clean\\\\Structural_metrics\\\\&quot;), recursive = TRUE) } #writing out .rds files saveRDS(structural_metrics_df, paste0(dir,&quot;\\\\10_CROWNS_clean\\\\Structural_metrics\\\\&quot;,name,&quot;_&quot;, date,&quot;_structuralMetrics_tag&quot;,tag_value,&quot;.rds&quot;)) #USED PERT= TRUE : https://cran.r-project.org/web/packages/alphashape3d/alphashape3d.pdf Below are a individual tree point cloud (left), clipped to the top 25% of the tree and its corresponding alphashape used for volume (right). Figure 5.3: Left: . Right: . . "],["micasense-irradiance-correction.html", "Chapter 6 MicaSense Irradiance Correction 6.1 Background 6.2 The Problem 6.3 Is Correction Needed? 6.4 Correcting Irradiance Values for Micasense Cameras", " Chapter 6 MicaSense Irradiance Correction 6.1 Background The MicaSense system measures radiance at the camera, which needs to be transformed into surface reflectance. The basic relationship is given by: \\[ R = \\frac{L}{I_{\\text{hor}}} \\] where \\(L\\) is the at-camera radiance and \\(I_{\\text{hor}}\\) is the horizontal irradiance. 6.1.1 Deriving Horizontal Irradiance Below we go through the derivation to compute \\(I_{\\text{hor}}\\). The MicaSense system includes both the cameras and a downwelling light sensor (DLS). Since the DLS is tilted at the angle of the drone during flight and the sun is rarely directly overheard the sensor, the DLS measures non-horizontal irradiance, referred to as spectral irradiance (\\(I_{\\text{spec}}\\)). Spectral irradiance is a function of direct irradiance (\\(I_{\\text{direct}}\\)), scattered irradiance (\\(I_{\\text{scattered}}\\)), and the angle between the sun and the sensor ( \\(\\theta_{\\text{sun-sensor}}\\)): \\[ I_{\\text{spec}} = I_{\\text{direct}} \\cdot \\cos(\\theta_{\\text{sun-sensor}}) + I_{\\text{scattered}} \\] If we take \\(r\\) to be the ratio of scattered to direct irradiance, \\(r = \\frac{I_{\\text{scattered}}}{I_{\\text{direct}}}\\), we can substitute \\(I_{\\text{scattered}} = r \\cdot I_{\\text{direct}}\\) into the equation: \\[ I_{\\text{spec}} = I_{\\text{direct}} \\cdot \\cos(\\theta_{\\text{sun-sensor}}) + r \\cdot I_{\\text{direct}} \\] Next, by factoring out \\(I_{\\text{direct}}\\) from the equation: \\[ I_{\\text{spec}} = I_{\\text{direct}} \\cdot \\left( \\cos(\\theta_{\\text{sun-sensor}}) + r \\right) \\] We can solve for \\(I_{\\text{direct}}\\) by dividing both sides of the equation by \\(\\cos(\\theta_{\\text{sun-sensor}}) + r\\): \\[ I_{\\text{direct}} = \\frac{I_{\\text{spec}}}{\\cos(\\theta_{\\text{sun-sensor}}) + r} \\] If we assume that \\(I_{\\text{hor}}\\) can be expressed in terms of \\(I_{\\text{direct}}\\) with a similar function involving the zenith angle, we can express this as: \\[ I_{\\text{hor}} = I_{\\text{direct}} \\cdot \\left( \\cos(\\theta_{\\text{zenith}}) + r \\right) \\] Substituting the equation for \\(I_{\\text{direct}}\\) into the equation for \\(I_{\\text{hor}}\\) gives: \\[ I_{\\text{hor}} = \\frac{I_{\\text{spec}}}{\\cos(\\theta_{\\text{sun-sensor}}) + r} \\cdot \\left( \\cos(\\theta_{\\text{zenith}}) + r \\right) \\] 6.1.2 Calculating Reflectance Finally, substituting \\(I_{\\text{hor}}\\) into the equation for reflectance, we obtain: \\[ R = L \\cdot \\frac{\\cos(\\theta_{\\text{sun-sensor}}) + r}{I_{\\text{spec}} \\cdot \\left( \\cos(\\theta_{\\text{zenith}}) + r \\right)} \\] Thus we see that reflectance is highly dependent on the ratio of scattered to direct irradiance as well as the sun-sensor angle. This is important for understanding the issue we have found when working with the DLS2 and the MicaSense MX Dual camera system. 6.2 The Problem The issue we have noticed is that the sun-sensor angle can be absurdly high or low, especially in strong illumination conditions. This leads to equally absurd, sometimes impossible, values for direct and scattered irradiance, that can be multiples higher than the direct solar irradiance. To begin, let us first look at some main differences between the legacy DLS1 and the current DLS2 system. The DLS1: calculates the sun zenith angle using the GPS location of the drone and the time of acquisition calculates the sun-sensor angle from the yaw/pitch/roll of the drone The DLS2: directly measures the sun sensor angle and the direct and diffuse irradiance components using a proprietary method that takes in information from the light sensors on the surface of the DSL2 6.3 Is Correction Needed? Deciding whether data needs to be corrected can be challenging. We have found that looking at these three questions can be helpful: 1) Do the sun sensor angles make sense? 2) Does the relationship between direct and horizontal irradiance make sense? 3) Does the scattered: direct ratio look right for the lighting conditions of that day? Below we have plotted the sun sensor angle (top left), the position of each photo (bottom left) and the irradiance values (right) from the exif data of the Micasense images. We have used the yaw values to identify flight lines (blue and green) and roll to filter for images taken when the drone is turning (red). This data was captured on June 25th, 2024 on Vancouver Island in British Columbia, Canada. As seen by the black horizontal line in the above figure, the solar zenith angle at the time of the flight was around 30°. 6.3.1 Do the sun sensor angles make sense? Solar zenith angle is the angle between the normal (vertical from the Earths surface) at the location of interest and the position of the sun. Given the tilt of the Earth, the solar zenith reaches its maximum (i.e. greatest angle between the sun and the normal to the Earths surface) in the winter, as the northern hemisphere is tilted away from the sun, and its minimum in the summer when the northern hemisphere is tilted towards the sun. The solar zenith also changes throughout the day, with its minimum occurring at solar noon, a.k.a. when the sun is at its highest. This is why it is suggested to fly within +/- 2 hours of solar noon to avoid shadows in your imagery. Below we have graphed solar zenith angles over the winter solstice (December 24, 2024), summer solstice (June 21, 2024) and the date of the flight acquisition (June 25th, 2024) that will be used in this example to demonstrate the daily and seasonal change in values. The gray vertical bar highlights the flight time, which was just within the +/- 2 hours of solar noon bounds and shows the solar zenith at around 30°, as we saw in the above figure. Now that we’ve refreshed our understanding of the solar zenith, let’s dive into the sun sensor angle. The sun sensor angle is, as it sounds, the angle between the normal (perpendicular) of the sensor and the direction of the sun. Given that drone flights occur relatively close to the ground compared to the distance from the Earth to the sun, this difference is often negligible. Therefore, the solar zenith angle can serve as a rough estimate of the sun sensor angle for a sensor that is parallel to the Earth’s surface - making it a helpful check to see if the sun sensor angle recorded by the DLS2 is within a reasonable range. However, drones often fly at an angle, which can cause a cyclical pattern in sun sensor angles depending on flight direction. For example, let’s take a look at the diagram below. When the drone is flying in the general direction of the sun (left), the forward tilt of the drone mid-flight results in a smaller sun sensor angle compared to when the drone is flying away from the general direction of the sun, where the tilt of the drone away from the sun increases the sun sensor angle (right). This pattern of sun sensor angle dependence on drone orientation relative to the sun can be see for the June 25th flight. Below are the uncorrected sun sensor angles, derived from the DLS2 sensor located on the top of the drone, colored by flight line direction. Here red indicates turning and blue and green are opposite flight directions. Now that we understand the reason behind the cyclical pattern of the sun sensor angles, we can use the solar zenith value (plotted as the horizontal black line) to make an informed decision on whether or not the values need correcting. To help this decision we recommend plotting the DLS2 sun sensor values and the sun sensor values calculated with the DLS1 method [LINK] which will be referred to as the “corrected” values. The dashed horizontal lines are the moving averages for the DLS1 and DLS2. Here the moving average of the DLS2 is at times almost 10°lower than the solar zenith, whereas the moving average of the DLS1 (corrected method) is more or less consistently 2-3° lower than the solar zenith. Given this data, we would conclude that the DLS1 method provides more accurate sun sensor angles compared to the DLS2. We will now continue to look at irradiance to confirm if correction is needed. 6.3.2 Does the relationship between direct and horizontal irradiance make sense? First, lets define direct and horizontal irradiance with respect to the DLS: Direct Irradiance: The direct component of the sunlight reaching the sensor’s surface that is not being scattered. On a clear sunny day, this component is high. Scattered Irradiance: The sunlight that is scattered by particles in the atmosphere. This component is generally weaker on sunny days and higher on overcast days. Horiztonal Irradiance: The total irradiance on a horizontal surface, including both direct sunlight (projected onto the horizontal plane) and diffuse light. Hence, if the sun is directly above the sensor and it is a clear sunny day, the direct irradiance component will likely be quite close to the horizontal irradiance, however still lower given that horizontal irradiance inlcudes diffuse light as well. Below, we have plotted the direct and horizontal irradiance from the DLS2 and values calculated with the DLS1 method. We can see that the direct irradiance from the DLS2 is greater than the horizontal irradiance, which is physically impossible. In contrast, the direct irradiance from the DLS1 method is approximately 50 W/m²/nm less than the horizontal irradiance. Explanations for the lower direct irradiance from the DLS1 compared to the horizontal irradiance include: The ~30° sun sensor angle, which reduces the effective direct component captured by the sensor. The portion of scattered irradiance in the horizontal irradiance signal that is not included in the direct irradiance value. From this graph, we would recommend correcting the data using the DLS1 method outlined below 6.3.3 Does the scattered: direct ratio make sense for the lighting conditions of that day? For a clear sunny day when the sun is around its peak, direct radiation generally accounts for ~85% of the total insolation with scattered radiation accounting for the remaining ~15%. For an completely overcast day, scattered radiation contributes to 100% of solar radiation. Hence for a clear sunny day in the summer a ratio of 1:6 scattered to direct irradiance is often used as an estimate. This means that direct irradiance is around 6 times stronger than the scattered irradiance. The data plotted below from June 25th was captured on a sunny day with occasional clouds. The scattered: direct ratio for the data plotted above is 0.09 for the DLS2 and 0.46 for the DLS1. DLS2: The ratio of 0.08 scattered:direct components from the DSL2 states that the direct irradiance during the flight was 12.5 time greater than the scattered irradiance, or in other words that 91% of the total radiation reaching the earth was direct radiation and 9% was scattered radiation. DLS1: The ratio of 0.46 scattered:direct components from the DLS1 states that the direct component was 2.17 time greater than the scattered component, hence 46% of the solar radiation was from the scattered component and the remaining 54% from the direct component. Given that the flight on June 25th was flown just at the end of the 2 hour solar noon window under a sunny sky with the occasional cloud, we would assume that the scattered component would be greater than the minimum ~15% for fully clear skies at solar noon. Hence a value of 46% scattered radiation could be the result of extra scattering by the occasional clouds and a slight increase in scattering from a greater solar zenith angle. Overall the value of 0.46 is more logical than the value of 0.09, since 0.09 implies less scattered radiation than the generally accepted minimum scattered component percentage on a clear sunny day. Overall, we recommend correcting this flight using the DLS1 method. The DLS1 corrections are done on individual photos, if the orthomosaic has already been built, no need to rebuild the dense cloud, you can re-upload the imagery, re-calibrate, and re-build the orthomosaic. 6.4 Correcting Irradiance Values for Micasense Cameras This chapter explains the process of correcting irradiance values for Micasense cameras using R. The correction involves several steps, including filtering and mutating data, calculating solar angles, and estimating scattered to direct light ratios. Each step is crucial for ensuring accurate irradiance readings, which are essential for downstream analyses. The code was taken largely from Micasense’s GitHub and converted into R to use in our workflow. To fix the sun-sensor angle and irradiance values we have taken the methodology used by the DLS1. The process involves: Using the GPS location and time of acquisition to determine the solar zenith angle, and the yaw/pitch/roll of the drone to determine the sun sensor angle Generating a rolling regression of the relationship: \\(I_{\\text{spec}} = I_{\\text{direct}} \\cdot \\cos(\\theta_{\\text{sun-sensor}}) + I_{\\text{scattered}}\\) to determine the direct/ scattered irradiance ratio, \\(r\\), and only keeping realistic models with good fits Computing the horizontal irradiance using \\(I_{\\text{hor}} = I_{\\text{direct}} \\cdot (cos(\\theta_{\\text{zenith}}) + r)\\) Editing the exif data of the images so that the imagery can be processed by Metashape with the corrected irradiance values 6.4.1 Reading Exif Data First, we read in the exif data of the imagery we want to correct using the exifr package. # This code was written to be within a for loop that looped through many acquisition dates. Hence the path names used will be in the form &quot; paste0(dir, date,&quot;\\\\1_Data\\\\&quot;,MS_folder_name,&quot;\\\\&quot;...etc)&quot;. Change these path names to match your data library(exifr) dir &lt;- &quot;directory to folder holding folders representing each flight aquisition date&quot; #change to match your data MS_folder_name &lt;- &quot;Micasense&quot; #change to match the name of your folder holding the folders of imagery from the Micasense camera band_length &lt;- 10 # 10 bands for Micasense RedEdge-MX Dual #Loops through bands for (j in 1:band_length){ # bands 1 through 10 pics = list.files(paste0(dir, date,&quot;\\\\1_Data\\\\&quot;,MS_folder_name,&quot;\\\\&quot;), #path the original data that will NOT be written over pattern = c(paste0(&quot;IMG_...._&quot;, j, &quot;.tif&quot;), paste0(&quot;IMG_...._&quot;, j, &quot;_&quot;, &quot;.tif&quot;)), recursive = TRUE, full.names = TRUE) # list of directories to all images if(!dir.exists(paste0(dir,date,&quot;\\\\1_Data\\\\&quot;,MS_folder_name,&quot;\\\\CSV\\\\&quot;))){ #creating CSV folders dir.create(paste0(dir,date,&quot;\\\\1_Data\\\\&quot;,MS_folder_name,&quot;\\\\CSV\\\\&quot;))} mask_images &lt;- list.files(paste0(dir,date,&quot;\\\\2_Inputs\\\\metashape\\\\MASKS\\\\&quot;,MS_folder_name,&quot;\\\\&quot;)) #path to folder with masks that are exported from metashape mask_img_names &lt;- gsub(&quot;_mask\\\\.png$&quot;, &quot;&quot;, mask_images) # removes the suffix &quot;_mask.png&quot; from each element in the &#39;mask_images&#39; list, ie : IMG_0027_1_mask.png becomes : IMG_0027_1 print(paste0(&quot;example of mask name: &quot;,mask_img_names[[1]])) #print to ensure your naming is correct, the list of mask_img_names will be used to identify panel images in the micasense image # Loops through images in each band for (i in 1:length(pics)){ #for each image in Micasense_Cleaned_save, one at a time pic = pics[i] pic_root &lt;- gsub(&quot;.*/(IMG_.*)(\\\\.tif)$&quot;, &quot;\\\\1&quot;, pic)# The pattern captures the filename starting with &quot;IMG_&quot; and ending with &quot;.tif&quot;.Ie: IMG_0027_1.tiff becomes IMG_0027_1 print(pic_root) if (substr(pic_root,11,11) == &quot;0&quot;){ # Distinguishes between _1 (band 1) and _10 (band 10), ie IMG_0027_1 verse IMG_0027_10 band = substr(pic_root,10,11) # for _10 } else { band = substr(pic_root,10,10) # for _1, _2, _3, ... _9 (bands 1-9) } img_exif = exifr::read_exif(pic) # read the XMP data print(paste0(date, &quot; band &quot;, j, &quot; &quot;, i, &quot;/&quot;, length(pics))) # keep track of progress # Creating df with same column names as exif data if (i == 1){ # If it&#39;s the first image, make new df from XMP data exif_df = as.data.frame(img_exif)%&gt;% mutate(panel_flag = ifelse( gsub(&quot;.*/(IMG_.*)(\\\\.tif)$&quot;, &quot;\\\\1&quot;, SourceFile) %in% mask_img_names, 1, 0)) # Give a value of 1 if the pic root name is in the list of mask root names and is therefore a calibration panel, give 0 otherwise (ie not a panel) } else { # Add each new image exif to dataframe exif_df = merge(exif_df, img_exif, by = intersect(names(exif_df), names(img_exif)), all = TRUE)%&gt;% mutate(panel_flag = ifelse( gsub(&quot;.*/(IMG_.*)(\\\\.tif)$&quot;, &quot;\\\\1&quot;, SourceFile) %in% mask_img_names, 1, 0)) # Give a value of 1 if the pic root name is in the list of mask root names and is therefore a calibration panel, give 0 otherwise (ie not a panel) } } saveRDS(exif_df, paste0(dir,date, &quot;\\\\1_Data\\\\&quot;,MS_folder_name,&quot;\\\\CSV\\\\XMP_data_&quot;, date, &quot;_&quot;, j, &quot;.rds&quot;)) # save output for each band, set path #binding all bands into one dataframe if (j == 1){ df_full = exif_df }else{ # Add missing columns to xmp_all and fill with NA values df_full= rbind(df_full, exif_df) } saveRDS(df_full, paste0(dir,date,&quot;\\\\1_Data\\\\&quot;,MS_folder_name,&quot;\\\\CSV\\\\XMP_data_&quot;, date,&quot;_AllBands.rds&quot;)) #.rds file containing the original exif data if all images for the flight } Read in the exif data for all bands. xmp_all &lt;- readRDS(paste0(dir,date,&quot;\\\\1_Data\\\\&quot;,MS_folder_name,&quot;\\\\CSV\\\\XMP_data_&quot;, date,&quot;_AllBands.rds&quot;)) 6.4.2 Calculating Solar Zenith Anlge Below we convert the DateTimeOrginal column from the exif data to a date object and use the date object and average lat and lon to calculate solar angle. xmp_all_filtered &lt;- xmp_all %&gt;% mutate(Date_time = ymd_hms(DateTimeOriginal), BandName_Wavelength = paste0(BandName, &quot;_&quot;, CentralWavelength), Date = as.Date(Date_time), Time = format(Date_time, format = &quot;%H:%M:%S&quot;), img_name = str_split(FileName, &quot;\\\\.tif&quot;, simplify = TRUE)[, 1], img_root = sub(&quot;_(\\\\d+)$&quot;, &quot;&quot;, img_name)) %&gt;% drop_na(DateTimeOriginal) %&gt;% dplyr::mutate( site_avg_lat = median(GPSLatitude, na.rm = TRUE), site_avg_long = median(GPSLongitude, na.rm = TRUE), solar_angle = photobiology::sun_zenith_angle(time = ymd_hms(DateTimeOriginal), geocode = tibble::tibble(lon = unique(site_avg_long), lat = unique(site_avg_lat), address = &quot;Greenwich&quot;))) Next, we check for any missing timestamps in image metadata as this usually indicates that the image was not saved properly and can’t be opened which will throw errors later in the workflow. In our experience, these images are few and far between, so we remove them from the analysis. We recommend manually checking that the image is in fact corrupted before removing it from analysis. missing_imgs &lt;- xmp_all[is.na(as.Date(xmp_all$CreateDate, format = &quot;%Y:%m:%d&quot;))]$FileName missing_imgs_roots &lt;- sub(&quot;_[^_]*$&quot;, &quot;&quot;, missing_imgs) xmp_all_filtered_mis &lt;- xmp_all_filtered %&gt;% #removing images with missing date information filter(!FileName %in% c(missing_imgs)) 6.4.3 Caluclating Sun-Sensor Angles Here we explain the difference between the archived DLS1 method and the current DLS2 method. DSL1: The following code has been taken from Micasense’s GitHub and coded in R to match our workflow compute_sun_angle &lt;- function(SolarElevation, SolarAzimuth, Roll, Pitch, Yaw) { ori &lt;- c(0, 0, -1) SolarElevation &lt;- as.numeric(SolarElevation) SolarAzimuth &lt;- as.numeric(SolarAzimuth) Roll &lt;- as.numeric(Roll) Pitch &lt;- as.numeric(Pitch) Yaw &lt;- as.numeric(Yaw) elements &lt;- c(cos(SolarAzimuth) * cos(SolarElevation), sin(SolarAzimuth) * cos(SolarElevation), -sin(SolarElevation)) nSun &lt;- t(matrix(elements, ncol = 3)) c1 &lt;- cos(-Yaw) s1 &lt;- sin(-Yaw) c2 &lt;- cos(-Pitch) s2 &lt;- sin(-Pitch) c3 &lt;- cos(-Roll) s3 &lt;- sin(-Roll) Ryaw &lt;- matrix(c(c1, s1, 0, -s1, c1, 0, 0, 0, 1), ncol = 3, byrow = TRUE) Rpitch &lt;- matrix(c(c2, 0, -s2, 0, 1, 0, s2, 0, c2), ncol = 3, byrow = TRUE) Rroll &lt;- matrix(c(1, 0, 0, 0, c3, s3, 0, -s3, c3), ncol = 3, byrow = TRUE) R_sensor &lt;- Ryaw %*% Rpitch %*% Rroll nSensor &lt;- R_sensor %*% ori angle &lt;- acos(sum(nSun * nSensor)) return(angle) } SSA_xmp_all_filtered &lt;- xmp_all_filtered %&gt;% rowwise() %&gt;% mutate(SunSensorAngle_DLS1_rad = compute_sun_angle(SolarElevation, SolarAzimuth, Roll, Pitch, Yaw), SunSensorAngle_DLS1_deg = SunSensorAngle_DLS1_rad * 180 / pi) saveRDS(SSA_xmp_all_filtered,paste0(dir,date,&quot;\\\\1_Data\\\\&quot;,MS_folder_name,&quot;\\\\CSV\\\\XMP_&quot;, date,&quot;_with_SSA.rds&quot;)) # Save XMP rds file with sun sensor angle added DLS2: SSA_xmp_all_filtered$SunSensorAngle_DLS2_rad &lt;- sapply(xmp_all_filtered$EstimatedDirectLightVector, function(vec) acos(-1 * as.numeric(vec[[3]]))) SSA_xmp_all_filtered$SunSensorAngle_DLS2_deg &lt;- as.numeric(SSA_xmp_all_filtered$SunSensorAngle_DLS2_rad) / pi * 180 Check that the sun-sensor angles are within a reasonable range. They should range from ~30 degrees mid summer to ~80 degrees mid winter. xmp_all_ssa = SSA_xmp_all_filtered %&gt;% # Converting from radians to degrees mutate(Yaw_deg = as.numeric(Yaw)*180/pi, Roll_deg = as.numeric(Roll)*180/pi, Pitch_deg = as.numeric(Pitch)*180/pi) %&gt;% # Grouping images by Date and band group_by(Date, BandName) %&gt;% arrange(ymd_hms(DateTimeOriginal)) %&gt;% # Converting DateTimeOriginal to a Date and Time object and arranging in order mutate(GPSLatitude_plot = scale(as.numeric(GPSLatitude)), # These are for clean ggplotting, no other reason to scale GPSLongitude_plot = scale(as.numeric(GPSLongitude)), cos_SSA = cos(SunSensorAngle_DLS1_rad), Irradiance = as.numeric(Irradiance), Date2 = ymd_hms(DateTimeOriginal)) # this is the date/time we will use moving forward (SSA_plot &lt;- xmp_all_ssa %&gt;% ggplot(aes(Date_time)) + geom_line(aes(y = SunSensorAngle_DLS2_deg, color = &quot;DLS2 (from metadata)&quot;), linewidth = 1) + geom_line(aes(y = SunSensorAngle_DLS1_deg, color = &quot;DLS1 (calculated)&quot;), linewidth = 1) + geom_vline(data = subset(xmp_all_ssa, panel_flag == 1), aes(xintercept = as.numeric(Date_time)), color = &quot;grey&quot;, alpha = 0.3) + geom_line(aes(y = solar_angle), color = &quot;black&quot;, linewidth = 1, alpha = 1) + scale_x_datetime(date_breaks = &quot;10 min&quot;, date_labels = &quot;%H:%M&quot;) + labs(x = &quot;Time&quot;, y = &quot;Sun Sensor Angle&quot;, title = paste0(&quot;SSA, Site: &quot;,site_to_plot,&quot;, Date: &quot;, date_to_plot, &quot;, Camera: &quot;, camera_to_plot))+ labs(subtitle = &quot;Grey vertical lines are calibration panel images, black hoirzontal line is the solar angle (calculated from lat, long, and flight time)&quot;, size = 10) + scale_color_manual(values = c(&quot;blue&quot;, &quot;red&quot;), labels = c(&quot;DLS2 (from metadata)&quot;, &quot;DLS1 (calculated)&quot;), name = &quot;Sun Sensor Angle Comparison&quot;)+ facet_wrap(. ~BandName_Wavelength, scales = &quot;free_x&quot;, ncol = 2)+ # this will plot each band as its own plot as a check for all data theme_bw() ) 6.4.4 Scattered:Direct Ratio Estimate Here we estimate scattered:direct ratio for each flight by relating cosine of the sun-sensor angle to spectral irradiance measured by the DLS2. This relationship, in a perfect world, should give you the scattered irradiance as the intercept, which is independent of angle, and the direct irradiance, which is the slope, and therefore perfectly proportional to sun angle. In reality, these relationships are extremely messy and most data needs to be discarded. Below are three main steps to find the estimated scattered:direct ratio: First, generate a rolling regression of this linear relationship over a certain time window Second, eliminate all models with poor fits using the R\\(^2\\) value Third, drop any models with negative slopes or intercepts (physically impossible) First, we generate a rolling regression of the linear relationship \\(I_{\\text{spec}} = I_{\\text{direct}} \\cdot \\cos(\\theta_{\\text{sun-sensor}}) + I_{\\text{scattered}}\\) via a regression of irradiance on \\(\\cos(\\theta_{\\text{sun-sensor}})\\) over a specified time window (we used 30 seconds here). # via a regression of Irradiance on cos_SSA over a specified time window (30s here) mod_frame = xmp_all_ssa_site_date %&gt;% drop_na(Date2) %&gt;% drop_na(cos_SSA) %&gt;% drop_na(Irradiance) %&gt;% # Fit a rolling regression # for each image, fit a linear model of all images (of the same band) within 30 seconds of the image tidyfit::regress(Irradiance ~ cos_SSA, m(&quot;lm&quot;), .cv = &quot;sliding_index&quot;, .cv_args = list(lookback = lubridate::seconds(30), index = &quot;Date2&quot;), .force_cv = TRUE, .return_slices = TRUE) # df : summary of models, adding R sqaured and Dates df = mod_frame %&gt;% # Get a summary of each model and extract the r squared value mutate(R2 = map(model_object, function(obj) summary(obj)$adj.r.squared)) %&gt;% # Extract the slope and intercept coef() %&gt;% unnest(model_info) %&gt;% mutate(Date2 = ymd_hms(slice_id)) # df_params : adding slope (direct irradiance) and y-intercept (scatterd irradiance) values df_params = df %&gt;% dplyr::select(Date:estimate, Date2) %&gt;% # we will have to go from long, with 2 observations per model, to wide pivot_wider(names_from = term, values_from = estimate, values_fn = {first}) %&gt;% dplyr::rename(&quot;Intercept&quot; = `(Intercept)`, &quot;Slope&quot; = &quot;cos_SSA&quot;) # Cleaning up the df df_p = df %&gt;% filter(term == &quot;cos_SSA&quot;) %&gt;% dplyr::select(Date:model, R2, p.value, Date2) # Joining model info, parameter (slope, y intercept) info and XMP data with sun sensor angle and using the linear relationship # (spectral_irr = direct_irr * cos(SSA) + scattered_irr) to create % scattered and scattered/direct ratios df_filtered = df_params %&gt;% left_join(df_p) %&gt;% left_join(xmp_all_ssa_site_date) %&gt;% mutate(percent_scattered = Intercept / (Slope + Intercept), dir_diff = Intercept/Slope) Next, we eliminate all models with poor fits. In this case we eliminate models with R\\(^2\\) &lt; 0.4 and drop any models with negative slopes or intercepts (physically impossible) df_to_use = df_filtered %&gt;% mutate(R2 = as.numeric(R2)) %&gt;% filter(R2 &gt; .4 &amp; Slope &gt; 0 &amp; Intercept &gt; 0) %&gt;% group_by(Date) %&gt;% mutate(mean_scattered = mean(percent_scattered), dir_diff_ratio = mean(dir_diff)) #save out dataframe as an rds saveRDS(df_to_use,paste0(dir,date,&quot;\\\\1_Data\\\\&quot;,MS_folder_name,&quot;\\\\CSV\\\\&quot;,date,&quot;_rolling_regression_filteredModels_used_plot1.rds&quot;)) #SET PATH to save the rds to Below we plot the pattern in the rolling regression and make sure you’re keeping enough models. If you are loosing too many models, adjust the filters. (RR_params &lt;- df_to_use %&gt;% group_by(Date, BandName) %&gt;% ggplot(aes(x = Date2, y = percent_scattered, color = R2)) + geom_point(data = df_filtered, color = &quot;grey&quot;) + geom_hline(yintercept = 1, linetype = 2) + geom_hline(yintercept = 0, linetype = 2) + geom_hline(aes(yintercept = mean_scattered), color = &quot;red4&quot;, linewidth = 1) + scale_y_continuous(breaks = seq(0, 1, .2), limits = c(0,1)) + ggnewscale::new_scale_color() + labs(title = paste0(&quot;Site: &quot;,site_to_plot,&quot;, Date: &quot;, date_to_plot, &quot;, Camera: &quot;, camera_to_plot), x = &quot;Time (UTC)&quot;)+ theme_bw(base_size = 16) + facet_wrap(. ~ Date, scales = &quot;free&quot;)) Next, check that the linear relationships you’re keeping look realistic. The slope should be steeper for sunny days (i.e. more direct irradiance) and shallower for overcast days. (linear_plots &lt;- df_to_use %&gt;% filter(BandName == &quot;Blue&quot;) %&gt;% ggplot(aes(x = cos_SSA, y = Irradiance, color = R2)) + geom_point(data = filter(df_filtered, BandName == &quot;Blue&quot;), color = &quot;grey30&quot;, alpha = .4) + geom_point(data = filter(df_filtered, BandName == &quot;Blue&quot; &amp; R2 &gt; .4), aes(color = as.numeric(R2))) + geom_smooth(method = &quot;lm&quot;, se = FALSE, aes(group = BandName)) + lims(x = c(0, 1), y = c(0, max(df_to_use$Irradiance))) + geom_abline(aes(slope = Slope, intercept = Intercept, color = R2), alpha = .3) + labs(title = paste0(&quot;Site: &quot;,site_to_plot,&quot;, Date: &quot;, date_to_plot, &quot;, Camera: &quot;, camera_to_plot))+ theme_bw(base_size = 16) + scale_color_viridis_c() + facet_wrap(. ~ Date, scales = &quot;free_y&quot;)) We recommend checking that there is no excessive spatial pattern in the data by plotting the locations of imagery used in the models. Here you want to ensure that imagery throughout the plot is being used rather than only images from a certain location. The code to do this is below: (photos_kept &lt;- df_to_use %&gt;% filter(GPSLongitude != 0 &amp; GPSLatitude != 0) %&gt;% # Filter out imgs with GPSLongitude and GPSLatitude of zero, # this is rare and in my experience were corrupted imgs where the XMP could not be properly read ggplot(aes(x = GPSLongitude, y = GPSLatitude, color = SunSensorAngle_DLS1_deg)) + geom_point(data = df_filtered, color = &quot;grey60&quot;) + geom_point(size = 3) + labs(title = paste0(&quot;Site: &quot;,site_to_plot,&quot;, Date: &quot;, date_to_plot, &quot;, Camera: &quot;, camera_to_plot))+ theme_bw() + scale_color_viridis_c() + facet_wrap(. ~ Date, scales = &quot;free&quot;)) Lastly, we compute the scattered/ direct irradiance ratios dataframe from the dataframe of filtered models (ratios = df_to_use %&gt;% dplyr::select(Date, mean_scattered, dir_diff_ratio, GPSLatitude, GPSLongitude, Date2) %&gt;% group_by(Date) %&gt;% mutate(Lat_mean = mean(GPSLatitude), Long_mean = mean(GPSLongitude), Date_mean = mean(Date2)) %&gt;% distinct(Date, mean_scattered, dir_diff_ratio, Lat_mean, Long_mean, Date_mean)) saveRDS(ratios,paste0(dir,date,&quot;\\\\1_Data\\\\&quot;,MS_folder_name,&quot;\\\\CSV\\\\&quot;,date,&quot;_ratios.rds&quot;)) #SET PATH to save the rds to # Print the values to use in the calibration as a check (does this ratio looks reasonable?) round(ratios$dir_diff_ratio, 2) The ratio for the DLS1 was 0.46 and the mean_scattered component was ~32 W/m\\(^2\\). 6.4.5 Computing Horizontal (Corrected) Irradiance This is the Fresnel correction, which adjusts for the DLS reflecting, rather than measuring, some of the irradiance that hits it. We aquired this code from Micasense’s GitHub and converted to R to use in this workflow. fresnel_transmission = function(phi, n1, n2, polarization) { f1 = cos(phi) f2 = sqrt(1 - (n1 / n2 * sin(phi))^2) Rs = ((n1 * f1 - n2 * f2) / (n1 * f1 + n2 * f2))^2 Rp = ((n1 * f2 - n2 * f1) / (n1 * f2 + n2 * f1))^2 T = 1 - polarization[1] * Rs - polarization[2] * Rp T = pmin(pmax(T, 0), 1) # Clamp the value between 0 and 1 return(T) } multilayer_transmission = function(phi, n, polarization) { T = 1.0 phi_eff = phi for (i in 1:(length(n) - 1)) { n1 = n[i] n2 = n[i + 1] phi_eff = asin(sin(phi_eff) / n1) T = T * fresnel_transmission(phi_eff, n1, n2, polarization) } return(T) } # Defining the fresnel_correction function fresnel_correction = function(x) { Irradiance = x$Irradiance SunSensorAngle_DLS1_rad = x$SunSensorAngle_DLS1_rad n1=1.000277 n2=1.38 polarization=c(0.5, 0.5) # Convert sun-sensor angle from radians to degrees SunSensorAngle_DLS1_deg &lt;- SunSensorAngle_DLS1_rad * (180 / pi) # Perform the multilayer Fresnel correction Fresnel &lt;- multilayer_transmission(SunSensorAngle_DLS1_rad, c(n1, n2), polarization) return(Fresnel) } Now we put it all together to compute the horizontal irradiance. Here the horizontal irradiance can be thought of as a corrected value for the total irradiance that is reaching a point on the flat ground directly underneath the drone. xmp_corrected = xmp_all_ssa %&gt;% group_by(Date, BandName) %&gt;% #filter(BandName == &quot;Red&quot;) %&gt;% #slice_head(n = 900) %&gt;% nest(data = c(Irradiance, SunSensorAngle_DLS1_rad)) %&gt;% # Creates a nested df where each group is stored as a list-column named data, containing the variables Irradiance and SunSensorAngle_DLS1_rad mutate(Fresnel = as.numeric(map(.x = data, .f = fresnel_correction))) %&gt;% # Applies the fresnel_correction function to each group of nested data unnest(data) %&gt;% #unnesting # Joining the ratios left_join(ratios, by = &quot;Date&quot;) %&gt;% mutate(SensorIrradiance = as.numeric(SpectralIrradiance) / Fresnel, # irradiance adjusted for some reflected light from the DLS diffuser DirectIrradiance_new = SensorIrradiance / (dir_diff_ratio + cos(as.numeric(SunSensorAngle_DLS1_rad))), # adjusted for sun angle, HorizontalIrradiance_new = DirectIrradiance_new * (dir_diff_ratio + sin(as.numeric(SolarElevation))), ScatteredIrradiance_new = HorizontalIrradiance_new - DirectIrradiance_new) saveRDS(xmp_corrected,paste0(dir,date,&quot;\\\\1_Data\\\\&quot;,MS_folder_name,&quot;\\\\CSV\\\\&quot;,date,&quot;_xmp_corrected.rds&quot;)) #SET PATH to save the rds to Now we plot the sensor irradiance along with the corrected horizontal, direct, and scattered irradiance values (Sensor_irr &lt;- xmp_corrected %&gt;% filter(BandName == &quot;Blue&quot;) %&gt;% ggplot(aes(x = Date2)) + geom_point(aes(y = SensorIrradiance, color = &quot;Sensor Irradiance&quot;),size = 1,show.legend = TRUE) + geom_point(aes(y = HorizontalIrradiance_new, color = &quot;Horizontal_DLS1&quot;), size = 1, show.legend = TRUE) + geom_point(aes(y = DirectIrradiance_new, color = &quot;Direct_DLS1&quot;), size = 1, show.legend = TRUE) + geom_point(aes(y = ScatteredIrradiance_new, color = &quot;Scattered_DLS1&quot;), size = 1, show.legend = TRUE) + geom_hline(yintercept = 0) + scale_color_manual(values = c(&quot;Sensor Irradiance&quot;= &quot;black&quot;, &quot;Horizontal_DLS1&quot; = &quot;red&quot;, &quot;Direct_DLS1&quot; = &quot;orange&quot;, &quot;Scattered_DLS1&quot; = &quot;purple&quot;)) + #lims(y = c(50, 150)) + labs(y = &quot;Irradiance&quot;, title = paste0(&quot;Site: &quot;,site_to_plot,&quot;, Date: &quot;, date_to_plot, &quot;, Camera: &quot;, camera_to_plot))+ theme_bw() + facet_wrap(. ~ Date, scales = &quot;free&quot;, ncol = 3) + labs(color = &quot;Irradiance Type&quot;)) Now that we have corrected the data using the DLS1 method, we can compare the DLS1 values to the DLS2 values to see if correction is needed. The code below creates a dataframe that has the median scatted/direct ratios and median percent scattered irradiance values for the DLS1 method and from the DLS2 sensor. These values will be used to annotate the following comparison plots. avg_ratio_data &lt;- xmp_corrected %&gt;% filter(!is.na(BandName))%&gt;% group_by(BandName) %&gt;% summarize(median_ScatteredDirectRatio_DLS2 = median(as.numeric(ScatteredIrradiance) /as.numeric(DirectIrradiance), na.rm = TRUE), median_ScatteredDirectRatio_DLS1_calc = median(as.numeric(ScatteredIrradiance_new) /as.numeric(DirectIrradiance_new), na.rm = TRUE), median_percent_scat_DLS2 = median(100*as.numeric(ScatteredIrradiance)/(as.numeric(ScatteredIrradiance)+as.numeric(DirectIrradiance))), median_percent_scat_DLS1 = median(100*as.numeric(ScatteredIrradiance_new)/(as.numeric(ScatteredIrradiance_new)+as.numeric(DirectIrradiance_new))), max_Date_time = max(Date_time), max_dir = max(DirectIrradiance, na.rm = TRUE)) %&gt;% ungroup()%&gt;% mutate( Scattered_To_Direct_Ratio_DLS2 = as.character(round(median_ScatteredDirectRatio_DLS2,2)), Scattered_To_Direct_Ratio_DLS1 = as.character(round(median_ScatteredDirectRatio_DLS1_calc,2)), Percent_Scat_DLS2 = as.character(round(median_percent_scat_DLS2,2)), Percent_Scat_DLS1 = as.character(round(median_percent_scat_DLS1,2)), x = max_Date_time, y = max_dir, )%&gt;% dplyr::select(c(Scattered_To_Direct_Ratio_DLS2,Scattered_To_Direct_Ratio_DLS1,Percent_Scat_DLS2,Percent_Scat_DLS1, x, y, BandName, camera)) unique(xmp_corrected$CenterWavelength) #make sure all wavebands are present saveRDS(avg_ratio_data,paste0(dir,date,&quot;\\\\1_Data\\\\&quot;,MS_folder_name ,&quot;\\\\CSV\\\\&quot;,date,&quot;_avg_ratio_data.rds&quot;)) #SET PATH to save the rds to Plotting original VS corrected exif irradiance data to compare values from the DLS2 to corrected values (DLS1 method). data = avg_ratio_data %&gt;% filter(BandName %in% c(&quot;Blue&quot;)) # only need to look at one band (Dir_scat_irr_plot &lt;- xmp_corrected %&gt;% filter(BandName %in% c(&quot;Blue&quot;)) %&gt;% mutate(DirectIrradiance = as.numeric(DirectIrradiance), ScatteredIrradiance = as.numeric(ScatteredIrradiance), Irradiance = as.numeric(Irradiance), SpectralIrradiance = as.numeric(SpectralIrradiance), HorizontalIrradiance = as.numeric(HorizontalIrradiance), # Date_time = ymd_hms(CreateDate), Time = format(Date_time, format = &quot;%H:%M:%S&quot;), #scaled DirectIrradiance_scaled = as.numeric(DirectIrradiance)*0.01, ScatteredIrradiance_scaled = as.numeric(ScatteredIrradiance)*0.01, SpectralIrradiance_scaled = as.numeric(SpectralIrradiance)*0.01) %&gt;% ggplot(aes(Date_time)) + geom_line(aes(y = DirectIrradiance, color = &quot;Direct&quot;), linewidth = 1) + geom_line(aes(y = DirectIrradiance_new, color = &quot;Direct_DLS1&quot;), linewidth = 1, linetype = &quot;dashed&quot;) + geom_line(aes(y = ScatteredIrradiance, color = &quot;Scattered&quot;), linewidth = 1) + geom_line(aes(y = ScatteredIrradiance_new, color = &quot;Scattered_DLS1&quot;), linewidth = 1,linetype = &quot;dashed&quot;) + geom_line(aes(y = HorizontalIrradiance, color = &quot;Horizontal&quot;),linewidth = 1) + geom_line(aes(y = HorizontalIrradiance_new, color = &quot;Horizontal_DLS1&quot;),linewidth = 1, linetype = &quot;dashed&quot;) + geom_line(aes(y = Irradiance, color = &quot;Irradiance&quot;), linewidth = 1) + geom_line(aes(y = HorizontalIrradiance, color = &quot;Horizontal&quot;),linewidth = 1) + geom_line(aes(y = SpectralIrradiance, color = &quot;Spectral&quot;), linewidth = 1.5, linetype = &quot;dotted&quot;) + labs(x = &quot;Time (UTC)&quot;, y = &quot;Irradiance&quot;, title = paste0(&quot;DLS1 (corrected) vs. DLS2 (metadata) Irradiance, Site: &quot;,site_to_plot,&quot;, \\nDate: &quot;, date_to_plot,&quot;, Camera: &quot;, camera_to_plot, &quot;\\nScattered_To_Direct_Ratio_DLS1:&quot;, data$Scattered_To_Direct_Ratio_DLS1, &quot;\\nScattered_To_Direct_Ratio_DLS2:&quot;, data$Scattered_To_Direct_Ratio_DLS2, &quot;\\nPercent_Scat_DLS1:&quot;, data$Percent_Scat_DLS1, &quot;\\nPercent_Scat_DLS2:&quot;, data$Percent_Scat_DLS2 )) + scale_x_datetime(date_breaks = &quot;10 min&quot;, date_labels = &quot;%H:%M&quot;) + scale_color_manual(values = c(&quot;Direct&quot; = &quot;blue&quot;, &quot;Direct_DLS1&quot; = &quot;lightblue&quot;, &quot;Scattered&quot; = &quot;black&quot;, &quot;Scattered_DLS1&quot; = &quot;grey&quot;, &quot;Horizontal&quot; = &quot;red&quot;, &quot;Horizontal_DLS1&quot; = &quot;orange&quot;, &quot;Spectral&quot; = &quot;forestgreen&quot;, &quot;Irradiance&quot; =&quot;purple&quot; ), name = &quot;Irradiance (W/m2/nm)&quot;)+ facet_grid(BandName ~ camera, scales = &quot;free&quot;)+ theme_bw() ) Refer to the section Is Correction Needed? to help decide whether or not your data needs correcting. Up to this point, we have calculated “corrected” irradiance and sun sensor angles however have not overwritten the exif data within the MicaSense imagery. In this next step, we will overwrite image exif data with the corrected DLS1 values. Before moving on to this step ensure that : You have decided that your data needs correcting You have created a backup of your original data, since you will be irreversibly overwriting the exif data of the imagery. Prior to running the code below we copied all images in our Micasense folder to a folder in the same directory as the Micasense folder however named named Micasense_cor. Imagery in the Micasense_cor folder will be the imagery we will be editing, and the Micasense folder will contain the unedited imagery The MicaSense.config has been downloaded from GitHub, the file can be found in the “Scripts” folder under MicaSense # Writing over exif data with corrected SSA, horizontal irradiance, direct irradiance, and scattered irradiance corrected_directory &lt;- paste0(dir,date,&quot;\\\\1_Data\\\\&quot;,MS_folder_name,&quot;_cor\\\\&quot;) #i.e. this folder is called Micasense_cor and is a copied version of the Micasense folder, we will be correcting the exif data of images in the Micasense_cor folder and leaving the Micasense folder untouched with the original exif data original_directory &lt;- paste0(dir,date,&quot;\\\\1_Data\\\\&quot;,MS_folder_name,&quot;\\\\&quot;) # path the folder of micasense images xmp_corrected = readRDS(paste0(dir, date, &quot;\\\\&quot;, MS_folder_name,&quot;\\\\CSV\\\\&quot;,date,&quot;_xmp_corrected.rds&quot;)) %&gt;% #path to corrected xmp data .rds file mutate(SourceFile = str_replace(SourceFile,original_directory, corrected_directory), # Changing source file name from the original directory to the corrected_directory. This will result in imagery in the corrected direcotry only to be edited TargetFile = SourceFile) #the TargetFiles are the path names to the files to be edited # As vectors (img_list = xmp_corrected$FileName) (targets = xmp_corrected$TargetFile) (SSA = xmp_corrected$SunSensorAngle_DLS1_rad) (horirrorig = xmp_corrected$HorizontalIrradiance) (horirr = xmp_corrected$HorizontalIrradiance_new) (dirirr = xmp_corrected$DirectIrradiance_new) (scairr = xmp_corrected$ScatteredIrradiance_new) targets[1] #checking its the right imgs for (i in seq_along(targets)) { # given a micasense config file, overwrite tags with computed values # using exiftool_call from the exifr package call = paste0(&quot;-config G:/GitHub/GenomeBC_BPG/MicaSense/MicaSense.config&quot;, # SET PATH to the config file, this file is on the PARSER GitHub in the same folder as this R script &quot; -overwrite_original_in_place&quot;, &quot; -SunSensorAngle=&quot;, SSA[i], &quot; -HorizontalIrradiance=&quot;, horirr[i], &quot; -HorizontalIrradianceDLS2=&quot;, horirrorig[i], &quot; -DirectIrradiance=&quot;, dirirr[i], &quot; -ScatteredIrradiance=&quot;, scairr[i], &quot; &quot;, targets[i]) exiftool_call(call, quiet = TRUE) print(paste0(i, &quot;/&quot;, length(targets), &quot; updated img:&quot;,img_list[i] )) } Lastly, check the exif data of the first and last image to ensure that the data has been properly overwritten. "],["thermal-conversion.html", "Chapter 7 Thermal Conversion 7.1 Folder Set-Up 7.2 Weather Data 7.3 DJI Thermal SDK: temperature conversion 7.4 Convert to Raster", " Chapter 7 Thermal Conversion This method converts raw imagery from the Zenmuse H20T to temperature rasters. We wrote this method to bypass DJI’s thermal analysis tool app since we have thousands of images and the app would often crash when a few were uploaded. This script uses the DJI thermal analysis tool’s software developer kit (SDK) Prior to running this code you must: - Download the .bat file for step 2 from the GitHub library(exiftoolr) library(dplyr) library(lubridate) library(OpenImageR) library(raster) library(doParallel) library(stringr) # Read in CSV of weather data (ensure adjustments for daylight savings have been made if necessary) df &lt;- read.csv(&quot;D:\\\\Fdc_PR_Canoe\\\\Site_Info\\\\Powell_River_Canoe2_All_Data_2023_02_07_PST_1_DaylightSavings_mar16_nov6_2022.csv&quot;) df$Date &lt;- as.Date(df$Date, &#39;%m/%d/%y&#39;) #comes out as year-0m-0d dates_DST &lt;- c(unique(df$Date)) dates_DST dates_list &lt;- as.list(dates_DST) dates_list Canoe_df &lt;- df 7.1 Folder Set-Up This step goes over how we organize our imagery for the following thermal conversion. Here we: create a folder named: “Merged_T” within the original “H20T” folder, this is the location the raw thermal images will be copied. This allow us to (a) keep a backup of the original data and (b) easily iterate over the images to convert them in later steps “Temperature_rasters” within the Merged_T folder that will hold converted imagery “Temperature_rasters_EXIF” within the Merged_T folder that will hold converted imagery that has original imagery exif data written to it locate all folders within the defined directory that have DJI and H20T in the name Iterate over the folders found in the above step and: find files that end in _T.JPG (the raw thermal images) copy over the raw thermal images to the merged folder Note: Our folder structure was: paste0(“I:\\PARSER_Ext\\”,site,“\\Flights\\”, data_date, “\\1_Data\\H20T”), where “site” is a string of the site name that we defined prior to the for loop and data_date is a string representing the flight date that is defined in the for loop. This was done so we could easily iterate over sites and dates. Feel free to change this to match your folder structure. # Setting the site and flight date to convert thermal imagery flight_dates &lt;- c(&quot;2023_05_10&quot;) # Here only one date is shown however this can be a list of however many dates you would like to iterate over site &lt;- &quot;Fdc_PR_Canoe&quot; merged &lt;- &quot;Merged_T&quot; #name of the &#39;merged&#39; folder that will contain all the h20T images from the multiple folders ouput by the H20T for(i in 1:length(flight_dates)){ data_date &lt;- flight_dates[i] print(data_date) dir &lt;- paste0(&quot;I:\\\\PARSER_Ext\\\\&quot;,site,&quot;\\\\Flights\\\\&quot;, data_date, &quot;\\\\1_Data\\\\H20T&quot;) #create folders if (!dir.exists(paste0(dir, &quot;\\\\&quot;,merged))) { dir.create(paste0(dir, &quot;\\\\&quot;,merged),recursive = TRUE) } if (!dir.exists(paste0(dir, &quot;\\\\&quot;,merged,&quot;\\\\Temperature_rasters&quot;))) { dir.create(paste0(dir, &quot;\\\\&quot;,merged,&quot;\\\\Temperature_rasters&quot;),recursive = TRUE) } if (!dir.exists(paste0(dir, &quot;\\\\&quot;,merged,&quot;\\\\Temperature_rasters_EXIF&quot;))) { dir.create(paste0(dir, &quot;\\\\&quot;,merged,&quot;\\\\Temperature_rasters_EXIF&quot;),recursive = TRUE) } list_dir &lt;- list.files(dir, full.names = TRUE,pattern = c(&quot;DJI.+-H20T&quot;)) #selecting folders in the H20T folder that have DJI and end in -H20T, this is how we named our folders, change this pattern to match your folders. You should be selecting all folders with H20T imagery ending in _T for that flight print(list_dir) #to check only correct directories are being read for (d in 1:length(list_dir)){ folder_dir &lt;- list_dir[d] #calling each directly separately T_files &lt;- list.files(folder_dir, pattern = &quot;_T\\\\.JPG$&quot;) #selecting all JPEGs ending in _T file.copy(from = paste0(folder_dir,&quot;\\\\&quot;, T_files), to = paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\&quot;, T_files), overwrite = FALSE) #copying h20T images ending in _T (aka unprocessed thermal images) into the thermal only folder for ease in DJI SDK step } } 7.2 Weather Data For this step you will need : Humidity, Ambient Temperature First, we use the first and last image in the Merged_T folder to define the start and end time of the flight, along with the date of the flight as a date object. #getting image capture date from exif data of the first image in the H20T folder data_date &lt;- flight_dates[1] time_file_list &lt;- list.files(paste0(dir,&quot;\\\\&quot;,merged), pattern = &quot;_T\\\\.JPG$&quot;) #Retrieving the time the FIRST H20T thermal image was taken (img_1 &lt;- time_file_list[1]) #find the first thermal image and print the name start_exif_flight_date &lt;- data.frame(exif_read(paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\&quot;,img_1), tags = &quot;CreateDate&quot;, quiet = TRUE)) #find the time the image was taken/written from the CreateDate tag in the exif data (start_flight_date_time &lt;- start_exif_flight_date$CreateDate) #Start of the flight #Retrieving the time the LAST H20T thermal image was taken (img_last &lt;- tail(time_file_list, n=1))#End flight time end_flight_date_time &lt;- data.frame(exif_read(paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\&quot;,img_last), tags = &quot;CreateDate&quot;, quiet = TRUE)) (end_flight_date_time &lt;- end_flight_date_time$CreateDate)#End of the flight (flight_date &lt;- as.Date(start_exif_flight_date$CreateDate, &#39;%Y:%m:%d %H:%M:%S&#39;)) #the flight date as a date object (flight_date_filt &lt;- as.Date(flight_date, format = &#39;%Y-%m-%d&#39;)) #flight date in the format of 2024-03-10, YYYY-mm-dd, this ill be used in the next step to filter weather data Next, since our weather station provided hourly measurements, we calculate a weighted average of temperature and humidity values based on time intervals. This was done to account for the fact that variables will change throughout the flight, so gathering a weighted average where each hourly measurement is weighted in proportion to how much of the hour fell within the flight window gives us a better estimate of the overall weather variables than if a plain average was taken. To begin, we read in the weather data and set a start time and end time of the flight, where start time is rounded down to the nearest hour (i.e. 10:15am -&gt; 10:00am) and end time is rounded up to the nearest hour (i.e. 11:30am -&gt; 12:00pm). These values will be used to filter the weather data. cfs_weather &lt;- &quot;path to weather data&quot; #loading in HOBO weather station data cfs_weather$Date &lt;- as.Date(cfs_weather$Date, format = &quot;%m/%d/%y&quot;) #converting date in the format m/d/Y to a date object written as YYY-mm-dd start_time_avg &lt;- &quot;10:00:00&quot; #Based off of start_flight_date_time of 10:15am stop_time_avg &lt;- &quot;12:00:00&quot; #Based off of end_flight_date_time of 11:30am For simplicity we kept weights as 0.25 (15min), 0.5 (30min), 0.75 (45min) or 1 (1 hr) and rounded the start time to the earliest 15 min marker and the end time to the latest 15 min marker. For example, for a flight that started at 10:03am and ended at 11:09am, we rounded 10:03am to 10:00am and 11:09am to 11:15am. Weights for start, middle, and end hours: The start weight corresponds to how much of the first hour is covered by the flight time. For example, if the flight starts at 10:15, 45 minutes (or 0.75 of the hour) are included in that hour, so the start weight is 0.75. -The middle weight (if the flight spans more than one hour) represents the full hour(s) covered by the flight. In the example of a flight from 10:15 to 11:30, the middle weight (for the 11:00-12:00 interval) would be 1, since the full hour is included. The end weight applies similarly for the last hour of the flight. If the flight ends at 11:30, 30 minutes (or 0.5 of the hour) are included in that hour, so the end weight is 0.5. #Ie: Flight starts at 10:15am and ends at 11:30am start_weight &lt;- 0.75 # 75% of 10am mid_weight &lt;- 1 # 100% of 11am end_weight &lt;- 0.5 # 50% of 12pm sum_weight &lt;- sum(start_weight, mid_weight,#comment out the mid_weight if no mid weight end_weight) #creating df with weighted average air temperature and humidity values (cfs_weather_time_avg &lt;- cfs_weather%&gt;% filter(Date == flight_date_filt ) %&gt;% filter(Time &lt;= stop_time_avg &amp; Time &gt;= start_time_avg)%&gt;% #filtering for values taken between the defined start (10am) and end (12pm) times mutate(Average_hum = (start_weight*RH[1] + mid_weight*RH[2] #comment out this line if no mid_weight and just below index of [3] to [2] + end_weight*RH[3])/sum_weight, Average_air_temp = (start_weight*Temperature[1] + mid_weight*Temperature[2]#comment out this line if no mid_weight and just below index of [3] to [2] + end_weight*Temperature[3])/sum_weight)) #check values (avg_hum &lt;- cfs_weather_time_avg$Average_hum) (avg_temp &lt;- cfs_weather_time_avg$Average_air_temp) (date &lt;- as.Date(cfs_weather_time_avg$Date, &#39;%d-%m-%Y&#39;)) #creating a data frame with weighted averaged values Canoe_flight_table &lt;- data.frame( date, avg_hum, avg_temp ) Canoe_flight_table &lt;- Canoe_flight_table %&gt;% distinct()#removing duplicate rows #resetting index in table to go from 1-N rownames(Canoe_flight_table) &lt;- NULL #setting variables for weighted average of humidity and ambient temperature, these variables will be used as inputs into DJI&#39;s thermal anlysis tool SDK to convert rasters to temperature flight_humidity &lt;- Canoe_flight_table[Canoe_flight_table$date == flight_date, &quot;avg_hum&quot;] flight_humidity flight_amb_temp &lt;- Canoe_flight_table[Canoe_flight_table$date == flight_date, &quot;avg_temp&quot;] flight_amb_temp 7.3 DJI Thermal SDK: temperature conversion 7.3.1 Parameters required for thermal conversion: Reflection : reflection of target is currently set to 23 range is -40 to 500 ^ Reflected Temperature: the surface of the target that is measured could reflect the energy radiated by the surrounding objects. This reflected energy could be picked up by the camera along with the radiation, which could cause an error in the temperature reading. If there are no objects with extreme high or low temperatures nearby, set this parameter as the ambient temperature. Reflected temperature configurations could affect the measurement result, and the bigger the difference between the reading and the ambient temperature, the bigger the impact. Here we use the weighted average ambient temperature calculated above from the HOBO climate logger (aka flight_amb_temp) Humidity: weighted average from on-site HOBO climate logger (flight_humidity) Distance: Using max distance of 25m meter since we are flying higher (~40m away from object for h20T depending on site) Emissivity: #setting parameters: (flight_amb_temp) # already set above, () to print out the value in the console to ensure it is the correct one (flight_humidity) # already set above distance &lt;- 25 emissivity &lt;- 0.98 7.3.2 Editing .bat file for DJI SDK tool and running in terminal First, we copy the original .bat file into the Merged_T folder. We do this so that the original does not get edited and instead each flight will have their own edited .bat file. This allows you to go back and see what parameters were used if needed. The original .bat file can be found on GitHub and looks like this: REM to change dir to the location of this file REM start time used to average humidity and temperature values: start_time_avg REM end time used to average humidity and temperature values: stop_time_avg REM weights (in 15min intervals ie 0.25, 0.5, 0.75 or 1, round down for start and up for end time): start w: start_weight, mid w: mid_weight (might not have one), end w: end_weight cd /D &quot;%~dp0&quot; mkdir DJI_SDK_raw for %%i in (*.JPG) do ( echo Working on %%i... C:\\dji_thermal_sdk_v1.3_20220517\\utility\\bin\\windows\\release_x64\\dji_irp.exe -s %%i -a measure -o DJI_SDK_raw/%%~ni.raw --humidity hum_change --distance dist_change --emissivity emis_change --reflection reflec_change ) pause This script makes a folder called DJI_SDK_raw in the directory the .bat is saved in (which will become the Merged_T folder once we copy it over in the below code) and converts the imagery in the Merged_T folder ending in .JPG to temperature using DJI’s thermal anlysis tool’s SDK. Before continuing to the next steps: save the .bat file from GitHub and update the omp_dir path in the below R code to the folder where this file was saved to download the dji thermal sdk here edit the location of the dji_irp.exe in your saved .bat file to match where the program is saved on your computer i.e. replace the “C:\\dji_thermal_sdk_v1.3_20220517\\utility\\bin\\windows\\release_x64\\dji_irp.exe” in the above script to the directory you saved the dji_irp.exe file from above step 2 to omp_dir &lt;- &quot;C:/Users/owaite/Documents/Scripts/DJI_SDK_TemperatureConversion_Script&quot; #change to the directory where you saved the original .bat script from the above GitHub link to file.copy(from = paste0(omp_dir,&quot;\\\\&quot;, &quot;DJI_SDK_loop.bat&quot;), #copy the original omp.bat file into the Merged_T folder to = paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\&quot;,&quot;DJI_SDK_loop.bat&quot;), overwrite = FALSE) bat_old &lt;- file(paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\&quot;,&quot;DJI_SDK_loop.bat&quot;)) #opening .bat file to edit bat_new &lt;- readLines(bat_old) #reading in lines of .bat file to edit and saving them out close(bat_old) #replacing values comment out mid_weight line if necessary bat_new &lt;- gsub(&quot;hum_change&quot;,flight_humidity, bat_new) bat_new &lt;- gsub(&quot;dist_change&quot;,distance, bat_new) bat_new &lt;- gsub(&quot;emis_change&quot;,emissivity,bat_new) bat_new &lt;- gsub(&quot;reflec_change&quot;,flight_amb_temp,bat_new) bat_new &lt;- gsub(&quot;start_time_avg&quot;,start_time_avg, bat_new) bat_new &lt;- gsub(&quot;stop_time_avg&quot;,stop_time_avg,bat_new) bat_new &lt;- gsub(&quot;start_weight&quot;, start_weight, bat_new) bat_new &lt;- gsub(&quot;mid_weight&quot;, mid_weight, bat_new) #if no mid_weight, comment this line out bat_new &lt;- gsub(&quot;end_weight&quot;, end_weight, bat_new) #Write the new file fileConn &lt;- file(paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\DJI_SDK_loop_updated.bat&quot;)) writeLines(bat_new, fileConn) close(fileConn) Next, we run the updated .bat file in windows terminal using the R code below. This will automatically output a folder of new raw images with temperature in binary saved to the Merged_T/DJI_SDK_raw folder shell.exec(paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\DJI_SDK_loop_updated.bat&quot;)) 7.4 Convert to Raster Below we will: read in the binary data outputted by the thermal SDK convert to a raster update new temperature raster with exif data from original _T.JPG H20T images https://community.rstudio.com/t/solution-to-reading-raw-images-into-r/45435 dir_raw &lt;- paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\DJI_SDK_raw\\\\&quot;) raw_list &lt;- list.files(dir_raw) length(raw_list) #seeing how long it takes for code to run start.time &lt;- Sys.time() #loop that only writes out a folder with the Temperature rasters for (i in 1:length(raw_list)){ #for (i in 1:3){ wh &lt;- c(640, 512) print(&quot;new&quot;) #str_m &lt;- paste0(&quot;measure_p_&quot;,i-1,&quot;.raw&quot;) #i-1 because .raw images start at zero not 1 str_m &lt;- raw_list[i] print(str_m) #getting name of .raw file but without the .raw extension so it can be saved as a tiff with the same name as the .raw (which has the same name as the original H20T images) so they can be swapped after alignment in metashape name &lt;- substr(str_m,1,nchar(str_m)-4) print(name) #If else below allows you to rerun this code from where it left off if R aborts the session test_exists &lt;- paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\Temperature_rasters_EXIF\\\\&quot;, name,&quot;.tiff&quot;) if (file.exists(test_exists)){ print(&quot;exists&quot;) } else { #divide by 2 because 16 means 2 bit integers (?) #print(paste0(dir_raw, str_m)) file.info(paste0(dir_raw, str_m))$size/2 #getting product of width and height and checking it is correct prod(wh) #getting numerical values v &lt;- readBin(paste0(dir_raw,str_m), what = &quot;integer&quot;, n = prod(wh), size = 2, signed = TRUE, endian = &quot;little&quot;) #temp in deg celcius, DJI requires we divide by 10 here for degrees in celcius v_temp &lt;- v/10 ## check the values range(v_temp) ## creating a matrix of the proper size matrix &lt;- matrix(v_temp, wh[1], wh[2])[wh[1]:1,] #matrix #needs library OpenImageR #rotate the matrix by 90 to get correct orientation of image mat_rotated = rotateFixed(matrix, 90) #to view image #image(matrix(v_temp, wh[1], wh[2])[wh[1]:1,], useRaster = TRUE, col = grey.colors(256)) rast &lt;- raster(mat_rotated) #creating a raster #plot(rast) #matching original image to measure.raw file #original h20T images: dir_h20t &lt;- paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\&quot;) #h20t_list &lt;- list.files(dir_h20t, pattern = &quot;_T\\\\.JPG$&quot;) #since names are the same, here we call the name of the raw file -.raw and add .JPG str_h20t &lt;- paste0(substr(str_m, 1, nchar(str_m)-3),&quot;JPG&quot;) #reading in jpg h20T raster r_h20t &lt;- raster(paste0(dir_h20t,str_h20t)) r_h20t_path &lt;- paste0(dir_h20t,str_h20t) #for later exiftool print(r_h20t_path) # the extent is bound by the resolution that you have assigned to it - so must change extent then resolution: https://stackoverflow.com/questions/36019332/copy-metadata-between-two-raster-objects-r/ #need to add these before exif because exif command used wont over write already written ones extent(rast) &lt;- extent(r_h20t) res(rast) &lt;- res(r_h20t) #crs(rast) &lt;- crs(r_h20t) #writing out the itterator print(i) print(paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\Temperature_rasters\\\\&quot;, name, &quot;.tiff&quot;)) writeRaster(rast, paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\Temperature_rasters\\\\&quot;, name,&quot;.tiff&quot;), overwrite = TRUE) r_temperature_path &lt;- paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\Temperature_rasters\\\\&quot;, name,&quot;.tiff&quot;) #STEP 3: Use EXIFTOOL in terminal to add all remaining/missing metadata from the original H20T images to the new single band TIFFS out_dir &lt;- paste0(dir,&quot;\\\\&quot;,merged,&quot;\\\\Temperature_rasters_EXIF\\\\&quot;, name,&quot;.tiff&quot;) #shell lets you run windows terminal cmds from R shell(paste0(&quot;exiftool -wm cg -tagsfromfile &quot;,r_h20t_path,&quot; -all:all &quot;,r_temperature_path, &quot; -o &quot;,out_dir)) #output are temperature tiffs with same metadata as original h20T images } percent &lt;- i/length(raw_list)*100 print(paste0(&quot;percent complete: &quot;, round(percent, 3))) } end.time &lt;- Sys.time() time.taken &lt;- round(end.time - start.time,2) time.taken therm_dates &lt;- c(&quot;2022_10_05&quot;) for(x in 1:length(therm_dates)){ date_therm &lt;- therm_dates[x] print(date_therm) dir &lt;- paste0(&quot;I:\\\\PARSER_Ext\\\\Fdc_PR_Canoe\\\\Flights\\\\&quot;,date_therm,&quot;\\\\1_Data\\\\H20T&quot;) #directory to h20T imagery list_dir &lt;- list.files(dir, full.names = TRUE,pattern = c(&quot;DJI.+-H20T&quot;)) #folders containing orignal imagery print(list_dir) # GETTING ALL RGB IMAGES for (d in 1:length(list_dir)){ #calling each directly separately folder_dir &lt;- list_dir[d] if(d == 1){ files_W &lt;- list.files(folder_dir, pattern = &quot;_W.JPG$&quot;) }else{ files_W_add &lt;- list.files(folder_dir, pattern = &quot;_W.JPG$&quot;) files_W &lt;- append(files_W,files_W_add) } } dir_IR &lt;- paste0(&quot;I:\\\\PARSER_Ext\\\\Fdc_PR_Canoe\\\\Flights\\\\&quot;,date_therm,&quot;\\\\1_Data\\\\H20T\\\\Merged_T\\\\Temperature_rasters_EXIF_DST&quot;) #list of thermal files files_T &lt;- list.files(dir_IR, pattern = &quot;_T.tiff$&quot;) # Function to extract the core name from a file path get_core_name &lt;- function(file_path) { basename(file_path) %&gt;% gsub(&quot;_[TW]$&quot;, &quot;&quot;, .) } #creating a new folder for the metahspae test merged &lt;- &quot;Merged_T&quot; if (!dir.exists(paste0(dir, &quot;\\\\&quot;,merged,&quot;\\\\RGB_&amp;_IR_EXIF_DST&quot;))) { dir.create(paste0(dir, &quot;\\\\&quot;,merged,&quot;\\\\RGB_&amp;_IR_EXIF_DST&quot;),recursive = TRUE) } dir_new &lt;- paste0(dir, &quot;\\\\&quot;,merged,&quot;\\\\RGB_&amp;_IR_EXIF_DST&quot;) for (d in 1:length(list_dir)){ #calling each directly separately print(d) folder_dir_W &lt;- list_dir[d] files_W &lt;- list.files(folder_dir_W, pattern = &quot;_W.JPG$&quot;) for (i in 1:length(files_T)) { core_name_T &lt;- substr(files_T[i], 1, nchar(files_T[i]) - 6) matching_file_W &lt;- files_W[str_detect(files_W, core_name_T)] if(length(matching_file_W)&gt;=1){ #copying over files file.copy(from = paste0(folder_dir_W,&quot;\\\\&quot;, matching_file_W), to = paste0(dir_new,&quot;\\\\&quot;, matching_file_W), overwrite = FALSE) file.copy(from = paste0(dir_IR,&quot;\\\\&quot;, files_T[i]), to = paste0(dir_new,&quot;\\\\&quot;, files_T[i]), overwrite = FALSE) } } } } HELLOO THIS IS A TEST Note: We have run across an error in some imagery, DJI is working on a solution. The error is as follows: ERROR: call dirp_set_measurement_params failed ERROR: call prv_isp_config failed Test done with return code -6 "]]
