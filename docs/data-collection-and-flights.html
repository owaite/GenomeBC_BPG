<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Data Collection and Flights | Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials</title>
  <meta name="description" content="Chapter 6 Data Collection and Flights | Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Data Collection and Flights | Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Data Collection and Flights | Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials" />
  
  
  

<meta name="author" content="Jake King*, Olivia Waite, Miriam Isaac-Renton, Nicholas C. Coops, Samuel Grubinger, Liam Irwin, Lise Van Der Merwe, Jon Degner, Alex Liu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="preflight-planning.html"/>
<link rel="next" href="processing-orthomosaics.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Drone Based Phenotyping for Research Trials</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#github-link"><i class="fa fa-check"></i>GitHub link</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-cite-this-report"><i class="fa fa-check"></i>How to cite this report:</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="abstract.html"><a href="abstract.html"><i class="fa fa-check"></i><b>1</b> Abstract</a></li>
<li class="chapter" data-level="2" data-path="background-and-basis-of-knowledge.html"><a href="background-and-basis-of-knowledge.html"><i class="fa fa-check"></i><b>2</b> Background and basis of knowledge</a></li>
<li class="chapter" data-level="3" data-path="glossary.html"><a href="glossary.html"><i class="fa fa-check"></i><b>3</b> Glossary</a></li>
<li class="chapter" data-level="4" data-path="range-of-sites-assessed.html"><a href="range-of-sites-assessed.html"><i class="fa fa-check"></i><b>4</b> Range of Sites Assessed</a>
<ul>
<li class="chapter" data-level="4.1" data-path="range-of-sites-assessed.html"><a href="range-of-sites-assessed.html#jordan-river"><i class="fa fa-check"></i><b>4.1</b> Jordan River</a></li>
<li class="chapter" data-level="4.2" data-path="range-of-sites-assessed.html"><a href="range-of-sites-assessed.html#powell-river"><i class="fa fa-check"></i><b>4.2</b> Powell River</a></li>
<li class="chapter" data-level="4.3" data-path="range-of-sites-assessed.html"><a href="range-of-sites-assessed.html#sites"><i class="fa fa-check"></i><b>4.3</b> 2024 Sites</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="preflight-planning.html"><a href="preflight-planning.html"><i class="fa fa-check"></i><b>5</b> Preflight Planning</a>
<ul>
<li class="chapter" data-level="5.1" data-path="preflight-planning.html"><a href="preflight-planning.html#achieving-positional-accuracy-on-multi-temporal-and-multi-sensor-data-collections"><i class="fa fa-check"></i><b>5.1</b> Achieving positional accuracy on multi-temporal and multi-sensor data collections</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="preflight-planning.html"><a href="preflight-planning.html#kinematic-processing-rtkppk"><i class="fa fa-check"></i><b>5.1.1</b> Kinematic processing: RTK/PPK</a></li>
<li class="chapter" data-level="5.1.2" data-path="preflight-planning.html"><a href="preflight-planning.html#ground-control-points-gcp"><i class="fa fa-check"></i><b>5.1.2</b> Ground Control Points (GCP)</a></li>
<li class="chapter" data-level="5.1.3" data-path="preflight-planning.html"><a href="preflight-planning.html#absolute-and-relative-reference-with-precise-point-positioning-ppp"><i class="fa fa-check"></i><b>5.1.3</b> Absolute and Relative Reference with Precise Point Positioning (PPP)</a></li>
<li class="chapter" data-level="5.1.4" data-path="preflight-planning.html"><a href="preflight-planning.html#terrain-following-on-sites-with-elevation-change"><i class="fa fa-check"></i><b>5.1.4</b> Terrain Following on Sites with Elevation Change</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="preflight-planning.html"><a href="preflight-planning.html#site-selection-considerations"><i class="fa fa-check"></i><b>5.2</b> Site Selection Considerations</a></li>
<li class="chapter" data-level="5.3" data-path="preflight-planning.html"><a href="preflight-planning.html#drone-and-sensors-what-will-i-need-to-purchase-and-how-much-will-it-cost"><i class="fa fa-check"></i><b>5.3</b> Drone and Sensors: What will I need to purchase and how much will it cost?</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="preflight-planning.html"><a href="preflight-planning.html#hardware-costs"><i class="fa fa-check"></i><b>5.3.1</b> Hardware Costs</a></li>
<li class="chapter" data-level="5.3.2" data-path="preflight-planning.html"><a href="preflight-planning.html#drone-training-in-canada"><i class="fa fa-check"></i><b>5.3.2</b> Drone Training in Canada</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-collection-and-flights.html"><a href="data-collection-and-flights.html"><i class="fa fa-check"></i><b>6</b> Data Collection and Flights</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data-collection-and-flights.html"><a href="data-collection-and-flights.html#hardware---dji-matrice-3000-rtk-m300"><i class="fa fa-check"></i><b>6.1</b> Hardware - DJI Matrice 3000 RTK (M300)</a></li>
<li class="chapter" data-level="6.2" data-path="data-collection-and-flights.html"><a href="data-collection-and-flights.html#battery-management"><i class="fa fa-check"></i><b>6.2</b> Battery Management</a></li>
<li class="chapter" data-level="6.3" data-path="data-collection-and-flights.html"><a href="data-collection-and-flights.html#workflow-site-reconnaissance"><i class="fa fa-check"></i><b>6.3</b> Workflow: Site Reconnaissance</a></li>
<li class="chapter" data-level="6.4" data-path="data-collection-and-flights.html"><a href="data-collection-and-flights.html#data-collection-flight-planning"><i class="fa fa-check"></i><b>6.4</b> Data Collection: Flight Planning</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="data-collection-and-flights.html"><a href="data-collection-and-flights.html#flight-planning-theory"><i class="fa fa-check"></i><b>6.4.1</b> Flight planning theory</a></li>
<li class="chapter" data-level="6.4.2" data-path="data-collection-and-flights.html"><a href="data-collection-and-flights.html#flight-planning-with-dji-pilot"><i class="fa fa-check"></i><b>6.4.2</b> Flight planning with DJI Pilot</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="data-collection-and-flights.html"><a href="data-collection-and-flights.html#data-collection-sensors-parameters-and-ideal-conditions"><i class="fa fa-check"></i><b>6.5</b> Data Collection: Sensors, Parameters, and Ideal Conditions</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="data-collection-and-flights.html"><a href="data-collection-and-flights.html#micasense-10-band-spectral-sensor-for-vegetative-indices"><i class="fa fa-check"></i><b>6.5.1</b> MicaSense: 10-Band Spectral Sensor for Vegetative Indices</a></li>
<li class="chapter" data-level="6.5.2" data-path="data-collection-and-flights.html"><a href="data-collection-and-flights.html#zenmuse-p1-high-quality-natural-colour-rgb-imagery"><i class="fa fa-check"></i><b>6.5.2</b> Zenmuse P1: High-Quality Natural-Colour (RGB) Imagery</a></li>
<li class="chapter" data-level="6.5.3" data-path="data-collection-and-flights.html"><a href="data-collection-and-flights.html#zenmuse-l1-lidar-and-rgb"><i class="fa fa-check"></i><b>6.5.3</b> Zenmuse L1: LiDAR and RGB</a></li>
<li class="chapter" data-level="6.5.4" data-path="data-collection-and-flights.html"><a href="data-collection-and-flights.html#zenmuse-h20t-thermal-and-rgb"><i class="fa fa-check"></i><b>6.5.4</b> Zenmuse H20T: Thermal and RGB</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="processing-orthomosaics.html"><a href="processing-orthomosaics.html"><i class="fa fa-check"></i><b>7</b> Agisoft Photogrammetry pipeline</a>
<ul>
<li class="chapter" data-level="7.1" data-path="processing-orthomosaics.html"><a href="processing-orthomosaics.html#overview."><i class="fa fa-check"></i><b>7.1</b> Overview.</a></li>
<li class="chapter" data-level="7.2" data-path="processing-orthomosaics.html"><a href="processing-orthomosaics.html#detailed-workflow"><i class="fa fa-check"></i><b>7.2</b> Detailed Workflow</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="Georeferencing-Plot-Trees.html"><a href="Georeferencing-Plot-Trees.html"><i class="fa fa-check"></i><b>8</b> Georeferencing Plot Trees</a>
<ul>
<li class="chapter" data-level="8.1" data-path="Georeferencing-Plot-Trees.html"><a href="Georeferencing-Plot-Trees.html#workflow-in-qgis"><i class="fa fa-check"></i><b>8.1</b> Workflow in QGIS</a></li>
<li class="chapter" data-level="8.2" data-path="Georeferencing-Plot-Trees.html"><a href="Georeferencing-Plot-Trees.html#alternate-workflow-in-r"><i class="fa fa-check"></i><b>8.2</b> Alternate Workflow in R</a></li>
<li class="chapter" data-level="8.3" data-path="Georeferencing-Plot-Trees.html"><a href="Georeferencing-Plot-Trees.html#final-steps-in-qgis"><i class="fa fa-check"></i><b>8.3</b> Final Steps in QGIS</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="crown-delineation.html"><a href="crown-delineation.html"><i class="fa fa-check"></i><b>9</b> Crown Delineation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="crown-delineation.html"><a href="crown-delineation.html#circles-proportional-to-tree-height"><i class="fa fa-check"></i><b>9.1</b> Circles Proportional to Tree Height</a></li>
<li class="chapter" data-level="9.2" data-path="crown-delineation.html"><a href="crown-delineation.html#supercell-raster"><i class="fa fa-check"></i><b>9.2</b> Supercell Raster</a></li>
<li class="chapter" data-level="9.3" data-path="crown-delineation.html"><a href="crown-delineation.html#filter-merge-supercells"><i class="fa fa-check"></i><b>9.3</b> Filter &amp; Merge Supercells</a></li>
<li class="chapter" data-level="9.4" data-path="crown-delineation.html"><a href="crown-delineation.html#manually-edit-crowns"><i class="fa fa-check"></i><b>9.4</b> Manually Edit Crowns</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="shadow-mask.html"><a href="shadow-mask.html"><i class="fa fa-check"></i><b>10</b> Shadow Masks</a></li>
<li class="chapter" data-level="11" data-path="Vegetation-Indicies.html"><a href="Vegetation-Indicies.html"><i class="fa fa-check"></i><b>11</b> Crown-level Vegetation Indicies</a>
<ul>
<li class="chapter" data-level="11.1" data-path="Vegetation-Indicies.html"><a href="Vegetation-Indicies.html#vi-workflow"><i class="fa fa-check"></i><b>11.1</b> VI Workflow</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html"><i class="fa fa-check"></i><b>12</b> LiDAR Processing Workflow</a>
<ul>
<li class="chapter" data-level="12.1" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html#dji-terra"><i class="fa fa-check"></i><b>12.1</b> DJI Terra</a></li>
<li class="chapter" data-level="12.2" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html#register-the-lidar-to-the-dap-point-cloud"><i class="fa fa-check"></i><b>12.2</b> Register the LiDAR to the DAP point cloud</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html#prepare-lidar-for-registration"><i class="fa fa-check"></i><b>12.2.1</b> Prepare LiDAR for registration</a></li>
<li class="chapter" data-level="12.2.2" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html#register-lidar-to-dap-cloud-in-cloudcompare"><i class="fa fa-check"></i><b>12.2.2</b> Register LiDAR to DAP cloud in CloudCompare</a></li>
<li class="chapter" data-level="12.2.3" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html#rescale-and-tile-the-registered-lidar"><i class="fa fa-check"></i><b>12.2.3</b> Rescale and tile the registered LiDAR:</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html#normalization-and-individual-tree-point-clouds"><i class="fa fa-check"></i><b>12.3</b> Normalization and individual tree point clouds</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html#creating-a-dtm-from-the-registered-lidar-tiles"><i class="fa fa-check"></i><b>12.3.1</b> Creating a DTM from the registered LiDAR tiles</a></li>
<li class="chapter" data-level="12.3.2" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html#normalizing-the-tiles-using-the-dtm"><i class="fa fa-check"></i><b>12.3.2</b> Normalizing the tiles using the DTM</a></li>
<li class="chapter" data-level="12.3.3" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html#creating-a-4cm-chm-from-the-max-z-values"><i class="fa fa-check"></i><b>12.3.3</b> Creating a 4cm CHM from the max Z values</a></li>
<li class="chapter" data-level="12.3.4" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html#segmenting-tiles"><i class="fa fa-check"></i><b>12.3.4</b> Segmenting tiles</a></li>
<li class="chapter" data-level="12.3.5" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html#merging-segmented-tiles-to-create-one-large-point-cloud-containing-the-top-defined-height-of-each-tree"><i class="fa fa-check"></i><b>12.3.5</b> Merging segmented tiles to create one large point cloud containing the top defined height % of each tree</a></li>
<li class="chapter" data-level="12.3.6" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html#clipping-the-point-cloud-into-individual-point-clouds-per-tree"><i class="fa fa-check"></i><b>12.3.6</b> Clipping the point cloud into individual point clouds per tree</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="lidar-processing-workflow.html"><a href="lidar-processing-workflow.html#individual-tree-metrics"><i class="fa fa-check"></i><b>12.4</b> Individual Tree Metrics</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html"><i class="fa fa-check"></i><b>13</b> MicaSense Irradiance Correction</a>
<ul>
<li class="chapter" data-level="13.1" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#background"><i class="fa fa-check"></i><b>13.1</b> Background</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#deriving-horizontal-irradiance"><i class="fa fa-check"></i><b>13.1.1</b> Deriving Horizontal Irradiance</a></li>
<li class="chapter" data-level="13.1.2" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#calculating-reflectance"><i class="fa fa-check"></i><b>13.1.2</b> Calculating Reflectance</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#the-problem"><i class="fa fa-check"></i><b>13.2</b> The Problem</a></li>
<li class="chapter" data-level="13.3" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#is-correction-needed"><i class="fa fa-check"></i><b>13.3</b> Is Correction Needed?</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#do-the-sun-sensor-angles-make-sense"><i class="fa fa-check"></i><b>13.3.1</b> Do the sun sensor angles make sense?</a></li>
<li class="chapter" data-level="13.3.2" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#does-the-relationship-between-direct-and-horizontal-irradiance-make-sense"><i class="fa fa-check"></i><b>13.3.2</b> Does the relationship between direct and horizontal irradiance make sense?</a></li>
<li class="chapter" data-level="13.3.3" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#does-the-scattered-direct-ratio-make-sense-for-the-lighting-conditions-of-that-day"><i class="fa fa-check"></i><b>13.3.3</b> Does the scattered: direct ratio make sense for the lighting conditions of that day?</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#DLS1-correction"><i class="fa fa-check"></i><b>13.4</b> Correcting Irradiance Values for MicaSense Cameras</a>
<ul>
<li class="chapter" data-level="13.4.1" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#reading-exif-data"><i class="fa fa-check"></i><b>13.4.1</b> Reading Exif Data</a></li>
<li class="chapter" data-level="13.4.2" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#calculating-solar-zenith-anlge"><i class="fa fa-check"></i><b>13.4.2</b> Calculating Solar Zenith Anlge</a></li>
<li class="chapter" data-level="13.4.3" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#SSA-correction"><i class="fa fa-check"></i><b>13.4.3</b> Caluclating Sun-Sensor Angles</a></li>
<li class="chapter" data-level="13.4.4" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#scattereddirect-ratio-estimate"><i class="fa fa-check"></i><b>13.4.4</b> Scattered:Direct Ratio Estimate</a></li>
<li class="chapter" data-level="13.4.5" data-path="MicaSense-Irradiance-Correction.html"><a href="MicaSense-Irradiance-Correction.html#computing-horizontal-corrected-irradiance"><i class="fa fa-check"></i><b>13.4.5</b> Computing Horizontal (Corrected) Irradiance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="Thermal-Conversion.html"><a href="Thermal-Conversion.html"><i class="fa fa-check"></i><b>14</b> Thermal Conversion</a>
<ul>
<li class="chapter" data-level="14.1" data-path="Thermal-Conversion.html"><a href="Thermal-Conversion.html#folder-set-up"><i class="fa fa-check"></i><b>14.1</b> Folder Set-Up</a></li>
<li class="chapter" data-level="14.2" data-path="Thermal-Conversion.html"><a href="Thermal-Conversion.html#weather-data"><i class="fa fa-check"></i><b>14.2</b> Weather Data</a></li>
<li class="chapter" data-level="14.3" data-path="Thermal-Conversion.html"><a href="Thermal-Conversion.html#dji-thermal-sdk-temperature-conversion"><i class="fa fa-check"></i><b>14.3</b> DJI Thermal SDK: temperature conversion</a>
<ul>
<li class="chapter" data-level="14.3.1" data-path="Thermal-Conversion.html"><a href="Thermal-Conversion.html#parameters"><i class="fa fa-check"></i><b>14.3.1</b> Parameters</a></li>
<li class="chapter" data-level="14.3.2" data-path="Thermal-Conversion.html"><a href="Thermal-Conversion.html#running-the-sdk"><i class="fa fa-check"></i><b>14.3.2</b> Running the SDK</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="Thermal-Conversion.html"><a href="Thermal-Conversion.html#convert-to-raster"><i class="fa fa-check"></i><b>14.4</b> Convert to Raster</a></li>
<li class="chapter" data-level="14.5" data-path="Thermal-Conversion.html"><a href="Thermal-Conversion.html#examples"><i class="fa fa-check"></i><b>14.5</b> Examples</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Guidelines and Procedures for Drone Based Phenotyping in Forest Research Trials</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data-collection-and-flights" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Data Collection and Flights<a href="data-collection-and-flights.html#data-collection-and-flights" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="hardware---dji-matrice-3000-rtk-m300" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Hardware - DJI Matrice 3000 RTK (M300)<a href="data-collection-and-flights.html#hardware---dji-matrice-3000-rtk-m300" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="figure"><span style="display:block;" id="fig:M300-flying"></span>
<img src="Photos_&_gifs/M300_flying.png" alt="DJI Matrice 300 RTK in flight over one of our Vancouver Island field sites, taken by Alex Liu with a Mavic3." width="100%" />
<p class="caption">
Figure 6.1: DJI Matrice 300 RTK in flight over one of our Vancouver Island field sites, taken by Alex Liu with a Mavic3.
</p>
</div>
<p>A complete SOP for mapping flights with the M300 was developed in-house and can be found in the <a href="https://github.com/owaite/GenomeBC_BPG/tree/main/SOPs">SOPs folder</a> on GitHub. Transport Canada (TC) regulations require logging all flights and to have the logs on-hand when flying. We have developed a simple spreadsheet that logs the details required by TC and other useful details for each flight. When undertaking a project with repeated data collection campaigns, we would recommend developing a similar approach to organizing these data. A copy of the one we use is available within the <a href="https://github.com/owaite/GenomeBC_BPG/tree/main/Example_Documents">Example Documents</a> folder on GitHub.</p>
<p>While the DJI M300 drone can fly in heavy rain, wind and under any light conditions, flying under such conditions can damage your equipment or create artifacts in the resulting data and generally reduce data quality and usability. Each sensor has specific requirements for producing the best-quality data, outlined in greater detail below. It is highly recommended to keep the flight app and all hardware up-to-date and to familiarize oneself with the “Release Notes” before each trip as the DJI platform evolves quickly. App and hardware updates require Wi-Fi, so updating in a location with service is crucial, ideally you can do this before visiting the field.</p>
</div>
<div id="battery-management" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Battery Management<a href="data-collection-and-flights.html#battery-management" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Managing battery life is an ongoing concern when flying drones. The <a href="https://enterprise.dji.com/matrice-300/specs">DJI M300 documentation</a> claims 55-minute flight time without a payload on one set of batteries. We have found flight time with a payload on newer batteries to be 30-35 minutes and 25-30 on older batteries. To keep enough batteries charged for a field day we start the day with four sets of charged batteries in the battery station and bring a small generator into the field to charge batteries as they are used.</p>
<p>The M300 RTK system is compatible with the TB60 and TB65 batteries. However, it is important to note they cannot be mixed when used on the drone’s two battery sockets. The TB60 and TB65 batteries have different voltage ratings and internal configurations, and mixing them will lead to imbalanced power distribution, and potentially causing power fluctuations or damaging the drone’s power system.</p>
<p>For each set of batteries, it is also important to differentiate between them. If batteries are used interchangeably, their charge and discharge rates will quickly begin to vary and so will their capacity. Therefore, it is required to mark them in pairs and only use them as such.</p>
<p>It is possible to manually set the discharge rate for each individual battery when it is slotted in the drone. The setting can be found under the battery logo of the triple dot menu in camera view. This can be helpful when you may have a few days before the next set of flights. By default, DJI batteries will automatically discharge to 95% after twelve hours, but after the user set time in days it will begin to discharge to 50-60%, the default setting is 2 days before safe discharge occurs.</p>
</div>
<div id="workflow-site-reconnaissance" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Workflow: Site Reconnaissance<a href="data-collection-and-flights.html#workflow-site-reconnaissance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ahead of the first data collection flights we follow the reconnaissance workflow outlined in Figure <a href="data-collection-and-flights.html#fig:Reconnasissance-flowDiagram">6.2</a>. This is to assess for hazards, confirm site perimeter, layout GCPs and collect their coordinates (where possible), and to locate trees to enable georeferencing existing trial data to the imagery. We followed this workflow in March 2024 at the Big Tree creek trial where were able to find suitable locations to install GCPs and were able to collect precise GPS points for the GCPs. We also measured azimuth and distance from the GCPs to the nearest trees to enable us to tie the row column grid to the imagery (georeferencing) later in the processing. Please refer to the <a href="https://github.com/owaite/GenomeBC_BPG/tree/main/SOPs">SOP_Emlid_RS3</a> for detailed instructions on using the Emlid GNSS receivers. The <a href="https://docs.emlid.com/reachrs3/">Emlid RS3 documentation</a> is excellent and covers a wide range of applications.</p>
<div class="figure"><span style="display:block;" id="fig:Reconnasissance-flowDiagram"></span>
<img src="Photos_&_gifs/Reconnasissance_flowDiagram.PNG" alt="Reconnaissance workflow." width="100%" />
<p class="caption">
Figure 6.2: Reconnaissance workflow.
</p>
</div>
</div>
<div id="data-collection-flight-planning" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Data Collection: Flight Planning<a href="data-collection-and-flights.html#data-collection-flight-planning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="flight-planning-theory" class="section level3 hasAnchor" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Flight planning theory<a href="data-collection-and-flights.html#flight-planning-theory" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In drone photogrammetry, Ground Sampling Distance (GSD) is a critical measure that represents the pixel size on the ground, directly influencing the resolution of the images captured. A smaller GSD indicates higher detail, meaning that more information is available in each image. The GSD is primarily determined by the drone’s altitude and the camera’s resolution—lower altitudes result in a smaller GSD, offering finer detail in the images.</p>
<p>The primary variables in the flight planning software are altitude, speed, and overlap (both front and side). Higher altitudes increase GSD, covering more area but reducing image resolution. Speed must be adjusted carefully; slower speeds ensure better image overlap and minimize motion blur, which is especially important in windy conditions. Overlap is critical for ensuring that each image captures enough common ground with adjacent images to allow for accurate 3D reconstruction and data processing. Higher overlap generally improves the quality of the final data but also increases the number of images required and the overall length of the flight. The required overlap is complicated by the complexity of the surface. We have found that attaining high quality output imagery on bare ground requires considerably lower overlap. This is due to two reasons, first, displacement of free moving objects on bare ground, such as sticks and rocks commonly found on recently logged trials, requires very strong winds and is overall a less complex surface than a closed canopy with thin coastal treetops moving in a breeze. The second reason is the elevation of the canopy itself. Unless you are using a DSM the overlap calculation is based on the elevation from the ground. When flying over mature sites with closed canopy the effective altitude is the flight altitude minus the canopy height. This will decrease the effective, or canopy, overlap which is an important consideration when collecting data over what is already a complex structure.</p>
<p>Figure <a href="data-collection-and-flights.html#fig:effective-overlap">6.3</a> outlines the difference in ground overlap and canopy overlap. We have developed a calculator in which you enter the flight altitude, overlaps, and canopy height and it will output the canopy overlap. You can then adjust the inputs until you get the required overlap. It is available in the <a href="https://github.com/owaite/GenomeBC_BPG/tree/main/Example_Documents">Example Documents</a> folder on GitHub. Figure <a href="data-collection-and-flights.html#fig:effective-overlap">6.3</a> below is an example calculation detailed in the next paragraph.</p>
<div class="figure"><span style="display:block;" id="fig:effective-overlap"></span>
<img src="Photos_&_gifs/effective_overlap.png" alt="Effective overlap calculation." width="100%" />
<p class="caption">
Figure 6.3: Effective overlap calculation.
</p>
</div>
</div>
<div id="flight-planning-with-dji-pilot" class="section level3 hasAnchor" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Flight planning with DJI Pilot<a href="data-collection-and-flights.html#flight-planning-with-dji-pilot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are various flight planning software options. We chose the DJI Pilot app, which comes installed on the remote control (RC), for its ease of use. The Big Tree creek Douglas fir site has approximately 2900 gridded tree planting positions at 2 m spacing. Because there was no accurate polygon for the site prior to the project, we marked positions for GCPs and made waypoints for the corners of the site with the DJI Pilot app during the reconnaissance flight. We then built the flight plan polygon from the corner points and added an extra margin for the first flights. Once the site corners were confirmed, for subsequent flights, we remade the polygon in a Geographic Information System (GIS) and clipped to 15 meters outside the edge trees. This polygon is the flight shape and was 2 Ha for the Big Tree site. While doing this reconnaissance flight, the pilot scouted the site for hazards and noted that, within 20 meters of the site, there were mature wildlife trees left standing after the last harvest. The median tree height for plot trees was 12.5m however, the wildlife trees were approximately 45 m tall. To allow for a 10m safety margin above the wildlife trees the minimum elevation was set to 55 m. Additionally, the site is on a 25 percent slope, which required terrain following. Flight planning is sensor-specific, after entering the sensor, the app presents a list of options. See Figure <a href="data-collection-and-flights.html#fig:P1-flightPlan">6.4</a> for a visual of the site polygon and flight parameters for a P1 flight. The recommended flight parameters for each sensor are quoted in the next section.</p>
<div class="figure"><span style="display:block;" id="fig:P1-flightPlan"></span>
<img src="Photos_&_gifs/P1_flight_plan.png" alt="Flight planning a P1 flight on the remote controller. Colored lines represent terrain following elevation." width="100%" />
<p class="caption">
Figure 6.4: Flight planning a P1 flight on the remote controller. Colored lines represent terrain following elevation.
</p>
</div>
</div>
</div>
<div id="data-collection-sensors-parameters-and-ideal-conditions" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Data Collection: Sensors, Parameters, and Ideal Conditions<a href="data-collection-and-flights.html#data-collection-sensors-parameters-and-ideal-conditions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The parameters we have listed for each sensor are based on collecting data on gridded 1.5 - 2 Ha closed canopy coastal Douglas fir and Western redcedar sites. The parameters are conservative and should allow for less than ideal flight conditions.</p>
<div id="micasense-10-band-spectral-sensor-for-vegetative-indices" class="section level3 hasAnchor" number="6.5.1">
<h3><span class="header-section-number">6.5.1</span> MicaSense: 10-Band Spectral Sensor for Vegetative Indices<a href="data-collection-and-flights.html#micasense-10-band-spectral-sensor-for-vegetative-indices" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The MicaSense RedEdge-MX Dual (MS) and its replacement, the Micasense RedEdge-P Dual (MS_P), (where P represents panchromatic), are 10-Band multispectral sensors that detect reflectance in ten specific wavebands of reflected light that provide useful information on vegetation health. The MS_P includes an 11th camera that is senstive to a wide range of wavelengths across the visible and NIR spectra, allowing it to capture a much larger amount of light (photons) per capture, and thus producing a higher spatial resolution product that allows for pansharpening.</p>
<p>The MicaSense (MS) cameras are powered by the M300, but the gimbal mount only provides power; the camera itself is not integrated and does not communicate with the drone or remote controller. The camera is started via a laptop or phone though a Wi-Fi port connection housed on the MS and runs until the camera is linked again and stopped or the drone is powered down.</p>
<p>The multispectral cameras are passive sensors and as such are sensitive to changing light conditions. To detect these changes Micasense cameras use a Downwelling Light Sensor (DLS), which is mounted to the top of the M300, to measure ambient light conditions during flight and sun to sensor angles. These measurements are written into the metadata of each photo. We are using the DLS2 which measures the ambient light and sun angle using 10 small circular sensors spread across the top of the DLS. Hence, it is imperative that these sensors are clean to avoid erroneous measurements.</p>
<p>Multispectral sensors also require calibration photos to be taken at the start and end of each flight. This involves taking pictures of a specialized MicaSense calibration panel with known reflectance values. These pictures need to be taken in an open area for the DLS to correctly mimic the current flight conditions, ideally the location is large enough to avoid scattering from nearby vegetation or other objects. Ensure there are no shadows being cast onto the panel or over the DLS as these calibration pictures are taken, either from nearby objects, vegetation, or the person taking the photos.</p>
<p>As seen in Figure <a href="data-collection-and-flights.html#fig:DLS-Micasense-connection">6.5</a> the MicaSense Dual cameras are two sets of cameras, each captures 5 unique wavebands. The cameras write images as .tif with an _(x) for each of (x) wavelengths and organizes the images into folders with 1000 images per folder, in as many folders as needed. Image capture rate can be set as high as one image per second. A 25–30-minute flight that covers a 1.5-2 Ha site using the flight parameters outlined below will yield 5-6 folders per camera system producing 10-12 thousand images.</p>
<p>The MS sensors are particularly prone to difficulties writing to data cards and the data must be carefully checked and reflown if necessary. We keep a spare set of SD cards for each sensor and will not overwrite any cards until the data has been confirmed by loading into the processing software. For the MS sensors, ensure SD cards used are formatted FAT32, they will not work in EXFAT32 or other formats. Formatting cards should be done through the MicaSense wireless connection using the built in format card button.</p>
<p>Figure <a href="data-collection-and-flights.html#fig:DLS-Micasense-connection">6.5</a></p>
<div class="figure"><span style="display:block;" id="fig:DLS-Micasense-connection"></span>
<img src="Photos_&_gifs/DLS2_Micasense_connection.png" alt="Micasense RedEdge-MX Dual and the DLS2 mounted on the M300. Here we are connecting the DLS2 to the Micasense camera." width="100%" />
<p class="caption">
Figure 6.5: Micasense RedEdge-MX Dual and the DLS2 mounted on the M300. Here we are connecting the DLS2 to the Micasense camera.
</p>
</div>
<p><strong>Sensor Specifications:</strong></p>
<ul>
<li><p><strong>Camera Type:</strong> MicaSense Dual cameras</p></li>
<li><p><strong>Image Format:</strong> .tif with a suffix indicating the band (e.g., _1, _2)</p></li>
<li><p><strong>Bands/Wavelengths:</strong></p></li>
</ul>
<table>
<caption><span id="tab:MS-Table">Table 6.1: </span>Band names, center wavelengths, and bandwidths collected by the Red and Blue cameras within the Micasense Dual camera systems. Panchro* is only available in the pan chromatic models.</caption>
<thead>
<tr class="header">
<th align="left">Band Name</th>
<th align="left">Center Wavelength (nm)</th>
<th align="left">Bandwidth (nm)</th>
<th align="left">Camera</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Blue</td>
<td align="left">475</td>
<td align="left">32</td>
<td align="left">Red</td>
</tr>
<tr class="even">
<td align="left">Green</td>
<td align="left">560</td>
<td align="left">27</td>
<td align="left">Red</td>
</tr>
<tr class="odd">
<td align="left">Red</td>
<td align="left">668</td>
<td align="left">14</td>
<td align="left">Red</td>
</tr>
<tr class="even">
<td align="left">Red Edge</td>
<td align="left">717</td>
<td align="left">12</td>
<td align="left">Red</td>
</tr>
<tr class="odd">
<td align="left">NIR</td>
<td align="left">842</td>
<td align="left">57</td>
<td align="left">Red</td>
</tr>
<tr class="even">
<td align="left">Panchromatic</td>
<td align="left">634</td>
<td align="left">463</td>
<td align="left">Red</td>
</tr>
<tr class="odd">
<td align="left">Coastal Blue</td>
<td align="left">444</td>
<td align="left">28</td>
<td align="left">Blue</td>
</tr>
<tr class="even">
<td align="left">Green</td>
<td align="left">531</td>
<td align="left">14</td>
<td align="left">Blue</td>
</tr>
<tr class="odd">
<td align="left">Red</td>
<td align="left">650</td>
<td align="left">16</td>
<td align="left">Blue</td>
</tr>
<tr class="even">
<td align="left">Red Edge</td>
<td align="left">705</td>
<td align="left">10</td>
<td align="left">Blue</td>
</tr>
<tr class="odd">
<td align="left">Red Edge</td>
<td align="left">740</td>
<td align="left">18</td>
<td align="left">Blue</td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>Image Capture Rate:</strong></p>
<ul>
<li><p>The default setting for timed intervals is one image capture every two seconds, with simultaneous capture for all bands.</p></li>
<li><p>The capture rate can be adjusted to a maximum of one image per second</p></li>
</ul></li>
<li><p><strong>Output:</strong> 5-6 folders per camera (10-12 thousand images) for a 25–30-minute flight covering a 1.5-2 Ha site</p></li>
<li><p><strong>GSD:</strong> 2.5-4cm at 40m above canopy. 1.5-2cm when processed pansharpened with the panchromatic band.</p></li>
</ul>
<p><strong>Flight Parameters:</strong></p>
<ul>
<li><p><strong>Flight Elevation:</strong></p>
<ul>
<li>40 m above canopy</li>
</ul></li>
<li><p><strong>Flight Speed:</strong> ~2 m/s (slower speeds reduce motion blur, especially in windy conditions)</p></li>
<li><p><strong>Image Overlap:</strong> Extra high to ensure sufficient coverage and allow for exclusion of poor-quality images during processing.</p>
<ul>
<li><p>Front: ~86% (speed and timed interval is the limiting factor)</p></li>
<li><p>Side: ~86%</p></li>
</ul></li>
<li><p><strong>Margins:</strong> 10 m around the site perimeter is sufficient for flight planning due to the wide-angle shot of the MS camera</p></li>
<li><p><strong>Flight Time:</strong> Approximately 30 minutes for a 1.5-2 Ha area site. This allows for a single battery flight, which helps to maintain even light conditions.</p></li>
</ul>
<p><strong>Best Practices:</strong></p>
<ul>
<li><p><strong>Weather Conditions:</strong> Avoid flying in any precipitation as MicaSense cameras are unprotected from moisture, with exposed data and power connections.</p></li>
<li><p><strong>Lighting:</strong> Conduct flights within two hours of solar noon to minimize shadowing in the imagery.</p>
<ul>
<li>The DLS calibration helps to reduce the effect changing light conditions, but extreme changes in light conditions will leave residual effects that are difficult to process out. Figure <a href="data-collection-and-flights.html#fig:drone-variable-light">6.6</a> below shows a flight in variable conditions (left) in which the calibration has failed to correct the changing light conditions. The flight on the right was flown on an ideal day with even diffuse light.</li>
</ul></li>
</ul>
<p>Figure <a href="data-collection-and-flights.html#fig:drone-variable-light">6.6</a></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:drone-variable-light"></span>
<img src="Photos_&_gifs/variable_light.png" alt="Left: MS orthomosaic in variable light. Right: MS orthomosaic in even diffuse light." width="50%" /><img src="Photos_&_gifs/diffuse_light.png" alt="Left: MS orthomosaic in variable light. Right: MS orthomosaic in even diffuse light." width="50%" />
<p class="caption">
Figure 6.6: Left: MS orthomosaic in variable light. Right: MS orthomosaic in even diffuse light.
</p>
</div>
<ul>
<li><p><strong>Wind Conditions:</strong> Lower wind speeds yield better results; however, the impact of wind will vary depending on species and site-specific factors. Consider flying at a higher altitude and overlap in windier conditions.</p></li>
<li><p><strong>Flight Elevation Considerations:</strong> While lower flight elevations produce higher resolution images, they increase flight time, making calibration more challenging.</p></li>
</ul>
<p>Click <a href="https://github.com/owaite/GenomeBC_BPG/tree/main/SOPs">here</a> to access our Micasense field SOP housed on GitHub.</p>
<p><a href="https://support.micasense.com/hc/en-us/articles/224893167-Best-practices-Collecting-Data-with-MicaSense-Sensors">Best practices: Collecting Data with MicaSense Sensors – MicaSense Knowledge Base</a> is an important resource.</p>
</div>
<div id="zenmuse-p1-high-quality-natural-colour-rgb-imagery" class="section level3 hasAnchor" number="6.5.2">
<h3><span class="header-section-number">6.5.2</span> Zenmuse P1: High-Quality Natural-Colour (RGB) Imagery<a href="data-collection-and-flights.html#zenmuse-p1-high-quality-natural-colour-rgb-imagery" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The P1 is RTK/PPK enabled and has the highest resolution and positional accuracy of the sensors available. It is used for georeferencing, crown delineation, and to aid in the registration of data from sensors with less spatial accuracy.</p>
<p><strong>Sensor Specifications:</strong></p>
<ul>
<li><p><strong>Sensor Type:</strong> Natural-colour (RGB)</p></li>
<li><p><strong>Resolution:</strong> 45MP</p></li>
<li><p><strong>Pixel Size:</strong> 1cm per 80m altitude, 7-9 mm at the paramaters detailed below.</p></li>
<li><p><strong>Positional Accuracy:</strong> 3-5cm without GCPs when using RTK/PPK.</p></li>
<li><p><strong>Waterproof:</strong> No</p></li>
</ul>
<p><strong>Flight Parameters:</strong></p>
<ul>
<li><p><strong>Flight Elevation:</strong> 65m above ground or canopy.</p></li>
<li><p><strong>Flight Speed:</strong> ~4m/s.</p></li>
<li><p><strong>Overlap:</strong> 85% front and side.</p></li>
<li><p><strong>Margin:</strong> 20 m to reduce edge error and ensure higher image quality.</p></li>
<li><p><strong>Flight Time:</strong> Approximately 15 minutes for a 2-Hectare site.</p></li>
</ul>
<p><strong>Best Practices:</strong></p>
<ul>
<li><p><strong>Timing:</strong> Conduct template flights before spring flush or “greenup” in order to easily detect experimental trees from ingress and brush.</p></li>
<li><p><strong>Wind Conditions:</strong> Ideal on days with little or no wind; &lt;10 km/h for coastal species (e.g., western redcedar, Douglas-fir); up to 20 km/h may be suitable for other species (e.g., juvenile interior spruce).</p></li>
<li><p><strong>Lighting:</strong> Best results are achieved on overcast days due to diffuse lighting conditions which reduces shadows and enhances detail. We have found that of full sun days using the “Auto” settings will produce oversaturated imagery. The <a href="https://en.wikipedia.org/wiki/Sunny_16_rule">sunny 16 rule</a> of photography does help with the P1 on full sun days.</p></li>
</ul>
<p>Figure <a href="data-collection-and-flights.html#fig:P1-RGB">6.7</a></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:P1-RGB"></span>
<img src="Photos_&_gifs/P1_pre_greenup.png" alt="These images are the same area, the left was taken with the above parameters with the P1 before greenup. The right was taken with the wide angle RGB on the Zenmuse H20T, a lower resolution camera, mid-August." width="50%" /><img src="Photos_&_gifs/RGB_mid_Aug.jpg" alt="These images are the same area, the left was taken with the above parameters with the P1 before greenup. The right was taken with the wide angle RGB on the Zenmuse H20T, a lower resolution camera, mid-August." width="50%" />
<p class="caption">
Figure 6.7: These images are the same area, the left was taken with the above parameters with the P1 before greenup. The right was taken with the wide angle RGB on the Zenmuse H20T, a lower resolution camera, mid-August.
</p>
</div>
</div>
<div id="zenmuse-l1-lidar-and-rgb" class="section level3 hasAnchor" number="6.5.3">
<h3><span class="header-section-number">6.5.3</span> Zenmuse L1: LiDAR and RGB<a href="data-collection-and-flights.html#zenmuse-l1-lidar-and-rgb" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="figure"><span style="display:block;" id="fig:RC-L1"></span>
<img src="Photos_&_gifs/RC_L1.png" alt="L1 LiDAR screenshot from the remote controller during a flight at Big Tree Creek site on July 7, 2024. On the left is the RGB image instantaneously acquired for the frame and the right shows the LIDAR data acquisition from the scanner as it acquired the data up the frame." width="100%" />
<p class="caption">
Figure 6.8: L1 LiDAR screenshot from the remote controller during a flight at Big Tree Creek site on July 7, 2024. On the left is the RGB image instantaneously acquired for the frame and the right shows the LIDAR data acquisition from the scanner as it acquired the data up the frame.
</p>
</div>
<p>LiDAR (Light Detection and Ranging): LiDAR is an active sensor that emits laser pulses at specific wavelengths and measures the time and intensity of the returns to calculate distances. This allows it to create precise 3D models of the terrain or objects. LiDAR relies on a highly accurate Inertial Measurement Unit (IMU) and GNSS to calculate positional data, including roll, pitch, yaw (orientation), and X, Y, Z coordinates (spatial position), enabling accurate georeferencing of the collected data.</p>
<p>The L1 LiDAR is used to build DTMs for terrain following when one is not available and is flown again with terrain following to calculate tree metrics.</p>
<p><strong>Sensor Specifications:</strong></p>
<ul>
<li><p><strong>Echo Mode:</strong> Triple returns for penetrating vegetation and tree canopies. Ideally this will record three distinct returns per pulse: First Return- Typically represents the top of the canopy. Second Return- Represents mid-canopy layers or understory vegetation. Third Return- Represents the ground</p></li>
<li><p><strong>RGB Imagery:</strong> The L1 also includes a 20 MP RGB camera, which can be used as a lower resolution substitute for a P1 camera.</p></li>
<li><p><strong>System Accuracy:</strong> 10cm Horizontal, 5cm Vertical (50m AGL)</p></li>
<li><p><strong>Moisture Resistance:</strong> Rated for rainy or foggy conditions, but moisture in the air can result in noise and overall degrade point cloud quality; not recommended to fly in such conditions.</p></li>
</ul>
<p><strong>Flight Parameters:</strong> These settings have been effective for coastal open and closed canopy sites.</p>
<ul>
<li><p><strong>Repetitive Scan Mode:</strong> Recommended for detailed terrain and vegetation profiling. This is being further tested.</p></li>
<li><p><strong>Inflight Automatic Calibration:</strong> Highly recommended for accuracy; the IMU recalibrates every 100 seconds during the flight.</p></li>
<li><p><strong>Overlap:</strong> 85% front and side (85x85) for RGB imagery.</p></li>
<li><p><strong>Flight Elevation:</strong> 60-85 m, depending on site conditions.</p></li>
<li><p><strong>Flight Speed:</strong> Approximately 4 m/s.</p></li>
<li><p><strong>Point Density:</strong> Greater than 1500 points per square meter.</p></li>
<li><p><strong>Elevation Optimization:</strong> Should be turned OFF to avoid inconsistencies in the point cloud dataset.</p></li>
<li><p><strong>Timing of Flight:</strong> When possible, it is best to collect LiDAR before greenup in the spring. LiDAR is typically taken before or after multispectral flights as it is not constricted to the -/+ 2 hours of solar noon window.</p></li>
</ul>
<p><strong>Zenmuse L2 LiDAR:</strong> The recently released L2 LiDAR sensor offers improved positional accuracy due to a smaller beam footprint and more precise IMU. It also supports up to five returns per pulse, enhancing the within canopy detail of the point cloud and potentially defining a more accurate DTM for areas with understory.</p>
</div>
<div id="zenmuse-h20t-thermal-and-rgb" class="section level3 hasAnchor" number="6.5.4">
<h3><span class="header-section-number">6.5.4</span> Zenmuse H20T: Thermal and RGB<a href="data-collection-and-flights.html#zenmuse-h20t-thermal-and-rgb" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Thermal Imaging (H20T DJI Thermal Camera): The H20T is a multi-sensor camera with thermal imaging, zoom, and wide-angle RGB (visible light) cameras in one sensor, allowing it to capture detailed visual and thermal data. The thermal sensor detects infrared radiation and converts it into temperature data, which can then be used to analyze heat signatures of objects, terrain, or structures. The H20T also integrates a laser rangefinder for accurate distance measurements, which can be useful for checking and confirming terrain following flight mission altitudes.</p>
<p><strong>Note:</strong> The H20T was not designed for thermal mapping and the raw thermal imagery needs to be converted to single band temperature tiffs. Our conversion method is detailed in the <a href="Thermal-Conversion.html#Thermal-Conversion">Thermal Conversion</a> chapter.</p>
<p>GCP’s are very helpful with thermal imagery. It is dificult to confirm that the crown polygons are positionally accurate without them, especially in closed canopy sites.</p>
<p><strong>Sensor Specifications:</strong></p>
<ul>
<li><p><strong>GSD:</strong> Thermal ~6cm at 70m, RGB wide ~2cm at 70m.</p></li>
<li><p><strong>Zoom:</strong> The H20T has a 23x hybrid optical zoom with digital zoom capabilities for a combined zoom of up to 200x in RGB mode.</p></li>
<li><p><strong>Thermal Sensitivity:</strong> &lt;50 mK at f/1.0, providing high sensitivity for detecting slight differences in temperature.</p></li>
<li><p><strong>Spectral Band:</strong> The thermal sensor operates in the 7.5 - 13.5 µm range, which is ideal for detecting heat sources in various environmental conditions.</p></li>
<li><p><strong>Thermal Accuracy:</strong> The temperature measurement accuracy is ±2°C or ±2%, whichever is greater.</p></li>
<li><p><strong>Gain Mode:</strong> The High Gain mode measures temperatures from -40°C to 150°C at a finer scale than the regular gain mode.  It is highly recommended to ensure the high gain mode is set for this application.</p></li>
</ul>
<p><strong>Flight Parameters:</strong></p>
<ul>
<li><p><strong>Optimal Flight Altitude:</strong> The size of our sites required ~70 m altitude. Flying as low as possible based on system and terrain restrictions yields more detailed thermal data.</p></li>
<li><p><strong>Flight Speed:</strong> Recommended at 1.5-4m/s depending on site size, aim for slower speeds if possible to maximize thermal data quality.</p></li>
<li><p><strong>Overlap:</strong> Thermal: &gt;85% front and side overlap (85x85) to ensure comprehensive coverage</p></li>
<li><p><strong>Environmental Conditions:</strong> Best in low wind clear conditions.</p></li>
<li><p><strong>Timing of Flight:</strong> Best results from early afternoon. We found that imagery taken later in the day can be overwhelmed by radiant heat and ealier has heavy “shadows” from overnight temperatures.</p></li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="preflight-planning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="processing-orthomosaics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/owaite/GenomeBC_BPG/edit/main/0_Data_Collection_Flights.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/owaite/GenomeBC_BPG/blob/main/0_Data_Collection_Flights.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
